---
title: "Quantitative Methods and Statistics"
subtitle: "An applied course using the `R` programming language"
author: 
  - name: "Jakob Hoffmann"
    affiliation: "Economic Geography Group, LMU Munich"
format:
  html:
    toc: true
---

# Why build statistical models?

Statistics is about **learning from data**, especially data that is incomplete or has other imperfections, such as measurement error (i.e., almost all scientific data).
The data themselves are of course not the ultimate goal - they measure (more or less crudely) abstract constructs that play a role in our scientific models of the world, or rather some aspect of it.

![The big picture of statistical inference (Kass 2011)](figures/real-and-theoretical-world.png)

So far, we have seen a range of simple visual and numerical methods to describe certain characteristics of a dataset.
Beyond these methods, statistical models can be helpful **tools for description** of complex multivariate data, e.g. by disentangling otherwise difficult to spot trends.
However, the simple descriptive methods we have seen so far also treat only implicitly two important aspects of the scientific process which we ultimately want to make more explicit: causality and uncertainty.

Implicit or explicit **claims of causality** are an inherent feature of scientific theories (TODO: Why, how?). If applied carefully (and if the stars align), statistical models can be used to test expectations from theory in both controlled (i.e., experimental) and uncontrolled (i.e., observational) settings.  

In the same vein, uncertainty is an inherent feature of all scientific production of knowledge from empirical observation.
TODO: why, how?
Statistical models will not rid us of these epistemic shades of grey, but they can be used to **quantify and assess uncertainty** so that decisions about how to weigh evidence can be made more explicitly.

Specifying a statistical model of the processes that procuded the observed data, called the **data generating process** (DGP), forces us to be explicit about our assumptions regarding statistical relationships and sources of uncertainty.
As such, the process of statistical modeling establishes transparency, which is one of the highest goods in science as a collaborative endeavour.

<!-- ## Causality

- causes of effects, effects of causes (Gelman)

- explanation vs. theory

### The fundamental problem of causal inference

### Experimental and observational research designs

## Uncertainty

The scientific production of knowledge from empirical observation is inherently subject to uncertainty.

### Aleatoric uncertainty

### Epistemic uncertainty

-   Describing and summarizing real-world phenomena

-   Predicting real-world phenomena

-   Identifying causal relationships


- The scientific goals of using statistics -->


# Modeling the data-generating process

Statistical models adapt two important characteristics which scientific models also exhibit: abstraction and assumptions.

- Abstraction

- Assumptions

- Randomness

- Generative models

- Theoretical and statistical models

- DGP comprises theorized process (the _signal_) _and_ the measurement process


# References

Kass, R. E. (2011). Statistical Inference: The Big Picture. Statistical Science, 26(1). https://doi.org/10.1214/10-STS337

Deffner, D., Fedorova, N., Andrews, J., & McElreath, R. (2024). Bridging theory and data: A computational workflow for cultural evolution. Proceedings of the National Academy of Sciences, 121(48), e2322887121. https://doi.org/10.1073/pnas.2322887121
