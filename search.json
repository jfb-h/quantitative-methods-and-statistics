[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "This website accompanies the introduction to quantitative methods and statistics taught in the Masters Degree in Human Geography at the Department of Geography, LMU Munich. The objective of this class is for students to develop a working knowledge of common applied statistical procedures which enables them to independently identify appropriate methods and conduct analysis using the R programming language in the context of quantitative research designs. Although most students in the class will have previously visited a first course in statistics, the class has no prerequisites in either statistics or programming. It will however move beyond descriptive statistics relatively quickly to focus on statistical modelling and inference.\n\n\nThe course will roughly follow the below structure, but we are also flexible to adapt the programme as we go along:\n\n\n\n\n\n\n\nSession\nTopics\n\n\n\n\n1 - 3\nR basics: data wrangling, visualization, descriptive statistics\n\n\n4\nStochastic models: probability & simulation\n\n\n5\nStatistical inference: sampling, estimation & uncertainty\n\n\n6 - 7\nLinear regression: basics, prediction, goodness of fit, transformations\n\n\n8\nLinear regression: multiple regression, interactions, smooth effects\n\n\n9 - 10\nCausal inference: experiments and observational studies\n\n\n11 - 13\nAdvanced topics: GLMs, random effects, dependent data\n\n\n\n\n\n\nThe class will finish with a 60 minute exam in the last week of the semester which will test proficiency with both theoretical as well as practical skills learned throughout the class.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#course-programme",
    "href": "index.html#course-programme",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "The course will roughly follow the below structure, but we are also flexible to adapt the programme as we go along:\n\n\n\n\n\n\n\nSession\nTopics\n\n\n\n\n1 - 3\nR basics: data wrangling, visualization, descriptive statistics\n\n\n4\nStochastic models: probability & simulation\n\n\n5\nStatistical inference: sampling, estimation & uncertainty\n\n\n6 - 7\nLinear regression: basics, prediction, goodness of fit, transformations\n\n\n8\nLinear regression: multiple regression, interactions, smooth effects\n\n\n9 - 10\nCausal inference: experiments and observational studies\n\n\n11 - 13\nAdvanced topics: GLMs, random effects, dependent data",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#certification",
    "href": "index.html#certification",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "The class will finish with a 60 minute exam in the last week of the semester which will test proficiency with both theoretical as well as practical skills learned throughout the class.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html",
    "href": "05-descriptive-multivariate.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "As in the last session, we will continue working with our CO₂ emissions, population, and GDP data. However, instead of analyzing variables in isolation, we will now move to multivariate descriptions and explore how variables relate to each other. We begin by loading packages and reading the datasets from parquet files:\n\n#library(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nemissions &lt;- arrow::read_parquet(\"data/emissions-population.parquet\")\ngdp &lt;- arrow::read_parquet(\"data/gdp.parquet\")\n\nBecause these data again share the same dimensions, we can join them:\n\ndf &lt;- left_join(emissions, gdp)\n\nJoining with `by = join_by(entity, code, year)`\n\nglimpse(df)\n\nRows: 215\nColumns: 6\n$ entity      &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", …\n$ code        &lt;chr&gt; \"AFG\", \"ALB\", \"DZA\", \"AND\", \"AGO\", \"AIA\", \"ATG\", \"ARG\", \"A…\n$ year        &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020…\n$ emissions   &lt;dbl&gt; 0.2970625, 1.6403993, 3.8185709, 4.9233950, 0.4942432, 9.6…\n$ population  &lt;dbl&gt; 39068977, 2871950, 44042094, 77397, 33451139, 14864, 91864…\n$ gdp_per_cap &lt;dbl&gt; 2769.686, 14662.796, 14194.155, 55488.492, 7556.968, NA, 2…\n\n\nA quick glance at the summaries of our three primary variables shows some missing values in GDP:\n\nsummary(df$emissions)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.03587  0.96823  2.96665  4.47960  5.54895 36.56351 \n\nsummary(df$population)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n1.822e+03 1.010e+06 6.934e+06 7.334e+07 2.617e+07 7.887e+09 \n\nsummary(df$gdp_per_cap)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   833.9   5893.0  15022.3  23684.9  35641.5 129865.6       19 \n\n\nWe will drop observations with missing variables for now:\n\ndf &lt;- drop_na(df)\n\nNow we have a combined dataset with emissions, population, and GDP data for countries. The kinds of multivariate analyses which are sensible depend on the combination of our variables’ data types, which we will walk through in the following.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#scatterplots",
    "href": "05-descriptive-multivariate.html#scatterplots",
    "title": "Quantitative Methods and Statistics",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots are the fundamental tool for visualizing the relationship between two numerical variables. Each point represents one observation, with its position determined by the values of both variables.\n\nggplot(df, aes(x = gdp_per_cap, y = emissions)) +\n    geom_point(alpha = 0.4) +\n    labs(\n        x = \"GDP per capita (USD)\",\n        y = \"CO2 emissions per capita (tons)\"\n    )\n\n\n\n\n\n\n\n\nThe scatterplot reveals a clear positive relationship: wealthier countries tend to have higher per-capita emissions. However, there is considerable variation, especially among wealthier countries, and a lot of the observations are clumped together at the lower end.\nTwo sessions ago, we have learned that in such a case, it might make sense to apply a log transformation to our variables:\n\nggplot(df, aes(x = gdp_per_cap, y = emissions)) +\n    geom_point(alpha = 0.6) +\n    scale_x_log10() +\n    scale_y_log10() +\n    labs(\n        x = \"log GDP per capita (USD)\",\n        y = \"log CO2 emissions per capita (tons)\"\n    )\n\n\n\n\n\n\n\n\nThis reveals a much less noisy picture of the relationship and also makes sense, conceptually: An increase in GDP per capita by a factor of X is typically associated with an increase in emissions by a factor of Y. Going forward, take note of the fact that the relationship is quite linear, but not perfectly so.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#pearsons-correlation-coefficient",
    "href": "05-descriptive-multivariate.html#pearsons-correlation-coefficient",
    "title": "Quantitative Methods and Statistics",
    "section": "Pearson’s correlation coefficient",
    "text": "Pearson’s correlation coefficient\nWhile scatterplots show us the relationship visually, correlation coefficients quantify the strength and direction of relationships between two variables in a symmetric fashion, meaning that the correlation of x and y is the same as the correlation of y and x. The best known correlation coefficient is Pearson’s correlation coefficient, which measures the strength of a linear relationship. We can easily compute it in R using the cor() function:\n\ncor(df$gdp_per_cap, df$emissions)\n\n[1] 0.6698732\n\n\nThe value of Person’s correlation coefficient can range from -1 (indicating a perfect negative linear relationship) to +1 (indicating a perfect positive linear relationship). The correlation of 0.67 for GDP per capita and per capita emissions confirms our visual impression of a strong positive relationship. The correlation becomes even stronger (0.9) when we correlate the log transformed values, again confirming our visual intuitions:\n\ncor(log(df$gdp_per_cap), log(df$emissions))\n\n[1] 0.8960481\n\n\nLet’s also check the correlation between GDP and population:\n\ncor(df$gdp_per_cap, df$population)\n\n[1] -0.02851595\n\n\nThe near-zero correlation (-0.03) tells us there’s essentially no linear relationship between a country’s wealth and its population size - rich countries can be small or large, and vice versa.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#spearmans-correlation-coefficient",
    "href": "05-descriptive-multivariate.html#spearmans-correlation-coefficient",
    "title": "Quantitative Methods and Statistics",
    "section": "Spearman’s correlation coefficient",
    "text": "Spearman’s correlation coefficient\nPearson correlation measures linear relationships, but we have seen before that the relationship between GDP and emissions is not perfectly linear. Spearman’s correlation coefficient measures monotonic relationships - whether one variable tends to increase (or decrease) as the other increases, regardless of whether this relationship is linear. It does so by first transforming the input variables to their rank representations and then computing the Pearson’s correlation coefficient. Because we are looking at ranks, it doesn’t matter if the magnitude of change in y differs with respect to the level of x, as long as the direction of change is still the same (i.e. the relationship is monotonic). Spearman correlation is also more robust to outliers since it works with ranks rather than actual values.\nTo see what the rank transformation is about, try it out yourself:\n\nrank(c(3.5, 12.7, 11.2, 2.8))\n\n[1] 2 4 3 1\n\n\nWith that, we could compute Spearman’s correlation ourselves by putting the rank transformed variables into the cor() function. However, we can also just pass the method = \"spearman\" argument to the cor() function, which is more convenient and efficient:\n\ncor(df$gdp_per_cap, df$emissions, method = \"spearman\")\n\n[1] 0.8808119\n\n\nThe Spearman correlation also assumes values in the -1 to +1 interval and so the value of 0.88 again signals a strong monotonic relationship, stronger than the basic version of Pearson’s correlation and similar to the log-log version. Note that log transforming variables before using Spearman’s correlation coefficient doesn’t do anything since the log transformation is a monotonic function and so the ranks are the same as for the untransformed variable.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#cross-tabulations",
    "href": "05-descriptive-multivariate.html#cross-tabulations",
    "title": "Quantitative Methods and Statistics",
    "section": "Cross-tabulations",
    "text": "Cross-tabulations\nCross-tabulations (or contingency tables) show the frequency of combinations between two categorical variables. Our primary variables are numerical, but we can create categorical ones (ordinal, to be precise) from them:\n\ndf &lt;- df |&gt; mutate(\n    emissions_level = case_when(\n        emissions &lt; 2 ~ \"Low emissions\",\n        emissions &lt; 8 ~ \"Medium emissions\",\n        emissions &gt;= 8 ~ \"High emissions\",\n        TRUE ~ NA\n    ),\n    emissions_level = factor(\n        emissions_level,\n        levels = c(\"Low emissions\", \"Medium emissions\", \"High emissions\"),\n        ordered = TRUE\n    ),\n        gdp_level = case_when(\n        gdp_per_cap &lt; 5000 ~ \"Low GDP\",\n        gdp_per_cap &lt; 20000 ~ \"Medium GDP\",\n        gdp_per_cap &gt;= 20000 ~ \"High GDP\",\n        TRUE ~ NA\n    ),\n    gdp_level = factor(\n        gdp_level,\n        levels = c(\"Low GDP\", \"Medium GDP\", \"High GDP\"),\n        ordered = TRUE\n    )\n)\n\nNow we can create a cross-tabulation of development level and emissions level. The easiest way is to use the table() function with two vectors a input:\n\ntable(df$gdp_level, df$emissions_level) \n\n            \n             Low emissions Medium emissions High emissions\n  Low GDP               39                0              0\n  Medium GDP            41               37              5\n  High GDP               4               47             23\n\n\nAlternatively, we can also use the familiar tidyverse machinery to create it manually (more flexible but also more code):\n\ncount(df, gdp_level, emissions_level) |&gt;\n    pivot_wider(\n        names_from = emissions_level, \n        values_from = n, \n        values_fill = 0)\n\n# A tibble: 3 × 4\n  gdp_level  `Low emissions` `Medium emissions` `High emissions`\n  &lt;ord&gt;                &lt;int&gt;              &lt;int&gt;            &lt;int&gt;\n1 Low GDP                 39                  0                0\n2 Medium GDP              41                 37                5\n3 High GDP                 4                 47               23\n\n\nThis table shows clear patterns: all low-income countries have low emissions, while high-income countries are distributed across all emission levels, with most having medium or high per capita emissions.\nIf we want proportions instead of raw counts, we can wrap our original table into prop.table(), to which we can pass an additional margin argument (where one indicates to divide by row totals and two would produce column :\n\nprop.table(table(df$gdp_level, df$emissions_level), margin = 1)\n\n            \n             Low emissions Medium emissions High emissions\n  Low GDP       1.00000000       0.00000000     0.00000000\n  Medium GDP    0.49397590       0.44578313     0.06024096\n  High GDP      0.05405405       0.63513514     0.31081081",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#stacked-barplots",
    "href": "05-descriptive-multivariate.html#stacked-barplots",
    "title": "Quantitative Methods and Statistics",
    "section": "Stacked barplots",
    "text": "Stacked barplots\nCross-tabulations give us the numbers, but visualizations make the patterns clearer. We can use stacked bar charts to compare the distribution of one categorical variable across levels of another.\n\nggplot(df, aes(x = gdp_level, fill = emissions_level)) + geom_bar(position = \"stack\")\n\n\n\n\n\n\n\n\nOften, we care more about the relative numbers than the overall group sizes. In that case, we can use proportional stacked bars, which make it even clearer how the emission profiles differ across development levels. We can create a proportional barplot by changing the position = \"stack\" argument to position = \"fill\":\n\nggplot(df, aes(x = gdp_level, fill = emissions_level)) + geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nIf we instead mostly care about the absolute numbers, creating a grouped barplot by passing position = \"dodge\" might be preferred.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#chi-squared-statistic-and-cramérs-v",
    "href": "05-descriptive-multivariate.html#chi-squared-statistic-and-cramérs-v",
    "title": "Quantitative Methods and Statistics",
    "section": "Chi-squared statistic and Cramér’s V",
    "text": "Chi-squared statistic and Cramér’s V\nWhile we cannot use the various correlation coefficients for categorical data, we can quantify the strength of association between categorical variables using the chi-squared statistic and Cramér’s V.\nThe chi-squared statistic measures how much the observed frequencies deviate from what we’d expect if there were no association, accounting for differences in group sizes. To compute it, we first need to produce the contingency table:\n\ncont_table &lt;- table(df$gdp_level, df$emissions_level)\n\nBased on that, we can use the chisq.test() function, which also produces a statistical test, which we will ignore for now. Instead, we just look at the summary statistics, which we can access with $statistics:\n\nchi_result &lt;- chisq.test(cont_table)\nchi_result$statistic\n\nX-squared \n 103.0496 \n\n\nThe resulting value of 103 is difficult to interpret because it depends on sample size. Cramér’s V standardizes this to a 0 to 1 scale, making it mare useful as a measure of association. While there is again no built in function in base R, it is easy to compute so we don’t need to pull in a package:\n\ncramers_v &lt;- sqrt(chi_result$statistic / (sum(cont_table) * min(dim(cont_table) - 1)))\ncramers_v\n\nX-squared \n0.5127198 \n\n\nA Cramér’s V of 0.51 indicates a reasonably strong association between development and emissions levels.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#boxplots",
    "href": "05-descriptive-multivariate.html#boxplots",
    "title": "Quantitative Methods and Statistics",
    "section": "Boxplots",
    "text": "Boxplots\nA classical visualization which is designed to compare the distributional differences of a numerical variable across groups is the boxplot or ‘box and whiskers’ plot. They display the median, quartiles (i.e. the range containing 50% of the data), and outliers, allowing for quick visual comparison of numerical summaries. Let’s look at a boxplot of emissions by GDP level:\n\nggplot(df, aes(x = gdp_level, y = emissions)) + geom_boxplot()\n\n\n\n\n\n\n\n\nIn a boxplot, the ‘box’ shows the interquartile range (IQR), the bar inside the box shows the median, the ‘whiskers’ show 1.5 * the IQR, and everything beyond the whiskers is considered an outlier. The plot is again a little bit hard to read due to the presence of some extreme emissions outliers. We have learned that we can help ourselves with a log transformation. To make it easier to read off values, we also add log ticks with the guide = \"axis_logticks\" argument:\n\nlibrary(scales)\n\nggplot(df, aes(x = gdp_level, y = emissions)) + geom_boxplot() + \n    scale_y_log10(guide = \"axis_logticks\")\n\n\n\n\n\n\n\n\nThe usual caveats for log plots apply: While it has become easier to see the distribution at lower levels, the same visual length now means different things at different levels (in absolute terms), so more care needs to be taken to not misinterpret the plot.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#groupwise-summaries",
    "href": "05-descriptive-multivariate.html#groupwise-summaries",
    "title": "Quantitative Methods and Statistics",
    "section": "Groupwise summaries",
    "text": "Groupwise summaries\nTo produce numerical summaries across groups, we can just use the now familiar group_by() and summarize() machinery and a set of measures of location and dispersion suited for our numerical variable:\n\ngroup_summaries &lt;- df |&gt;\n    group_by(gdp_level) |&gt;\n    summarize(\n        count = n(),\n        mean = mean(emissions),\n        weighted_mean = weighted.mean(emissions, population),\n        median = median(emissions),\n        sd = sd(emissions),\n        vc = sd / mean,\n        mad = mad(emissions),\n    )\n\ngroup_summaries\n\n# A tibble: 3 × 8\n  gdp_level  count  mean weighted_mean median    sd    vc   mad\n  &lt;ord&gt;      &lt;int&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Low GDP       39 0.377         0.270  0.288 0.323 0.855 0.241\n2 Medium GDP    83 2.75          4.09   2.13  2.41  0.875 1.71 \n3 High GDP      74 8.50          9.26   6.72  6.52  0.767 3.46 \n\n\nWe could just report this as a summary table or use it to compute specific comparisons, such as the ratio between two groups:\n\ngroup_summaries$mean[3] / group_summaries$mean[1]\n\n[1] 22.5393\n\n\nWe see that the average emissions per capita in rich countries is more than 22 times the average emissions of poor countries.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "03-data-visualization.html",
    "href": "03-data-visualization.html",
    "title": "Quantitative methods and statistics",
    "section": "",
    "text": "Last session, we learned basic data wrangling using the gapminder dataset. As we should now have a basic idea about its contents and structure, we will use it again to learn how to make informative and nice looking visualizations in R.\n\nlibrary(gapminder)\ndata(gapminder) # makes the dataset available\n\nWe will also load the tidyverse again, which contains the ggplot2 package for visualization:\n\nlibrary(tidyverse)\n\nFor the sake of making a simpler plot, we will filter the dataset to the year 2007, using the filter() function we learned about last time:\n\nlibrary(dplyr)\ngapminder2007 &lt;- filter(gapminder, year == 2007)",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#our-first-plot",
    "href": "03-data-visualization.html#our-first-plot",
    "title": "Quantitative methods and statistics",
    "section": "Our first plot",
    "text": "Our first plot\nFor plotting, we use the ggplot2 package, which is contained in the tidyverse. The fundamental building block for any plot is a call to the ggplot() function, to which we pass the data that we want to plot. On its own, this just creates a blank plot, as we haven’t specified which variables we want to plot and how we want to plot them:\n\nlibrary(ggplot2)\n\np &lt;- ggplot(data = gapminder2007)\np\n\n\n\n\n\n\n\n\nTo specify what pieces of our data we want to plot, we have to specify an aesthetic mapping from our data variables to the visual elements of our plot (such as positions or colors). This allows us to specify that we want to display variation in GDP along the x-axis and life expectancy along the y-axis:\n\np &lt;- ggplot(data = gapminder2007,              \n            mapping = aes(x = gdpPercap, y = lifeExp))\np\n\n\n\n\n\n\n\n\nWe can see that our plot now contains axis labels and ticks informed by the range of the data, but there is still no visual representation of the data, because we haven’t specified what kind of plot we want. To specify the how of our visualization, we need to add further instructions to our plotting code.\nggplot2 operates in terms of layers which we can add to our basic plot specification with + to include specific geometric representations of our data (such as points in a scatter plot):\n\np &lt;- p + geom_point()\np\n\n\n\n\n\n\n\n\nThis plot now contains one point for each country in the dataset, using GDP per capita and life expectancy for the x and y coordinates, respectively. It already tells us a lot about the relationship between per capita GDP and life expectancy:\n\nLife expectancy varies a lot and is generally lower for countries with very low GDP (&lt; 5000 USD), and is consistently high (mostly above 75 years) for richer countries (&gt; 20 000 USD).\nAt some point (beyond around 25 000 USD or so), more GDP is not really associated with higher life expectancy.\n\nWhile the plot is already informative, there are many things we can improve, such as including more information from our data by making use of additional visual channels, changing variable scaling, adding better labels, or giving the plot a title.",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#refining-the-plot",
    "href": "03-data-visualization.html#refining-the-plot",
    "title": "Quantitative methods and statistics",
    "section": "Refining the plot",
    "text": "Refining the plot\nOne first improvement would be to add some more information to our plot, such as the continental grouping of the countries. The obvious choice for the visual channel would be colors: There is only a handful of continents and countries in the same continent should be clustered to a certain degree, making colors easily distinguishable. To color points by continent, we need to add another entry to our mapping:\n\np &lt;- ggplot(\n    data = gapminder2007,\n    mapping = aes(\n        x = gdpPercap, \n        y = lifeExp, \n        color = continent\n    )\n) + geom_point()\n\np\n\n\n\n\n\n\n\n\nThat’s much better - we can now see a familiar picture of geographically differentiated economic development and its impact on life expectancy. If we want different colors, we can change them by adding another layer to our plot. We could manually add colors with scale_color_manual() but often it is more convenient to pick a predefined scale, like the ones available in the RColorBrewer package:\n\np &lt;- p + scale_color_brewer(palette = \"Set1\")\np\n\n\n\n\n\n\n\n\nIf you want to inspect the available scales, call RColorBrewer::display_brewer_all().\nRight now, the variation in the left part of the plot is hard to differentiate because the points are close together and there is strong variation in a very narrow GDP range. We can solve this by using a log transformation for GDP, i.e. by plotting the logarithm of GDP (typically base 10 in this kind of scenario) instead of actual GDP. Moving along the x axis by a fixed length will then no longer indicate an additive increase in GDP, but a multiplicative one. Because of this nonlinear nature of the logarithm transform, the points in the left part of the figure will be spread out more while the points to the right will be closer to each other.\nWe can implement this by simply adding another layer with the scale_x_log10() function to our plot:\n\np &lt;- p + scale_x_log10()\np\n\n\n\n\n\n\n\n\nWe can now see the variation in the lower GDP range much clearer and we now can see a log-linear trend, i.e., a linear increase in life expectancy with respect to the log of GDP. Some getting used to is required and care needs to be taken when interpreting and creating log scales - they will often yield more readable plots but also have a tendency to confuse people.\nFor some finishing touches, we should add labels and descriptions to our plot, which we do with the labs() function:\n\n p &lt;- p + labs(\n    x = \"Per-capita GDP\",\n    y = \"Life expectancy\",\n    color = \"\",\n    title = \"Economic development and life expectancy\",\n    subtitle = \"Inflation corrected GDP and life expectancy across 140 countries, recorded in 2007\",\n    caption = \"Source: Gapminder\"\n  )\n\np\n\n\n\n\n\n\n\n\nWe can also change the theme of our plot by picking one of the predefined themes and change some stylistic details like the legend position using the theme() function:\n\np &lt;- p + \n    theme_minimal() + \n    theme(legend.position = \"bottom\")\n\np\n\n\n\n\n\n\n\n\nIf we want to use the same theme for all the plots in our session, we can activate it as the default theme (ideally at the start of the script):\n\ntheme_set(theme_minimal())\n\nWhen we are happy with our plot, it is time to export it to a file so that we can easily include it in a paper, report, or presentation, which we do with the ggsave() function:\n\nggsave(\"myplot.png\", p, width = 8, height = 8, dpi = 320)\n\nHere, width and height are specified in inches, but there is an option to switch to cm. The dpi argument sets the output resolution, which should be reasonably high for inclusion in a document.",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#the-final-plot",
    "href": "03-data-visualization.html#the-final-plot",
    "title": "Quantitative methods and statistics",
    "section": "The final plot",
    "text": "The final plot\nHere is everything put together and commented, without using an incrementally improved plot variable as before. It is a lot of code, but at this point we should understand what each line does and be able to make further additions or changes.\n\np&lt;- ggplot(\n    # Dataset from which to plot variables\n    data = gapminder2007,\n    \n    # Mapping variables to visual aspects of the plot\n    mapping = aes(\n      x = gdpPercap, \n      y = lifeExp,\n      color = continent, \n      label = country\n    )\n  ) +\n  \n  # Visualizing point geometries for observations\n  geom_point() +\n\n  # Log scale for x variable\n  scale_x_log10() +\n\n  # Categorical color palette for continents\n  scale_color_brewer(palette = \"Set1\") +\n\n  # Labels for axes, titles etc.\n  labs(\n    x = \"Per-capita GDP\",\n    y = \"Life expectancy\",\n    color = \"\",\n    title = \"Economic development and life expectancy\",\n    subtitle = \"Inflation corrected GDP and life expectancy across 140 countries, recorded in 2007\",\n    caption = \"Source: Gapminder\"\n  ) +\n\n  # General design of the plot\n  theme_minimal() +\n\n  # Special design aspects, like legend position\n  theme(legend.position = \"bottom\")\n\n# Save the plot to a file (will save the last plot that was generated)\nggsave(\"myplot.png\", p, dpi = 320)\n\nSaving 7 x 5 in image",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#small-multiples-with-faceting",
    "href": "03-data-visualization.html#small-multiples-with-faceting",
    "title": "Quantitative methods and statistics",
    "section": "Small multiples with faceting",
    "text": "Small multiples with faceting\nOne powerful visualization technique we have not yet studied is faceting, which is used to create small-multiples plots, i.e. plots which repeat the same visualization structure across multiple panels. This is very useful to, e.g., highlight different trends across subgroups of a dataset.\nThe easiest way to facet a plot is with the facet_wrap() function, which we can again just add as a layer to a plot specification. Here is an example visualizing per-country GDP trends split by continent:\n\np2 &lt;- ggplot(\n    data = gapminder, \n    mapping = aes(\n        x = year, \n        y = gdpPercap,\n        group = country\n        )\n    ) +\n    geom_line(linewidth = 0.5, alpha = 0.3) +\n    facet_wrap(~continent, scales = \"free_y\", nrow = 1)\n\np2\n\n\n\n\n\n\n\n\nThe ~ is part of Rs formula notation, which here just indicates that we want to facet by the continent variable. Note also the use of the scales = \"free_y\" argument, which indicates that each panel should get its own y-axis instead of a shared one. This is necessary because of the big differences in GDP across continents, which would make the variation in some continents hard to see with a shared axis. Finally, note also the group = country in the mapping, which is necessary so that every country gets its own line.",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#combining-multiple-plots-with-patchwork",
    "href": "03-data-visualization.html#combining-multiple-plots-with-patchwork",
    "title": "Quantitative methods and statistics",
    "section": "Combining multiple plots with patchwork",
    "text": "Combining multiple plots with patchwork\nIf we want to combined multiple plots, we can use the patchwork package, which lets us stack plot objects on top of each other with / or side by side with +. We can also add instructions for the relative widhts / heights of each plot with plot_layout() and add annotations, such as letter tags to each plot. Here is an example combining the two plots we just created:\n\nlibrary(patchwork)\n\np / p2 + \n    plot_layout(heights = c(0.6, 0.4)) + \n    plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nLet’s refine this just a bit by giving the second plot the same colors as the first and cleaning up the spacing for the x ticks:\n\np2_colors &lt;- ggplot(gapminder, aes(year, gdpPercap, group = country, color = continent)) +\n    geom_line(linewidth = 0.5, alpha = 0.3, show.legend = FALSE) +\n    facet_wrap(~continent, scales = \"free_y\", nrow = 1) +\n    scale_color_brewer(palette = \"Set1\") +\n    scale_x_continuous(breaks = seq(1950, 2010, by = 20)) +\n    labs(x = \"\", y = \"Per-capita GDP\")\n\np / p2_colors + \n    plot_layout(heights = c(0.6, 0.4)) + \n    plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nNote how we also suppressed the color legend for the lines by setting show.legend = FALSE because we already have a legend for colors from the first plot. The customization possibilities are endless!",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "Programming workflows enable far more complex analyses than classical tools, by allowing to iterate faster and scaling better with data size and complexity. Scripting also changes how we think about and conduct data analysis: Unlike point-and-click interfaces, programming forces us to be explicit about every step of our analysis, making our work reproducible and our assumptions transparent.\n\n\nThere are many different ways to approach any data analysis project, but most follow the same rough outline:\n\nState research question or objective : what do we want to understand?\nGather and clean data : What data do we have to support our objective?\nVisualize and explore data : What is in our data?\nState assumptions and formulate model : How can our data provide evidence for our research objective?\nCheck the model : Are our assumptions violated? How well does our model capture the characteristics of the data?\nIterate : Go back to the drawing board to refine the model, if necessary\nDraw inferences : Condense evidence regarding the primary estimands of interest\n\nThis process is much easier to execute and much more transparent when the steps of your analysis are written down in a file compared to just existing in you head, and R is tailored for this kind of workflow.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#basic-workflow-of-quantitative-research",
    "href": "01-introduction.html#basic-workflow-of-quantitative-research",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "There are many different ways to approach any data analysis project, but most follow the same rough outline:\n\nState research question or objective : what do we want to understand?\nGather and clean data : What data do we have to support our objective?\nVisualize and explore data : What is in our data?\nState assumptions and formulate model : How can our data provide evidence for our research objective?\nCheck the model : Are our assumptions violated? How well does our model capture the characteristics of the data?\nIterate : Go back to the drawing board to refine the model, if necessary\nDraw inferences : Condense evidence regarding the primary estimands of interest\n\nThis process is much easier to execute and much more transparent when the steps of your analysis are written down in a file compared to just existing in you head, and R is tailored for this kind of workflow.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#variables",
    "href": "01-introduction.html#variables",
    "title": "Quantitative Methods and Statistics",
    "section": "Variables",
    "text": "Variables\nIn R, we assign values to names using the assignment operator &lt;- (the equal sign = also works for assignment but is generally frowned upon in R circles - I don’t really care, use what you want). Variable names are allowed to contain underscores _ and dots . but not any other symbols or spaces. They are also case sensitive:\n\nmy_number &lt;- 100\nmy.even.bigger.number &lt;- 1000\n\nTHIS_is_also.a_legal_name &lt;- -3.3334\n\n# this is}an-illegal*name &lt;- 10000\n\nBest practice: Use only lowercase and underscores (you will still see dots, but they are olschool). Use descriptive names that make your code self-documenting without being overly explicit (good variable naming is an art).\nBy creating variables, we keep track of values which we want to reuse in later steps in the analysis. If you want to show the value of a variable, just state it in its own line and execute the cell:\n\nx &lt;- 5\nx\n\n[1] 5\n\n\nVariables can be overwritten, which can be useful to keep the name space (the set off all names defined in the session) clean. The following code overwrites the variable x from the last cell, which you can validate by looking at the global environment:\n\nx &lt;- x + 10\nx\n\n[1] 15\n\n\nBut be careful, overwriting variables can lead to unexpected behavior and headaches down the road.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#functions",
    "href": "01-introduction.html#functions",
    "title": "Quantitative Methods and Statistics",
    "section": "Functions",
    "text": "Functions\nFunctions are objects which take inputs and return outputs (some functions don’t take inputs, or return any output, but produce some other side effect). Functions are called with parentheses directly behind their name and can take arguments. Arguments can be provided by position (in the order expected by the function) or by name (as keyword arguments):\n\n# Positional arguments - order matters\nround(3.14159, 2)\n\n[1] 3.14\n\n# Named arguments - order doesn't matter\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n# Mixed: positional first, then named\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nBest practice: Use positional arguments for the first 1-2 arguments if they are obvious, then switch to named arguments for clarity. For the example above, option three is preferred.\nBasically everything interesting you will do in R involves calling functions, so get comfortable with them.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#data-types",
    "href": "01-introduction.html#data-types",
    "title": "Quantitative Methods and Statistics",
    "section": "Data types",
    "text": "Data types\nComputers need to distinguish different types of data, such as numbers or text, to be able to do anything useful. While the internal structure of, e.g., floating point numbers or strings of characters representing text is quite complex, we here only need to know how R differentiates them.\nThe class function can be used to tell the data type of a variable. Here is a review of some of the most common data types and some of the basic things you can do with them:\n\nNumbers\nR has a distinction between real numbers (class double) and integers (class integer), which in practice you don’t often have to care about:\n\n# Numeric (double)\nx &lt;- 42.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer (less common)\ny &lt;- 42L\nclass(y)\n\n[1] \"integer\"\n\n\nNumbers support all the usual arithmetic operations:\n\n5 + 3\n\n[1] 8\n\n10 - 4\n\n[1] 6\n\n6 * 7\n\n[1] 42\n\n15 / 3\n\n[1] 5\n\n2^3  # exponentiation\n\n[1] 8\n\n# Order of operations matters\n2 + 3 * 4\n\n[1] 14\n\n(2 + 3) * 4\n\n[1] 20\n\n\n\n\nStrings\nStrings are how computers represent text and are of class character in R. If you want to create a variable holding a string, you need to put the text into quotes \":\n\nname &lt;- \"Jakob\"\nclass(name)\n\n[1] \"character\"\n\n\nStrings will be frequently used to specify how a function should go about its business and a common beginner mistake is forgetting the quotes.\nYou can join together different strings with paste or extract bits of a string with substr (among many other useful string processing operations):\n\n# Combine strings\nfirst_name &lt;- \"John\"\nlast_name &lt;- \"Doe\"\npaste0(first_name, \"_\", last_name)\n\n[1] \"John_Doe\"\n\n\n\n# Extract substrings (position-based)\ntext &lt;- \"Statistics\"\nsubstr(text, 1, 4)  # characters 1 through 4\n\n[1] \"Stat\"\n\n\n\n\nLogical\nThe logical (boolean) values true and false have their own datatype, called logical:\n\n# Boolean values\nis_true &lt;- TRUE\nis_false &lt;- FALSE\nclass(is_true)\n\n[1] \"logical\"\n\n\nLogical values typically result from comparing values:\n\n5 &gt; 3\n\n[1] TRUE\n\n5 &lt;= 3\n\n[1] FALSE\n\n\n\n\"phillip\" == \"phillip\"  # equal (note the double ==)\n\n[1] TRUE\n\n\"philipp\" != \"phillipp\" # not equal\n\n[1] TRUE\n\n\nWe can perform boolean logic on boolean values, the result of which is again a boolean value:\n\na &lt;- TRUE\nb &lt;- FALSE\n\na & b  # AND\n\n[1] FALSE\n\na | b  # OR\n\n[1] TRUE\n\n!a     # NOT\n\n[1] FALSE\n\n\nThis is very useful when, e.g., filtering a set of values by multiple conditions. When doing arithmetic, FALSE and TRUE are treated as 0 and 1, respectively. This can be useful for counting the cases that meet a certain condition, which can be achieved using a sum of booleans.\n\n\nVectors\nVectors are the most fundamental container data type and represent collections of elements of the same basic type (numbers, strings, booleans…):\n\n# Numeric vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\nnumbers\n\n[1] 1 2 3 4 5\n\n# Character vector\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\")\nnames\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# Logical vector\nbools &lt;- c(TRUE, FALSE, TRUE)\nbools\n\n[1]  TRUE FALSE  TRUE\n\n\nIf you try to combine different data types, they will be converted to the least common denominator:\n\nc(1, TRUE, \"abc\") # converted to string\n\n[1] \"1\"    \"TRUE\" \"abc\" \n\n\nYou can access individual elements or subsets of vectors with square brackets:\n\nnumbers &lt;- c(10, 20, 30, 40, 50)\n\n# Get single element (R uses 1-based indexing!)\nnumbers[1]  # first element\n\n[1] 10\n\nnumbers[3]  # third element\n\n[1] 30\n\n# Get multiple elements\nnumbers[c(1, 3, 5)]  # first, third, and fifth elements\n\n[1] 10 30 50\n\nnumbers[2:4]         # elements 2 through 4\n\n[1] 20 30 40\n\n\nInstead of specifying the positions of the elements to extract, you can also use a logical vector of the same length to indicate for each element whether to select it or not:\n\nmy_idx &lt;- c(TRUE, FALSE, FALSE, TRUE, TRUE)\n\nnumbers[my_idx]\n\n[1] 10 40 50\n\n\n\n\nFactors\nFactors are a special kind of vector and are Rs way of representing categorical (qualitative) data, i.e. statistical variables which can only take on a set of distinct values:\n\ncolors &lt;- factor(c(\"red\", \"blue\", \"red\", \"green\", \"blue\"))\ncolors\n\n[1] red   blue  red   green blue \nLevels: blue green red\n\n\nBy default, factors represent unordered (nominal) categorical variables. You can also create ordered factors with the ordered flag, in which case you also probably want to give the order of levels explicitly with the levels argument. In addition, you can change the labels used for the different levels of the factor. Here’s an example combining all of this:\n\nsizes &lt;- factor(\n  c(\"S\", \"B\", \"S\", \"M\", \"M\", \"S\"),\n  levels = c(\"S\", \"M\", \"B\"),\n  labels = c(\"small\", \"medium\", \"big\"),\n  ordered = TRUE\n)\nsizes\n\n[1] small  big    small  medium medium small \nLevels: small &lt; medium &lt; big\n\n\nFactors are essential for statistical analysis and will come up frequently when we start doing actual statistics.\n\n\nLists\nVectors are restricted to values of the same basic data type. If you want a container for differently typed elements that you can access by name, lists are the way to go:\n\n# Mixed list\nmy_list &lt;- list(\n  numbers = c(1, 2, 3),\n  names = c(\"A\", \"B\", \"C\"),\n  flag = TRUE\n)\nmy_list\n\n$numbers\n[1] 1 2 3\n\n$names\n[1] \"A\" \"B\" \"C\"\n\n$flag\n[1] TRUE\n\n\nList elements can be accessed either by name using the $ operator or by using double square brackets:\n\nmy_list &lt;- list(\n  numbers = c(1, 2, 3),\n  names = c(\"A\", \"B\", \"C\"),\n  bool = TRUE\n)\n\n# Access by name with $\nmy_list$numbers\n\n[1] 1 2 3\n\nmy_list$flag\n\nNULL\n\n# Access by position or name with [[]]\nmy_list[[1]]        # first element\n\n[1] 1 2 3\n\nmy_list[[\"names\"]]  # by name\n\n[1] \"A\" \"B\" \"C\"\n\n\n\n\nData frames\nData frames are the bread-and-butter data structure for data analysis in R. They are like spreadsheets, with rows representing observations and columns representing variables of potentially different data type. Here’s an example:\n\n# Create a data frame\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(20, 21, 19),\n  passed = c(TRUE, FALSE, TRUE)\n)\nstudents\n\n     name age passed\n1   Alice  20   TRUE\n2     Bob  21  FALSE\n3 Charlie  19   TRUE\n\n\nThe values of a data frame can be accessed in several ways:\n\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(20, 21, 19),\n  passed = c(TRUE, FALSE, TRUE)\n)\n\n\n# Access full columns by name with $ (returns vector)\nstudents$name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\nstudents$age\n\n[1] 20 21 19\n\n\n\n# Access with [row, column]\nstudents[1, 2]       # first row, second columns\n\n[1] 20\n\nstudents[1:3, \"age\"] # rows 1 to 3, 'age' column\n\n[1] 20 21 19\n\nstudents[, 2]        # all rows, second column\n\n[1] 20 21 19\n\n\nIn the future, we will often rely on higher-level functions to process data frames, but the basic options for accessing their data are still good to know.\n\n\nMissing values\nReal data often contains missing values, which R represents as NA:\n\n# Vector with missing values\nages &lt;- c(25, NA, 30, 22, NA)\nages\n\n[1] 25 NA 30 22 NA\n\n# Check for missing values\nis.na(ages)\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nMissing values are contagious, i.e. a summary of a vector with missings will often also return missing (how can you know the mean when some values are not known?). Most functions with this behavior have a flag to ignore missing values, like na.rm:\n\nmean(ages)                # Returns NA\n\n[1] NA\n\nmean(ages, na.rm = TRUE)  # Remove NAs first\n\n[1] 25.66667",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#packages",
    "href": "01-introduction.html#packages",
    "title": "Quantitative Methods and Statistics",
    "section": "Packages",
    "text": "Packages\nWhile R has a big standard library containing many useful functions, one of its biggest strengths is its huge package ecosystem which has packages for all kinds of basic or advanced functionality, from visualization to advanced modeling or GIS.\nIf you want to use a package (also called a library), there are two steps you need to perform:\n\nInstall the package (only needs to be done once, or if you want to update)\nLoad the package (needs to be done every time you start a new session)\n\nHere is how to do these:\n\n# Install once (downloads to your computer)\ninstall.packages(\"ggplot2\")\n\n# Load in each session (makes functions available)\nlibrary(ggplot2)\n\nAfter having succesfully installed a package, you should remove or comment out the install statement or else you will get prompted to reinstall the package whenever you rerun your notebook. Alternatively, you can use the RStudio interface for installing packages.\nOne package (or rather collection of packages) that comprises many useful functions for data science (data wrangling, visualization) is the tidyverse, which you can install and load as follows:\n\n# Install the entire collection\ninstall.packages(\"tidyverse\")\n\n# Load core packages\nlibrary(tidyverse)\n\nThe tidyverse includes packages like dplyr for data manipulation, ggplot2 for visualization, readr for reading data, tidyr for data reshaping, which you could also install and load individually.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#getting-help",
    "href": "01-introduction.html#getting-help",
    "title": "Quantitative Methods and Statistics",
    "section": "Getting help",
    "text": "Getting help\nR has great built-in help, which you can access by writing a ? followed by the name of the function you need help with:\n\n?mean\nhelp(mean)\n\nThe help pages you get with that follow a standard structure:\n\nDescription: What the function does\nUsage: How to call it\nArguments: What inputs it takes\nValue: What it returns\nExamples: Working code examples\n\nWhile they can be a little bit overwhelming at first, they are usually also the most authoritative resource on any piece of code.\nBeyond the technical help page for specific functions, you can also get an overview of a package’s functionality and look for vignettes, which are longer tutorials for certain packages:\n\n# See all functions in a package\nhelp(package = \"dplyr\")\n\n# Browse vignettes (longer tutorials)\nbrowseVignettes(\"dplyr\")\n\n\nLLMs\nLarge Language Models like ChatGPT, Claude, or GitHub Copilot can be very useful for explaining error messages, suggesting code solutions, or learning new functions. However, they can also be dangerous for at least two reasons:\n\nThey can produce wrong (or even malicous) code\nThey can keep you from learning what you need to learn\n\nIn general, LLMs become more useful when you know the basics - at this stage, you will know how to prompt effectively and the chatbot can help you with quickly writing boilerplate code or suggesting different approaches. Try to be a conscious and responsible user and don’t just copy paste blindly.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "02-data-wrangling.html",
    "href": "02-data-wrangling.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "The Gapminder dataset contains information about countries’ life expectancy, population, and GDP per capita over time. It was compiled by a Swedish foundation of the same name and is well-suited for demonstrating data analysis techniques because it is a real and insightful dataset that has just enough complexity to be useful without being overwhelming.\nThe dataset contains the following variables:\n\ncountry : Country names\ncontinent : Continental groupings\nyear : Years from 1952 to 2007 (5 year periods)\nlifeExp : Life expectancy in years\npop : Population count\ngdpPercap : GDP per capita (inflation-adjusted US dollars)",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#csv-files",
    "href": "02-data-wrangling.html#csv-files",
    "title": "Quantitative Methods and Statistics",
    "section": "CSV files",
    "text": "CSV files\nCSV (comma-separated values) is probably the most common data exchange format:\n\ndf_from_csv &lt;- read_csv(\"data/gapminder.csv\")\n\nRows: 1704 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (4): year, lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_from_csv\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nCSV is so common because it is a (deceivingly) simple plain-text format. However, its simplicity means it is somewhat underspecified which can lead to complications. Because of this, reading csv data properly can tricky and messy.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#excel-files",
    "href": "02-data-wrangling.html#excel-files",
    "title": "Quantitative Methods and Statistics",
    "section": "Excel files",
    "text": "Excel files\nExcel files require a separate package, and for pain-free reading the spreadsheet containing the data should follow a plain, single-header table format. If that is the case, the read command looks similarly simple to the csv case:\n\nlibrary(readxl)\ndf_from_excel &lt;- read_excel(\"data/gapminder.xlsx\")\ndf_from_excel\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nThe package also has options for reading data from different spreadsheets or from specified ranges of a spreadsheet.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#parquet-files",
    "href": "02-data-wrangling.html#parquet-files",
    "title": "Quantitative Methods and Statistics",
    "section": "Parquet files",
    "text": "Parquet files\nParquet is a more modern, efficient format that’s becoming increasingly popular:\n\nlibrary(arrow)\ndf_from_parquet &lt;- read_parquet(\"data/gapminder.parquet\")\ndf_from_parquet\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nParquet can be used to read huge datasets very quickly and is less ambiguous about data types than csv or excel files, which makes it a much more suitable data exchange format.\nLet’s verify these all have the same number of rows and columns with dim():\n\ndim(df_from_csv) == dim(df_from_excel) \n\n[1] TRUE TRUE\n\ndim(df_from_csv) == dim(df_from_parquet)\n\n[1] TRUE TRUE\n\n\nFrom now on, we’ll work with the CSV version and call it simply gapminder:\n\ngapminder &lt;- df_from_csv",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#selecting-columns-with-select",
    "href": "02-data-wrangling.html#selecting-columns-with-select",
    "title": "Quantitative Methods and Statistics",
    "section": "Selecting columns with select",
    "text": "Selecting columns with select\nMany real-world datasets come with a huge number of columns, of which you often only need a subset. You can use the select() function to return the input data frame with only the specified columns:\n\n# first argument is data frame, rest are column names to keep\nselect(gapminder, country, year, lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\n\nYou can also select by column position, specify ranges of columns, indicate which columns to drop instead of which to keep, or use helper functions like starts_with:\n\n# Select columns by position\nselect(gapminder, 1:3)\n\n# Drop columns by putting a minus in front\nselect(gapminder, -continent, -pop)\n\n# Select columns that start with a string\nselect(gapminder, starts_with(\"c\"))",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#renaming-columns-with-rename",
    "href": "02-data-wrangling.html#renaming-columns-with-rename",
    "title": "Quantitative Methods and Statistics",
    "section": "Renaming columns with rename",
    "text": "Renaming columns with rename\nColumn names aren’t always what you want. If you want to rename some functions and keep all the others as is, use rename() with the pattern new_name = old_name:\n\nrename(gapminder, life_expectancy = lifeExp, gdp_per_capita = gdpPercap)\n\n# A tibble: 1,704 × 6\n   country     continent  year life_expectancy      pop gdp_per_capita\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1 Afghanistan Asia       1952            28.8  8425333           779.\n 2 Afghanistan Asia       1957            30.3  9240934           821.\n 3 Afghanistan Asia       1962            32.0 10267083           853.\n 4 Afghanistan Asia       1967            34.0 11537966           836.\n 5 Afghanistan Asia       1972            36.1 13079460           740.\n 6 Afghanistan Asia       1977            38.4 14880372           786.\n 7 Afghanistan Asia       1982            39.9 12881816           978.\n 8 Afghanistan Asia       1987            40.8 13867957           852.\n 9 Afghanistan Asia       1992            41.7 16317921           649.\n10 Afghanistan Asia       1997            41.8 22227415           635.\n# ℹ 1,694 more rows\n\n\nIf you instead want to only keep the columns that you want to rename, you can use select with the same renaming pattern:\n\nselect(gapminder, life_expectancy = lifeExp, gdp_per_capita = gdpPercap)\n\n# A tibble: 1,704 × 2\n   life_expectancy gdp_per_capita\n             &lt;dbl&gt;          &lt;dbl&gt;\n 1            28.8           779.\n 2            30.3           821.\n 3            32.0           853.\n 4            34.0           836.\n 5            36.1           740.\n 6            38.4           786.\n 7            39.9           978.\n 8            40.8           852.\n 9            41.7           649.\n10            41.8           635.\n# ℹ 1,694 more rows\n\n\nThe difference is that rename will keep all the other columns while select will only keep the specified ones.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#filtering-rows-with-filter",
    "href": "02-data-wrangling.html#filtering-rows-with-filter",
    "title": "Quantitative Methods and Statistics",
    "section": "Filtering rows with filter",
    "text": "Filtering rows with filter\nWhile select is used to pick out certain columns, filter() keeps rows based on certain specified conditions:\n\nfilter(gapminder, year == 2007)\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp       pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2007    43.8  31889923      975.\n 2 Albania     Europe     2007    76.4   3600523     5937.\n 3 Algeria     Africa     2007    72.3  33333216     6223.\n 4 Angola      Africa     2007    42.7  12420476     4797.\n 5 Argentina   Americas   2007    75.3  40301927    12779.\n 6 Australia   Oceania    2007    81.2  20434176    34435.\n 7 Austria     Europe     2007    79.8   8199783    36126.\n 8 Bahrain     Asia       2007    75.6    708573    29796.\n 9 Bangladesh  Asia       2007    64.1 150448339     1391.\n10 Belgium     Europe     2007    79.4  10392226    33693.\n# ℹ 132 more rows\n\n\nWe can use logical operators like & (logical and) or | (logical or) to specify more complex conditions:\n\n# Keep rows for Europe and year 2007\nfilter(gapminder, year == 2007 & continent == \"Europe\")\n\n# A tibble: 30 × 6\n   country                continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;                  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania                Europe     2007    76.4  3600523     5937.\n 2 Austria                Europe     2007    79.8  8199783    36126.\n 3 Belgium                Europe     2007    79.4 10392226    33693.\n 4 Bosnia and Herzegovina Europe     2007    74.9  4552198     7446.\n 5 Bulgaria               Europe     2007    73.0  7322858    10681.\n 6 Croatia                Europe     2007    75.7  4493312    14619.\n 7 Czech Republic         Europe     2007    76.5 10228744    22833.\n 8 Denmark                Europe     2007    78.3  5468120    35278.\n 9 Finland                Europe     2007    79.3  5238460    33207.\n10 France                 Europe     2007    80.7 61083916    30470.\n# ℹ 20 more rows\n\n# Keep rows for Europe or Asia\nfilter(gapminder, continent == \"Europe\" | continent == \"Asia\")\n\n# A tibble: 756 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 746 more rows\n\n\nIf we only want to keep values appearing in a specified list, we can use the %in% operator:\n\n# Using %in% for multiple values\nfilter(gapminder, country %in% c(\"Germany\", \"France\", \"Italy\"))\n\n# A tibble: 36 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 France  Europe     1952    67.4 42459667     7030.\n 2 France  Europe     1957    68.9 44310863     8663.\n 3 France  Europe     1962    70.5 47124000    10560.\n 4 France  Europe     1967    71.6 49569000    13000.\n 5 France  Europe     1972    72.4 51732000    16107.\n 6 France  Europe     1977    73.8 53165019    18293.\n 7 France  Europe     1982    74.9 54433565    20294.\n 8 France  Europe     1987    76.3 55630100    22066.\n 9 France  Europe     1992    77.5 57374179    24704.\n10 France  Europe     1997    78.6 58623428    25890.\n# ℹ 26 more rows",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#dropping-missing-values-with-drop_na",
    "href": "02-data-wrangling.html#dropping-missing-values-with-drop_na",
    "title": "Quantitative Methods and Statistics",
    "section": "Dropping missing values with drop_na",
    "text": "Dropping missing values with drop_na\nThe gapminder dataset is clean, but real data often has missing values. drop_na() removes rows with missing data:\n\n# Create some missing data for demonstration\ngapminder_with_na &lt;- gapminder\ngapminder_with_na$lifeExp[1:5] &lt;- NA\n\n# Drop rows with any missing values\ndrop_na(gapminder_with_na)\n\n# A tibble: 1,699 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1977    38.4 14880372      786.\n 2 Afghanistan Asia       1982    39.9 12881816      978.\n 3 Afghanistan Asia       1987    40.8 13867957      852.\n 4 Afghanistan Asia       1992    41.7 16317921      649.\n 5 Afghanistan Asia       1997    41.8 22227415      635.\n 6 Afghanistan Asia       2002    42.1 25268405      727.\n 7 Afghanistan Asia       2007    43.8 31889923      975.\n 8 Albania     Europe     1952    55.2  1282697     1601.\n 9 Albania     Europe     1957    59.3  1476505     1942.\n10 Albania     Europe     1962    64.8  1728137     2313.\n# ℹ 1,689 more rows\n\n\nWe can also specify to only drop rows with missing values in certain columns:\n\ndrop_na(gapminder_with_na, lifeExp)\n\n# A tibble: 1,699 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1977    38.4 14880372      786.\n 2 Afghanistan Asia       1982    39.9 12881816      978.\n 3 Afghanistan Asia       1987    40.8 13867957      852.\n 4 Afghanistan Asia       1992    41.7 16317921      649.\n 5 Afghanistan Asia       1997    41.8 22227415      635.\n 6 Afghanistan Asia       2002    42.1 25268405      727.\n 7 Afghanistan Asia       2007    43.8 31889923      975.\n 8 Albania     Europe     1952    55.2  1282697     1601.\n 9 Albania     Europe     1957    59.3  1476505     1942.\n10 Albania     Europe     1962    64.8  1728137     2313.\n# ℹ 1,689 more rows\n\n\nNote that for statistical analysis, simply dropping rows can be dangerous because it might bias the results (why are these values missing? For which observations are they missing?). Accordingly, you should always make sure that you at least understand the extent and the pattern of missing values in your dataset before dropping incomplete rows.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#removing-duplicate-rows-with-distinct",
    "href": "02-data-wrangling.html#removing-duplicate-rows-with-distinct",
    "title": "Quantitative Methods and Statistics",
    "section": "Removing duplicate rows with distinct",
    "text": "Removing duplicate rows with distinct\ndistinct() removes duplicate rows with respect to the specified columns. E.g., if we only wanted a list of continents and countries, we could do the following:\n\n# Get unique combinations of continent and year\ndistinct(gapminder, continent, country)\n\n# A tibble: 142 × 2\n   continent country    \n   &lt;chr&gt;     &lt;chr&gt;      \n 1 Asia      Afghanistan\n 2 Europe    Albania    \n 3 Africa    Algeria    \n 4 Africa    Angola     \n 5 Americas  Argentina  \n 6 Oceania   Australia  \n 7 Europe    Austria    \n 8 Asia      Bahrain    \n 9 Asia      Bangladesh \n10 Europe    Belgium    \n# ℹ 132 more rows\n\n\nNote that this drops all the other columns. If you want to keep the first value for the other columns, specify .keep_all = TRUE (note the dot in the argument name):\n\ndistinct(gapminder, country, .keep_all = TRUE)\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Albania     Europe     1952    55.2  1282697     1601.\n 3 Algeria     Africa     1952    43.1  9279525     2449.\n 4 Angola      Africa     1952    30.0  4232095     3521.\n 5 Argentina   Americas   1952    62.5 17876956     5911.\n 6 Australia   Oceania    1952    69.1  8691212    10040.\n 7 Austria     Europe     1952    66.8  6927772     6137.\n 8 Bahrain     Asia       1952    50.9   120447     9867.\n 9 Bangladesh  Asia       1952    37.5 46886859      684.\n10 Belgium     Europe     1952    68    8730405     8343.\n# ℹ 132 more rows",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#sorting-rows-with-arrange",
    "href": "02-data-wrangling.html#sorting-rows-with-arrange",
    "title": "Quantitative Methods and Statistics",
    "section": "Sorting rows with arrange",
    "text": "Sorting rows with arrange\narrange() sorts rows by one or more columns:\n\n# Sort by life expectancy\narrange(gapminder, lifeExp)\n\n# A tibble: 1,704 × 6\n   country      continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Rwanda       Africa     1992    23.6 7290203      737.\n 2 Afghanistan  Asia       1952    28.8 8425333      779.\n 3 Gambia       Africa     1952    30    284320      485.\n 4 Angola       Africa     1952    30.0 4232095     3521.\n 5 Sierra Leone Africa     1952    30.3 2143249      880.\n 6 Afghanistan  Asia       1957    30.3 9240934      821.\n 7 Cambodia     Asia       1977    31.2 6978607      525.\n 8 Mozambique   Africa     1952    31.3 6446316      469.\n 9 Sierra Leone Africa     1957    31.6 2295678     1004.\n10 Burkina Faso Africa     1952    32.0 4469979      543.\n# ℹ 1,694 more rows\n\n\nWe can also sort by descending order using the desc() helper function and of course sort by multiple columns, in the specified order:\n\n# Sort by life expectancy in descending order\narrange(gapminder, desc(lifeExp))\n\n# A tibble: 1,704 × 6\n   country          continent  year lifeExp       pop gdpPercap\n   &lt;chr&gt;            &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Japan            Asia       2007    82.6 127467972    31656.\n 2 Hong Kong, China Asia       2007    82.2   6980412    39725.\n 3 Japan            Asia       2002    82   127065841    28605.\n 4 Iceland          Europe     2007    81.8    301931    36181.\n 5 Switzerland      Europe     2007    81.7   7554661    37506.\n 6 Hong Kong, China Asia       2002    81.5   6762476    30209.\n 7 Australia        Oceania    2007    81.2  20434176    34435.\n 8 Spain            Europe     2007    80.9  40448191    28821.\n 9 Sweden           Europe     2007    80.9   9031088    33860.\n10 Israel           Asia       2007    80.7   6426679    25523.\n# ℹ 1,694 more rows\n\n# Sort by multiple columns\narrange(gapminder, year, desc(lifeExp))\n\n# A tibble: 1,704 × 6\n   country        continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Norway         Europe     1952    72.7  3327728    10095.\n 2 Iceland        Europe     1952    72.5   147962     7268.\n 3 Netherlands    Europe     1952    72.1 10381988     8942.\n 4 Sweden         Europe     1952    71.9  7124673     8528.\n 5 Denmark        Europe     1952    70.8  4334000     9692.\n 6 Switzerland    Europe     1952    69.6  4815000    14734.\n 7 New Zealand    Oceania    1952    69.4  1994794    10557.\n 8 United Kingdom Europe     1952    69.2 50430000     9980.\n 9 Australia      Oceania    1952    69.1  8691212    10040.\n10 Canada         Americas   1952    68.8 14785584    11367.\n# ℹ 1,694 more rows",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#making-new-columns-with-mutate",
    "href": "02-data-wrangling.html#making-new-columns-with-mutate",
    "title": "Quantitative Methods and Statistics",
    "section": "Making new columns with mutate",
    "text": "Making new columns with mutate\nmutate() creates new columns or modifies existing ones. E.g., to create a column with the total GDP (instead of per-capita GDP), we can create a new column containing the product of gdpPercap and pop:\n\nmutate(gapminder, total_gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap    total_gdp\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n# ℹ 1,694 more rows\n\n\nYou can create multiple columns at once and reference newly created columns:\n\nmutate(gapminder,\n       total_gdp = gdpPercap * pop,\n       gdp_billions = total_gdp / 1e9,\n       gdp_trillions = gdp_billions / 1000)\n\n# A tibble: 1,704 × 9\n   country     continent  year lifeExp      pop gdpPercap total_gdp gdp_billions\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.   6.57e 9         6.57\n 2 Afghanistan Asia       1957    30.3  9240934      821.   7.59e 9         7.59\n 3 Afghanistan Asia       1962    32.0 10267083      853.   8.76e 9         8.76\n 4 Afghanistan Asia       1967    34.0 11537966      836.   9.65e 9         9.65\n 5 Afghanistan Asia       1972    36.1 13079460      740.   9.68e 9         9.68\n 6 Afghanistan Asia       1977    38.4 14880372      786.   1.17e10        11.7 \n 7 Afghanistan Asia       1982    39.9 12881816      978.   1.26e10        12.6 \n 8 Afghanistan Asia       1987    40.8 13867957      852.   1.18e10        11.8 \n 9 Afghanistan Asia       1992    41.7 16317921      649.   1.06e10        10.6 \n10 Afghanistan Asia       1997    41.8 22227415      635.   1.41e10        14.1 \n# ℹ 1,694 more rows\n# ℹ 1 more variable: gdp_trillions &lt;dbl&gt;\n\n\nA particularly useful helper function for creating new columns is case_when(), which let’s you specify conditional logic by specifying condition ~ value pairs. For example, we could assign continents to different groups:\n\nmutate(gapminder,\n   continent_group = case_when(\n     continent == \"Europe\" | continent == \"America\" ~ \"West\",\n     continent == \"Asia\" ~ \"East\",\n     continent == \"Africa\" ~ \"South\",\n     TRUE ~ \"Other\"\n   )\n)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap continent_group\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          \n 1 Afghanistan Asia       1952    28.8  8425333      779. East           \n 2 Afghanistan Asia       1957    30.3  9240934      821. East           \n 3 Afghanistan Asia       1962    32.0 10267083      853. East           \n 4 Afghanistan Asia       1967    34.0 11537966      836. East           \n 5 Afghanistan Asia       1972    36.1 13079460      740. East           \n 6 Afghanistan Asia       1977    38.4 14880372      786. East           \n 7 Afghanistan Asia       1982    39.9 12881816      978. East           \n 8 Afghanistan Asia       1987    40.8 13867957      852. East           \n 9 Afghanistan Asia       1992    41.7 16317921      649. East           \n10 Afghanistan Asia       1997    41.8 22227415      635. East           \n# ℹ 1,694 more rows",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#grouped-summaries-with-group_by-and-summarize",
    "href": "02-data-wrangling.html#grouped-summaries-with-group_by-and-summarize",
    "title": "Quantitative Methods and Statistics",
    "section": "Grouped summaries with group_by and summarize",
    "text": "Grouped summaries with group_by and summarize\nOne of the most powerful patterns is grouping data and calculating summaries for each group. The first step is grouping a data frame:\n\n# Average life expectancy by continent\ngapminder_grouped &lt;- group_by(gapminder, continent)\ngapminder_grouped\n\n# A tibble: 1,704 × 6\n# Groups:   continent [5]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nThe result doesn’t look much different than the original data frame. But if you look closely, you will see that it is marked as being grouped by continent, with a total of [5] groups. The next step is to compute one or more summaries for each of the groups:\n\nsummarize(gapminder_grouped, \n          count = n(), \n          mean_lifeexp = mean(lifeExp))\n\n# A tibble: 5 × 3\n  continent count mean_lifeexp\n  &lt;chr&gt;     &lt;int&gt;        &lt;dbl&gt;\n1 Africa      624         48.9\n2 Americas    300         64.7\n3 Asia        396         60.1\n4 Europe      360         71.9\n5 Oceania      24         74.3\n\n\nWe will come back to this soon in the context of more complex analysis pipelines.\nBecause counting rows by group is such a common practice, there is a function count() which is a shortcut for group_by() + summarize(n = n()):\n\n# Count observations by continent\ncount(gapminder, continent)\n\n# A tibble: 5 × 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa      624\n2 Americas    300\n3 Asia        396\n4 Europe      360\n5 Oceania      24\n\n# Count with sorting\ncount(gapminder, continent, sort = TRUE)\n\n# A tibble: 5 × 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa      624\n2 Asia        396\n3 Europe      360\n4 Americas    300\n5 Oceania      24\n\n# Count by multiple variables\ncount(gapminder, continent, year)\n\n# A tibble: 60 × 3\n   continent  year     n\n   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n 1 Africa     1952    52\n 2 Africa     1957    52\n 3 Africa     1962    52\n 4 Africa     1967    52\n 5 Africa     1972    52\n 6 Africa     1977    52\n 7 Africa     1982    52\n 8 Africa     1987    52\n 9 Africa     1992    52\n10 Africa     1997    52\n# ℹ 50 more rows\n\n\nMany of the tidyverse functions respect grouping structure, such as filter(). If we group by continent, for example, we can see that the filter below uses the maximum within each group and returns the row with the highest GDP for each continent:\n\ngroup_by(gapminder, continent) |&gt;\nfilter(gdpPercap == max(gdpPercap))\n\n# A tibble: 5 × 6\n# Groups:   continent [5]\n  country       continent  year lifeExp       pop gdpPercap\n  &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Australia     Oceania    2007    81.2  20434176    34435.\n2 Kuwait        Asia       1957    58.0    212846   113523.\n3 Libya         Africa     1977    57.4   2721783    21951.\n4 Norway        Europe     2007    80.2   4627926    49357.\n5 United States Americas   2007    78.2 301139947    42952.\n\n\nBecause this kind of operation is common, there is even a specialized function called slice_max() which lets you pick out the top n values according to some variable. Here’s an example of how to get the top 3 most populous countries for each continent in the year 2007:\n\ngapminder_2007 &lt;- filter(gapminder, year == 2007)\ndf_grouped &lt;- group_by(gapminder_2007, continent)\nslice_max(df_grouped, pop, n = 3)\n\n# A tibble: 14 × 6\n# Groups:   continent [5]\n   country       continent  year lifeExp        pop gdpPercap\n   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 Nigeria       Africa     2007    46.9  135031164     2014.\n 2 Egypt         Africa     2007    71.3   80264543     5581.\n 3 Ethiopia      Africa     2007    52.9   76511887      691.\n 4 United States Americas   2007    78.2  301139947    42952.\n 5 Brazil        Americas   2007    72.4  190010647     9066.\n 6 Mexico        Americas   2007    76.2  108700891    11978.\n 7 China         Asia       2007    73.0 1318683096     4959.\n 8 India         Asia       2007    64.7 1110396331     2452.\n 9 Indonesia     Asia       2007    70.6  223547000     3541.\n10 Germany       Europe     2007    79.4   82400996    32170.\n11 Turkey        Europe     2007    71.8   71158647     8458.\n12 France        Europe     2007    80.7   61083916    30470.\n13 Australia     Oceania    2007    81.2   20434176    34435.\n14 New Zealand   Oceania    2007    80.2    4115771    25185.\n\n\nNotice also how the output is still grouped. If you want to drop groups so that operations which come afterwards are ungrouped, many functions have arguments for that (look at their help page).",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#long-and-wide-format-data-with-pivot_wider-and-pivot_longer",
    "href": "02-data-wrangling.html#long-and-wide-format-data-with-pivot_wider-and-pivot_longer",
    "title": "Quantitative Methods and Statistics",
    "section": "Long and wide format data with pivot_wider and pivot_longer",
    "text": "Long and wide format data with pivot_wider and pivot_longer\nData can be organized in “long” format (one observation per row) or “wide” format (multiple observations per row spread across different columns). If that seems a little abstract, here’s a picture showing the difference:\n\n\n\nLong vs. wide format data (source: graph gallery)\n\n\nHere is how we can switch between the two representations using the pivot_wider() function:\n\ngapminder_wide &lt;- pivot_wider(gapminder,\n                              id_cols = country,\n                              names_from = year, \n                              values_from = lifeExp)\n\ngapminder_wide\n\n# A tibble: 142 × 13\n   country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997`\n   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghan…   28.8   30.3   32.0   34.0   36.1   38.4   39.9   40.8   41.7   41.8\n 2 Albania   55.2   59.3   64.8   66.2   67.7   68.9   70.4   72     71.6   73.0\n 3 Algeria   43.1   45.7   48.3   51.4   54.5   58.0   61.4   65.8   67.7   69.2\n 4 Angola    30.0   32.0   34     36.0   37.9   39.5   39.9   39.9   40.6   41.0\n 5 Argent…   62.5   64.4   65.1   65.6   67.1   68.5   69.9   70.8   71.9   73.3\n 6 Austra…   69.1   70.3   70.9   71.1   71.9   73.5   74.7   76.3   77.6   78.8\n 7 Austria   66.8   67.5   69.5   70.1   70.6   72.2   73.2   74.9   76.0   77.5\n 8 Bahrain   50.9   53.8   56.9   59.9   63.3   65.6   69.1   70.8   72.6   73.9\n 9 Bangla…   37.5   39.3   41.2   43.5   45.3   46.9   50.0   52.8   56.0   59.4\n10 Belgium   68     69.2   70.2   70.9   71.4   72.8   73.9   75.4   76.5   77.5\n# ℹ 132 more rows\n# ℹ 2 more variables: `2002` &lt;dbl&gt;, `2007` &lt;dbl&gt;\n\n\nHere, id_cols refers to the column(s) that identify the new rows, names_from specifies from which column the new column names in the wide table should be taken, and values_from specifies from which column the values to populate the new columns should be taken.\nTo convert back to long format, we can use the pivot_longer() function:\n\npivot_longer(gapminder_wide, -country)\n\n# A tibble: 1,704 × 3\n   country     name  value\n   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan 1952   28.8\n 2 Afghanistan 1957   30.3\n 3 Afghanistan 1962   32.0\n 4 Afghanistan 1967   34.0\n 5 Afghanistan 1972   36.1\n 6 Afghanistan 1977   38.4\n 7 Afghanistan 1982   39.9\n 8 Afghanistan 1987   40.8\n 9 Afghanistan 1992   41.7\n10 Afghanistan 1997   41.8\n# ℹ 1,694 more rows\n\n\nHere, it is simpler to just specify the columns which not to stack on top of each other, which in our case is just the country column used as identifier in the last step. You can see that the grouping column and the value column just have the generic names name and value, which you can change with the names_to and values_to arguments.\nThe tidyverse generally prefers long format for analysis (e.g. visualization), but sometimes wide format is more efficient for storage or more useful for presentation.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#joining-tables-on-a-common-index",
    "href": "02-data-wrangling.html#joining-tables-on-a-common-index",
    "title": "Quantitative Methods and Statistics",
    "section": "Joining tables on a common index",
    "text": "Joining tables on a common index\nOften data is spread across multiple tables. Let’s create a second dataset with hypothetical continent identifiers:\n\ncontinent_codes &lt;- data.frame(\n  continent = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\", \"Oceania\"),\n  continent_code = c(\"AF\", \"AM\", \"AS\", \"EU\", \"OC\")\n)\n\ncontinent_codes\n\n  continent continent_code\n1    Africa             AF\n2  Americas             AM\n3      Asia             AS\n4    Europe             EU\n5   Oceania             OC\n\n\nWe can add this identifier column to our main data frame by matching the continent column in both datasets. This operation is generally called a join - here we perform a left join, which means we want to keep all entries in the first (left) data frame, even if there is no matching identifier in the second (right) data frame. In that case, missing values will be inserted for the rows without a match.\nTo perform the left join, use the left_join() function with the two datasets and by specifying the common column with the by keyword:\n\nleft_join(gapminder, continent_codes, by = \"continent\")\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap continent_code\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         \n 1 Afghanistan Asia       1952    28.8  8425333      779. AS            \n 2 Afghanistan Asia       1957    30.3  9240934      821. AS            \n 3 Afghanistan Asia       1962    32.0 10267083      853. AS            \n 4 Afghanistan Asia       1967    34.0 11537966      836. AS            \n 5 Afghanistan Asia       1972    36.1 13079460      740. AS            \n 6 Afghanistan Asia       1977    38.4 14880372      786. AS            \n 7 Afghanistan Asia       1982    39.9 12881816      978. AS            \n 8 Afghanistan Asia       1987    40.8 13867957      852. AS            \n 9 Afghanistan Asia       1992    41.7 16317921      649. AS            \n10 Afghanistan Asia       1997    41.8 22227415      635. AS            \n# ℹ 1,694 more rows\n\n\nIf the common identifier column is called differently in the two datasets, you can pass something like this to by: c(\"left_name\" = \"right_name\") .",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html",
    "href": "04-descriptive-univariate.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "While the gapminder dataset has served us well so far, it is time to mix it up a bit. For this session, we’ll work with CO₂ emissions data from Our World in Data, which contains annual per-capita CO₂ emissions for countries worldwide. In addition to the emissions data, we will also get population data and GDP from the same source.\nQuite neatly, we can just directly download the file from their web page with the read_csv() function:\n\n# library(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\nemissions_raw &lt;- read_csv(\"https://ourworldindata.org/grapher/co-emissions-per-capita.csv\")\npopulation_raw &lt;- read_csv(\"https://ourworldindata.org/grapher/population-unwpp.csv\")\n\nLet’s rename some variables and filter out non-country entities (such as the European Union or other aggregates) to prepare our data for further use:\n\nemissions &lt;- select(\n        emissions_raw,\n        entity = Entity,\n        code = Code,\n        year = Year,\n        emissions = `Annual CO₂ emissions (per capita)`\n    ) |&gt;\n    # filter out non-country entities\n    filter(!is.na(code))\n\n# Look at the dataset structure\nglimpse(emissions)\n\nRows: 22,991\nColumns: 4\n$ entity    &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ code      &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG…\n$ year      &lt;dbl&gt; 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, …\n$ emissions &lt;dbl&gt; 0.001992146, 0.010837197, 0.011625335, 0.011467511, 0.013123…\n\n\nWe do the same for population:\n\npopulation &lt;- select(\n        population_raw,\n        entity = Entity,\n        code = Code,\n        year = Year,\n        population = `Population - Sex: all - Age: all - Variant: estimates`\n    ) |&gt;\n    filter(!is.na(code))\n\nFinally, these two datasets look like they could be combined into a single data frame with a country per row and columns for population and emissions. We achieve this with a join:\n\ndf &lt;- left_join(emissions, population) # joins on all common name columns\n\nJoining with `by = join_by(entity, code, year)`\n\ndf &lt;- filter(df, year == 2020)\ndf\n\n# A tibble: 215 × 5\n   entity              code   year emissions population\n   &lt;chr&gt;               &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Afghanistan         AFG    2020     0.297   39068977\n 2 Albania             ALB    2020     1.64     2871950\n 3 Algeria             DZA    2020     3.82    44042094\n 4 Andorra             AND    2020     4.92       77397\n 5 Angola              AGO    2020     0.494   33451139\n 6 Anguilla            AIA    2020     9.61       14864\n 7 Antigua and Barbuda ATG    2020     6.74       91864\n 8 Argentina           ARG    2020     3.65    45191960\n 9 Armenia             ARM    2020     2.35     2890893\n10 Aruba               ABW    2020     7.64      107411\n# ℹ 205 more rows\n\n\nLet’s save this data frame in a parquet file so that we can reuse it and don’t have to download it every time (which is generally considered good digital citizenship):\n\narrow::write_parquet(df, \"data/emissions-population.parquet\")\n\nHere, the arrow::write_parquet means ’use the function write_parquet from the arrow package, without loading the package explicitly.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#histograms",
    "href": "04-descriptive-univariate.html#histograms",
    "title": "Quantitative Methods and Statistics",
    "section": "Histograms",
    "text": "Histograms\nHistograms show the distribution of a continuous variable by dividing it into bins (usually of equal size) and counting observations in each bin. We can plot one using the now familiar ggplot2 machinery:\n\np &lt;- ggplot(data = df, aes(x = emissions)) +\n    geom_histogram(color = \"white\", fill = \"tomato\", bins = 15) +\n    labs(\n        x = \"per-capita emissions (tons of CO₂)\", \n        y = \"Number of countries\"\n    )\n\np\n\n\n\n\n\n\n\n\nThe histogram tells us that the distribution is heavily right-skewed, with most countries having relatively low per-capita emissions and a few countries exhibiting very high per-capita emissions. Next to being an interesting fact in and of itself (guess which country has the highest per-capita emissions), it is also relevant for the choice of summary statistics.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#measures-of-central-tendency",
    "href": "04-descriptive-univariate.html#measures-of-central-tendency",
    "title": "Quantitative Methods and Statistics",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nThere are different things we might want to say about the distribution shown by the histogram above, for example what its ‘center’ is. There is a variety of approaches to achieve this, each with different properties and trade-offs, making them suitable for different data and scenarios.\n\nThe arithmetic mean\nThe arithmetic mean (or simply “mean”) is the sum of all values divided by the number of observations. It is probably the most well known and most widely used measure of central tendency, even if not appropriate. We can call it on our emissions variable by accessing the data frame column with $ :\n\nmean(df$emissions)\n\n[1] 4.4796\n\n\nWe can see that the mean per-capita emissions across countries in 2020 is around 4.48 tons of CO2.\nThere is however a problem with the mean we just computed: It weighs all countries equally. In this case, it would however be much more appropriate to compute a weighted mean which weighs each country proportionally to its population size. We can do that with the weighted.mean() function, using the population column in our data frame as weights:\n\nwm &lt;- weighted.mean(df$emissions, df$population, na.rm = TRUE)\nwm\n\n[1] 4.401505\n\n\nWe can see that the weighted mean is a bit smaller, indicating that some of the high-emissions countries received lower weights (due to smaller populations) than some of the low-emissions countries. Here, we also had to specify na.rm = TRUE to remove missing values (NAs) before computing the mean.\nWe can also add a line to our histo{gram plot to indicate our mean (or any measure of central tendency):\n\np + geom_vline(xintercept = wm, color = \"blue\", linetype = \"dashed\", linewidth = 1.5)\n\n\n\n\n\n\n\n\n\n\nThe geometric mean\nWhen our data represents ratios or rates of change, the arithmetic mean is no longer appropriate. We should instead use the geometric mean which correctly handles the multiplicative logic inherent to the data.\nWe might think about, e.g. the average growth rate of per-capita emissions in Germany over the last 20 years. We can obtain this growth series as follows, using the lag() helper function:\n\ngrowth_de &lt;- emissions |&gt;\n    filter(entity == \"Germany\" & \n           year &gt;= 2000 & \n           year &lt;= 2020) |&gt;\n    arrange(year) |&gt;\n    mutate(growth_rate = emissions / lag(emissions))\n\nThere is no built-in function in R to compute the geometric mean, but it turns out that the geometric mean is just the exponential of the mean of the logs of the series, i.e. \\[GM(x) = \\exp \\left( \\frac{1}{n} \\sum_{i=1}^n \\log(x_i)\\right)\\]\nwhich we can compute easily ourselves with a single line of R:\n\nexp(mean(log(growth_de$growth_rate), na.rm = TRUE))\n\n[1] 0.9827034\n\n\nThe result is around 0.98, which tells us that, on average, the per-capita emissions in Germany have been decreasing by about 2% a year over the last 20 years.\n\n\nThe median\nThe median is the middle value when observations are ordered from smallest to largest. Because it does not care about the actual values in the tails (the ends) of a distribution, it is less sensitive to outliers than the mean.\n\nmedian(df$emissions, na.rm = TRUE)\n\n[1] 2.966649\n\n\nAt 2.97, the median is quite a bit lower than the mean. This is to be expected given the shape of our distribution: In right-skewed distributions like this one, the mean is always larger than the median because it’s pulled towards the extreme values.\nWe can add lines for both the median and the arithmetic mean to our plot to see how they compare:\n\np + geom_vline(xintercept = median(df$emissions), linewidth = 2, linetype = \"dashed\") + \n    geom_vline(xintercept = mean(df$emissions), linewidth = 2)\n\n\n\n\n\n\n\n\nAs for the mean, there is a weighted version of the median, which we however need to pull from the ggstats package (install it before you load it):\n\nlibrary(ggstats)\nwmed &lt;- weighted.median(df$emissions, df$population, na.rm = TRUE)\nwmed\n\n[1] 4.44839\n\n\n\n\nQuantiles\nThe median is a special kind of quantile, which divide the sorted data into equally-sized groups. The median is the 0.5 quantile (or 50th percentile) and divides the data into two groups, with half the data below the median value and half above it. But we just as well compute other quantiles, such as the 0.25 quantile (a quarter of the data below it , three quarters above) or the 0.99 quantile (just one percent of the data above it). Here is how to compute quantiles in R:\n\nqs &lt;- quantile(df$emissions, probs = c(0.33, 0.66, 0.9, 0.99), na.rm = TRUE)\n\nWe could again add them to our plot:\n\np + geom_vline(xintercept = qs)\n\n\n\n\n\n\n\n\nLooking at the plot, we see that 90% of countries have per-capita emissions of around 10t or less.\nIf we want a quick summary of a numerical variable and mean, we can use the summary() function, which gives us the variables minimum and maximum, the variables mean and the 25%, 50% (i.e. the median), and 75% quantiles (called the `quartiles). It also reports the number of missing values.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#measures-of-dispersion",
    "href": "04-descriptive-univariate.html#measures-of-dispersion",
    "title": "Quantitative Methods and Statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nWhile measures of central tendency tell us something about the ‘center’ of the distribution, measures of dispersion try to capture how ‘spread out’ (or concentrated) the data are. Just as for measures of central tendencies, there are many ways to measure dispersion, each again with their own characteristics and trade-offs.\n\nThe range\nThe simplest measure of dispersion is the range, which is just the difference between the maximum and minimum values. In R, the range() function gives us the minimum and maximum values of a variable, the difference of which we can get with diff():\n\ndf$emissions |&gt; range() |&gt; diff()\n\n[1] 36.52764\n\n\nThe range is useful to understand the empirical limits of the studied variable but of course very sensitive to outliers and so not all that useful as a dispersion measure.\n\n\nThe interquartile range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles, representing the spread of the middle 50% of the data. As such it is much more robust to outliers than the range. You can compute it with the IQR() function:\n\nIQR(df$emissions)\n\n[1] 4.580719\n\n\n\n\nVariance and standard deviation\nVariance measures the average squared deviation from the mean. The standard deviation is the square root of the variance and is in the same units as the original data, making it much more interpretable than the variance. You can easily compute both:\n\nvar(df$emissions)\n\n[1] 28.33959\n\nsd(df$emissions)\n\n[1] 5.323494\n\n\nVariance and standard deviation have nice theoretical properties and analogies in probability theory, making them the most widely used dispersion measures for numerical data. Just as for the mean, however, they are sensitive to outliers and can thus give misleading or surprising results.\n\n\nCoefficient of variation\nThe coefficient of variation (CV) is the ratio of standard deviation to the mean. It is useful for comparing variability across different datasets or across groups. We can compute it using the functions we already know:\n\nsd(df$emissions) / mean(df$emissions)\n\n[1] 1.188386\n\n\nBy itself, this is not particularly interesting. But we could use it to compare the spread of emissions in Europe and Africa, for example, which have very different means.\nHere are African and European country codes so we can split our dataset:\n\neurope &lt;- c(\n    \"ALA\", \"ALB\", \"AND\", \"AUT\", \"BLR\", \"BEL\", \"BIH\", \"BGR\", \"HRV\", \"CYP\",\n    \"CZE\", \"DNK\", \"EST\", \"FRO\", \"FIN\", \"FRA\", \"DEU\", \"GIB\", \"GRC\", \"GGY\",\n    \"HUN\", \"ISL\", \"IRL\", \"IMN\", \"ITA\", \"JEY\", \"XKX\", \"LVA\", \"LIE\", \"LTU\",\n    \"LUX\", \"MKD\", \"MLT\", \"MDA\", \"MCO\", \"MNE\", \"NLD\", \"NOR\", \"POL\", \"PRT\",\n    \"ROU\", \"RUS\", \"SMR\", \"SRB\", \"SCG\", \"SVK\", \"SVN\", \"ESP\", \"SJM\", \"SWE\",\n    \"CHE\", \"UKR\", \"GBR\", \"VAT\"\n)\n\nafrica &lt;- c(\n    \"DZA\", \"AGO\", \"BEN\", \"BWA\", \"BFA\", \"BDI\", \"CMR\", \"CPV\", \"CAF\", \"TCD\",\n    \"COM\", \"COD\", \"DJI\", \"EGY\", \"GNQ\", \"ERI\", \"ETH\", \"GAB\", \"GMB\", \"GHA\",\n    \"GIN\", \"GNB\", \"CIV\", \"KEN\", \"LSO\", \"LBR\", \"LBY\", \"MDG\", \"MWI\", \"MLI\",\n    \"MRT\", \"MUS\", \"MYT\", \"MAR\", \"MOZ\", \"NAM\", \"NER\", \"NGA\", \"COG\", \"REU\",\n    \"RWA\", \"SHN\", \"STP\", \"SEN\", \"SYC\", \"SLE\", \"SOM\", \"ZAF\", \"SSD\", \"SDN\",\n    \"SWZ\", \"TZA\", \"TGO\", \"TUN\", \"UGA\", \"ESH\", \"ZMB\", \"ZWE\"\n)\n\nBased on this, we can use our data wrangling techniques to split the data and compute our summary for the two groups:\n\ndf |&gt; mutate(\n    continent = case_when(\n        code %in% europe ~ \"Europe\",\n        code %in% africa ~ \"Africa\",\n        TRUE ~ \"Other\"\n    )) |&gt;\n    group_by(continent) |&gt;\n    summarize(\n        mean = mean(emissions),\n        sd = sd(emissions),\n        vc = sd / mean)\n\n# A tibble: 3 × 4\n  continent  mean    sd    vc\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Africa     1.05  1.47 1.40 \n2 Europe     5.90  2.60 0.442\n3 Other      5.57  6.43 1.15 \n\n\nBased on this, we can see that while the standard deviation is much larger in Europe in absolute terms, relative to the mean Europe is much more homogeneous in its per-capita emissions than Africa.\n\n\nMedian absolute deviation (MAD)\nSimilarly to the median being an alternative to the mean that is more robust against outliers, the median absolute deviation (MAD) is a robust dispersion measure, computed as the median of the absolute differences to the median. We could compute it ourselves but R has a built-in function:\n\nmad(df$emissions)\n\n[1] 3.210136\n\n\nAs before, we see that MAD is lower compared to the standard deviation because it is less sensitive to large outliers.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#bar-plots-and-frequency-tables",
    "href": "04-descriptive-univariate.html#bar-plots-and-frequency-tables",
    "title": "Quantitative Methods and Statistics",
    "section": "Bar plots and frequency tables",
    "text": "Bar plots and frequency tables\nThe most basic description of categorical data is a frequency table, showing how many observations fall into each category:\n\ndf_cat |&gt; count(emission_level)\n\n# A tibble: 3 × 2\n  emission_level     n\n  &lt;ord&gt;          &lt;int&gt;\n1 Low               88\n2 Medium            95\n3 High              32\n\n\nWe can also get proportions instead of raw counts by dividing our counts by the sum of counts across all categories:\n\nprops &lt;- df_cat |&gt; \n    count(emission_level) |&gt;\n    mutate(proportion = n / sum(n))\n\nprops\n\n# A tibble: 3 × 3\n  emission_level     n proportion\n  &lt;ord&gt;          &lt;int&gt;      &lt;dbl&gt;\n1 Low               88      0.409\n2 Medium            95      0.442\n3 High              32      0.149\n\n\nAbout 40% of countries fall into the low emissions category, while only about 14% are in the high category.\nWe can easily visualize a frequency table as a bar chart as an equivalent to the histogram for quantitative variables:\n\nggplot(df_cat, aes(x = emission_level)) +\n    geom_bar(fill = \"tomato\") +\n    labs(x = \"Emission Level\", y = \"Number of Countries\")",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#categorical-measures-of-central-tendency",
    "href": "04-descriptive-univariate.html#categorical-measures-of-central-tendency",
    "title": "Quantitative Methods and Statistics",
    "section": "Categorical measures of central tendency",
    "text": "Categorical measures of central tendency\n\nMedian for ordinal data\nWhen we have ordinal categorical data (categories with a meaningful order), we can use the median as a measure of central tendency because it only relies on the order of values but not the actual values themselves (in R, we still need to convert to a numerical representation, though):\n\nmedian(as.numeric(df_cat$emission_level))\n\n[1] 2\n\n\nMaybe unsurprisingly giving the above bar chart, the median category is ‘Medium’.\n\n\nMode for nominal data\nFor nominal (unorderded categorical) data, we cannot use the median any more. The most useful representation of the ‘center’ is in this case just the most frequently occurring category, called the mode. While R doesn’t have a built-in mode function, it is just the top value of our sorted counts:\n\ndf_cat |&gt; count(emission_level, sort = TRUE) |&gt; head(1)\n\n# A tibble: 1 × 2\n  emission_level     n\n  &lt;ord&gt;          &lt;int&gt;\n1 Medium            95\n\n\nThe mode of our emission level variable is again “Medium”, although in this case it would be more useful to report the median category anyways, as discussed above.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#categorical-measures-of-dispersion",
    "href": "04-descriptive-univariate.html#categorical-measures-of-dispersion",
    "title": "Quantitative Methods and Statistics",
    "section": "Categorical measures of dispersion",
    "text": "Categorical measures of dispersion\n\nEntropy\nFor measuring dispersion in categorical data, we have to measure how concentrated the distribution, with maximum concentration achieved when all observations fall into a single group. One measure which achieves this is entropy (having origins in physics and information theory), which reaches its maximum when all categories are equally likely, and its minimum when all observations fall into a single category. The entropy is mathematically defined as minus the sum of the probability-weighted log of each probability/proportion, which we can normalize to the (0, 1) range by dividing by the log of the number of categories:\n\\[\nH = -\\frac{1}{\\log(k)} \\sum_{i=1}^k p_i * \\log(p_i)\n\\]\nwhere \\(p_i\\) is the proportion of category \\(i\\). There is no built in function in base R but we can easily compute it ourselves:\n\n-sum(props$proportion * log(props$proportion)) / log(nrow(props))\n\n[1] 0.9193826\n\n\nThe resulting value is 0.92, telling us that the distribution is relatively but not quite perfectly even. A more interesting finding would probably come from comparing against a reference value (e.g., compare the spread of today’s distribution with 20 years ago).",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]