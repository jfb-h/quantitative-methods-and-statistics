[
  {
    "objectID": "06-statistical-models.html",
    "href": "06-statistical-models.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "So far, we have contented ourselves with describing the data we have. Ultimately, we are however often more interested in the processes underlying the data we see, the so called data generating process (DGP). The DGP is unobserved and possibly (probably) complex. It comprises the mechanisms we ultimately want to learn about, but also other, unexplained or unmodelled sources of variation, as well as artifacts of our data collection procedures, such as sampling variation and measurement error.\nTo understand this process, we need to model the DGP, typically with heavily simplified models. The ultimate goal here is to disentangle the different sources of variation that induce uncertainty into our data-based claims and inferences. Assessing (i.e., quantifying) this uncertainty is an important step because it tells us how confident we can be about our findings.",
    "crumbs": [
      "Statistical modeling",
      "Model building"
    ]
  },
  {
    "objectID": "06-statistical-models.html#probability-distributions",
    "href": "06-statistical-models.html#probability-distributions",
    "title": "Quantitative Methods and Statistics",
    "section": "Probability distributions",
    "text": "Probability distributions\nA probability distribution exhaustively distributes a unitless quantity (‘probability’) that is normalized to an amount of 1 across a set of outcomes of a process which we regard as random. We could do this by hand, but more typical is the use of a mathematical rule for distributing probability, such as encoded in the normal distribution used above. These rules typically have parameters which influence the distributional behavior, such as the evenness or concentration of the generated values.\nThe normal distribution is just one of many parametric families of probability distributions that are used in statistical models. Distributions are usefully distinguished by whether they model a categorical, discrete, or continuous random variable, whether there are constraints on their support (the range of values with non-zero probability under the distribution), and their shape (e.g., whether they have ‘fat tails’, or are symmetric or multimodal).\nHere is an overview of some common distributions and their characteristics:\n\n\n\n\n\n\n\n\n\nDistribution\nData type\nSupport\nParameters\n\n\n\n\nNormal\nContinuous\n\\(\\mathbb{R} \\quad (-\\infty, \\infty)\\)\n\\(\\mu\\) (location)  \\(\\sigma\\) (scale)\n\n\nStudent-t\nContinuous\n\\(\\mathbb{R} \\quad (-\\infty, \\infty)\\)\n\\(\\nu\\) (df)  \\(\\mu\\) (location)  \\(\\sigma\\) (scale)\n\n\nExponential\nContinuous\n\\([0,\\infty)\\)\n\\(\\lambda\\) (rate)\n\n\nBeta\nContinuous\n\\((0,1)\\)\n\\(\\mu\\) (mean)  \\(\\phi\\) (precision)\n\n\nBinomial\nDiscrete\n\\(\\{0,\\dots,n\\}\\)\n\\(n\\) (trials)  \\(\\theta\\) (success prob)\n\n\nPoisson\nDiscrete\n\\(\\mathbb{N}_0 \\quad \\{0, 1, 2, ...\\}\\)\n\\(\\lambda\\) (mean)\n\n\nNegative Binomial\nDiscrete\n\\(\\mathbb{N}_0 \\quad \\{0, 1, 2, ...\\}\\)\n\\(\\mu\\) (mean)  \\(\\phi\\) (scale)\n\n\nBernoulli\nCategorical\n\\(\\{0,1\\}\\)\n\\(\\theta\\) (success prob)\n\n\nCategorical\nCategorical\n\\(\\{1,\\dots,K\\}\\)\n\\(\\theta_1,\\dots,\\theta_K\\) (category probs)\n\n\n\nProbability theory is an incredibly rich but also notoriously tricky topic for study. Given the applied scope of this class, we will not delve too deep, as even only basic probability theory can easily fill a full semester. Nevertheless, much can already be achieved with only a rudimentary and more intuitive than technical understanding of what’s going on, and we will pick out what we need as we go along.",
    "crumbs": [
      "Statistical modeling",
      "Model building"
    ]
  },
  {
    "objectID": "06-statistical-models.html#exercises",
    "href": "06-statistical-models.html#exercises",
    "title": "Quantitative Methods and Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nSpecify a model of heights for a population consisting of both men and women, using the fact that that the average man in Germany is 178cm tall. Write it down in notation.\n\n\nSimulate 100 men and 100 women from the model and visualize the data alongside the underlying distributions.\n\n\nMake up a simple model for some outcome that is of interest to you / that you’re knowledgeable about by picking a distribution and specifying parameters. Check the data that the model produces for plausibility.",
    "crumbs": [
      "Statistical modeling",
      "Model building"
    ]
  },
  {
    "objectID": "06-statistical-models.html#sources-of-uncertainty",
    "href": "06-statistical-models.html#sources-of-uncertainty",
    "title": "Quantitative Methods and Statistics",
    "section": "Sources of uncertainty",
    "text": "Sources of uncertainty\nThe data we have typically doesn’t represent the full population about which we would like to say something. It might be difficult, for example, to measure the heights of more than 40 million women in Germany. Luckily, this is typically also not necessary. Under certain conditions, we can get a very good approximiation of population characteristics by looking at only a (comparatively) small sample. The simplest way to get a sample which is representative of the underlying population is to pick units randomly (this might be easier said than done, as it might be difficult to circumvent selection biases).\nWhile random sampling ensures a representative sample, it also produces uncertainty in our estimates: Every time you would pick a different random set of participants, you would get a slightly different result. We can again use simulation to better understand this process. First, let’s simulate a large population of individuals (say, 100 000 women):\n\nset.seed(123) # for reproducible RNG\npop &lt;- rnorm(n = 100000, 166, 7)\nmean(pop)\n\n[1] 166.0068\n\n\nThe ‘true’ mean in this population is 166.0068. Now, draw 50 people randomly from that population (without replacement, i.e., we can only draw everybody once):\n\nsamp &lt;- sample(pop, 50, replace = FALSE)\n\nFinally, compute the sample mean:\n\nmean(samp)\n\n[1] 166.5651\n\n\nThe mean we get from the sample is close to the population mean, but not the same, as we saw before. Infinitely drawing samples and computing some summary would yield the sampling distribution of that summary.\nTo get a sense of the sampling distribution of the mean, we can repeat this process many (e.g., 1000) times, each time recording the mean and then looking at the distribution of means that we get from our replicated samples:\n\nmeans &lt;- replicate(1000, mean(sample(pop, 50, replace = FALSE)))\n\nWe can again visualize the sampling distribution we get like this with a histogram:\n\nggplot(data.frame(means), aes(x = means)) + \n    geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\nThe standard deviation of the sampling distribution is called the standard error, and summarizes the variability of an estimate (here, the sample mean):\n\nsd(means)\n\n[1] 0.9731581\n\n\nYou can also see that the sampling distribution looks suspiciously like a normal distribution, and in many (but not all) cases the sampling distribution can indeed be treated as (approximately) normal, as per the central limit theorem.\nOne of the most important basic principles of statistics is that the variability in an estimate will decrease with more data. We can again establish that fact by simulation. Compare the standard error above to the standard error we get when we draw samples of 500 simulated women instead of 50:\n\nmeans500 &lt;- replicate(1000, mean(sample(pop, 500, replace = FALSE)))\n\nLet’s visualise the two sampling distributions together (using a density plot, which is just a fancy, smooth histogram):\n\ndf_sd &lt;- data.frame(\n    sampsize = factor(rep(c(\"n=50\", \"n=500\"), each = 1000)),\n    mean = c(means, means500)\n)\n\nsd(means500)\n\n[1] 0.3113594\n\nggplot(df_sd, aes(mean)) +\n    geom_density(fill = \"tomato\", alpha = 0.4) +\n    facet_wrap(~sampsize, nrow = 2)\n\n\n\n\n\n\n\n\nAs expected, the standard error for the larger sample is much lower than for the smaller sample.\nWhile drawing random samples from a larger population is common (especially in social sciences), it is not the only source of this kind of variability. In many cases where one effectively observes the full population (consider, e.g., a survey in an an organization where almost everybody participated), it still makes sense to think about and model variability, as there are other sources of randomness in data collection. Measurement is often difficult and imprecise and subject to error and fluctuations, which in absence of better explanations needs to be considered as random. The basic principle is the same: The precise data are considered random and thus uncertain, and we need to propagate that uncertainty to the quantities of interest that we compute from the data to draw inferences about the underlying population or process.\nIn real-world settings, quantifying uncertainty by repeatedly compiling new datasets is not sensible - the whole purpose of quantifying uncertainty is to allow inferences from limited data! Instead we rely on assumptions about the DGP combined with different estimation procedures to obtain summaries of uncertainty based on a single dataset. This is a large and complex topic in statistics and there are competing approaches, some of which will be discussed in more detail in this class.",
    "crumbs": [
      "Statistical modeling",
      "Model building"
    ]
  },
  {
    "objectID": "06-statistical-models.html#aleatoric-and-epistemic-uncertainty",
    "href": "06-statistical-models.html#aleatoric-and-epistemic-uncertainty",
    "title": "Quantitative Methods and Statistics",
    "section": "Aleatoric and epistemic uncertainty",
    "text": "Aleatoric and epistemic uncertainty\nWe have now seen two ways in which our inferences can be uncertain:\n\nThere is variation not explained by our model but instead assumed to be random. In our example, this is represented by the variability in the normal distribution (measured by the \\(\\sigma\\) parameter, i.e., the distribution’s standard deviation). Our model is just not powerful enough to reason about why different women have different heights; For this, we would need a better model. This kind of systemic uncertainty in our predictions about any specific outcome is called aleatoric uncertainty (alea is Latin for dice).\nNext to the intrinsic uncertainty of our model, there is uncertainty coming from the limited availability / precision of the data. The sampling uncertainty in the unknown parameters discussed above falls into this category. We call this uncertainty epistemic uncertainty. As we have seen, and unlike aleatoric uncertainty, this can be reduced by gathering more data, as our estimates become more and more precise.",
    "crumbs": [
      "Statistical modeling",
      "Model building"
    ]
  },
  {
    "objectID": "06-statistical-models.html#exercises-1",
    "href": "06-statistical-models.html#exercises-1",
    "title": "Quantitative Methods and Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nSimulate a large population for the model you made up before.\n\n\nRepeatedly draw samples of 10, 100, and 1000 individuals and compute the sampling distribution of the mean (or another quantity of interest).\n\n\nDescribe and visualize the sampling distribution.",
    "crumbs": [
      "Statistical modeling",
      "Model building"
    ]
  },
  {
    "objectID": "06-statistical-models-old.html",
    "href": "06-statistical-models-old.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "Why build statistical models?\nStatistics is about learning from data, especially data that is incomplete or has other imperfections, such as measurement error (i.e., almost all scientific data). The data themselves are of course not the ultimate goal - they measure (more or less crudely) abstract constructs that play a role in our scientific models of the world, or rather some aspect of it. So while we can directly learn from descriptions of the data, often we use the data to provide evidence for or against our ideas and theories of how the world works, and, vice versa, use models to understand how the data came to be the way they are.\n\n\n\nThe big picture of statistical inference (Kass 2011)\n\n\nBeyond the simple visual and numerical methods for describing data we have seen so far, statistical models can be helpful tools for description of complex multivariate data, e.g. by disentangling otherwise difficult to spot trends. However, the simple descriptives also treat only implicitly two important aspects of the scientific process which we ultimately want to make more explicit: causality and uncertainty.\nImplicit or explicit claims of causality are an inherent feature of scientific theories (TODO: Why, how?). If applied carefully (and if the stars align), statistical models can be used to test expectations from theory in both controlled (i.e., experimental) and uncontrolled (i.e., observational) settings.\nIn the same vein, uncertainty is an inherent feature of all scientific production of knowledge from empirical observation. TODO: why, how? Statistical models will not rid us of these epistemic shades of grey, but they can be used to quantify and assess uncertainty so that decisions about how to weigh evidence can be made more explicitly.\n\n\n\nModeling the data-generating process\nSpecifying a statistical model of the processes that procuded the observed data, called the data generating process (DGP), forces us to be explicit about our assumptions regarding statistical relationships and sources of uncertainty. As such, the process of statistical modeling can establish transparency in a way simple descriptive statistics cannot.\nStatistical models adapt two important characteristics which scientific models also exhibit: abstraction and assumptions.\n\nAbstraction\nAssumptions\nRandomness\nGenerative models\nTheoretical and statistical models\nDGP comprises theorized process (the signal) and the measurement process\n\n\n\nModeling randomness and uncertainty\n\n\nReferences\nKass, R. E. (2011). Statistical Inference: The Big Picture. Statistical Science, 26(1). https://doi.org/10.1214/10-STS337\nDeffner, D., Fedorova, N., Andrews, J., & McElreath, R. (2024). Bridging theory and data: A computational workflow for cultural evolution. Proceedings of the National Academy of Sciences, 121(48), e2322887121. https://doi.org/10.1073/pnas.2322887121"
  },
  {
    "objectID": "13-multilevel-models.html#nonlinear-relationships",
    "href": "13-multilevel-models.html#nonlinear-relationships",
    "title": "Quantitative Methods and Statistics",
    "section": "Nonlinear relationships",
    "text": "Nonlinear relationships"
  },
  {
    "objectID": "13-multilevel-models.html#smoothing-splines",
    "href": "13-multilevel-models.html#smoothing-splines",
    "title": "Quantitative Methods and Statistics",
    "section": "Smoothing splines",
    "text": "Smoothing splines"
  },
  {
    "objectID": "13-multilevel-models.html#informational-efficiency-and-partial-pooling",
    "href": "13-multilevel-models.html#informational-efficiency-and-partial-pooling",
    "title": "Quantitative Methods and Statistics",
    "section": "Informational efficiency and partial pooling",
    "text": "Informational efficiency and partial pooling"
  },
  {
    "objectID": "13-multilevel-models.html#varying-intercepts",
    "href": "13-multilevel-models.html#varying-intercepts",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts",
    "text": "Varying intercepts"
  },
  {
    "objectID": "13-multilevel-models.html#varying-intercepts-and-coefficients",
    "href": "13-multilevel-models.html#varying-intercepts-and-coefficients",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts and coefficients",
    "text": "Varying intercepts and coefficients"
  },
  {
    "objectID": "13-multilevel-models.html#higher-level-predictors",
    "href": "13-multilevel-models.html#higher-level-predictors",
    "title": "Quantitative Methods and Statistics",
    "section": "Higher-level predictors",
    "text": "Higher-level predictors"
  },
  {
    "objectID": "11-generalized-linear-models.html#nonlinear-relationships",
    "href": "11-generalized-linear-models.html#nonlinear-relationships",
    "title": "Quantitative Methods and Statistics",
    "section": "Nonlinear relationships",
    "text": "Nonlinear relationships"
  },
  {
    "objectID": "11-generalized-linear-models.html#smoothing-splines",
    "href": "11-generalized-linear-models.html#smoothing-splines",
    "title": "Quantitative Methods and Statistics",
    "section": "Smoothing splines",
    "text": "Smoothing splines"
  },
  {
    "objectID": "11-generalized-linear-models.html#informational-efficiency-and-partial-pooling",
    "href": "11-generalized-linear-models.html#informational-efficiency-and-partial-pooling",
    "title": "Quantitative Methods and Statistics",
    "section": "Informational efficiency and partial pooling",
    "text": "Informational efficiency and partial pooling"
  },
  {
    "objectID": "11-generalized-linear-models.html#varying-intercepts",
    "href": "11-generalized-linear-models.html#varying-intercepts",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts",
    "text": "Varying intercepts"
  },
  {
    "objectID": "11-generalized-linear-models.html#varying-intercepts-and-coefficients",
    "href": "11-generalized-linear-models.html#varying-intercepts-and-coefficients",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts and coefficients",
    "text": "Varying intercepts and coefficients"
  },
  {
    "objectID": "11-generalized-linear-models.html#higher-level-predictors",
    "href": "11-generalized-linear-models.html#higher-level-predictors",
    "title": "Quantitative Methods and Statistics",
    "section": "Higher-level predictors",
    "text": "Higher-level predictors"
  },
  {
    "objectID": "09-experiments.html#estimating-causal-effects-from-experiments",
    "href": "09-experiments.html#estimating-causal-effects-from-experiments",
    "title": "Quantitative Methods and Statistics",
    "section": "Estimating causal effects from experiments",
    "text": "Estimating causal effects from experiments"
  },
  {
    "objectID": "09-experiments.html#estimating-causal-effects-from-observational-studies",
    "href": "09-experiments.html#estimating-causal-effects-from-observational-studies",
    "title": "Quantitative Methods and Statistics",
    "section": "Estimating causal effects from observational studies",
    "text": "Estimating causal effects from observational studies"
  },
  {
    "objectID": "08-linear-regression.html#one-binary-and-one-numerical-predictor",
    "href": "08-linear-regression.html#one-binary-and-one-numerical-predictor",
    "title": "Quantitative Methods and Statistics",
    "section": "One binary and one numerical predictor",
    "text": "One binary and one numerical predictor"
  },
  {
    "objectID": "08-linear-regression.html#two-numerical-predictors",
    "href": "08-linear-regression.html#two-numerical-predictors",
    "title": "Quantitative Methods and Statistics",
    "section": "Two numerical predictors",
    "text": "Two numerical predictors"
  },
  {
    "objectID": "08-linear-regression.html#more-than-two-predictors",
    "href": "08-linear-regression.html#more-than-two-predictors",
    "title": "Quantitative Methods and Statistics",
    "section": "More than two predictors",
    "text": "More than two predictors"
  },
  {
    "objectID": "08-linear-regression.html#interaction-effects",
    "href": "08-linear-regression.html#interaction-effects",
    "title": "Quantitative Methods and Statistics",
    "section": "Interaction effects",
    "text": "Interaction effects"
  },
  {
    "objectID": "02-data-wrangling.html",
    "href": "02-data-wrangling.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "The Gapminder dataset contains information about countries’ life expectancy, population, and GDP per capita over time. It was compiled by a Swedish foundation of the same name and is well-suited for demonstrating data analysis techniques because it is a real and insightful dataset that has just enough complexity to be useful without being overwhelming.\nThe dataset contains the following variables:\n\ncountry : Country names\ncontinent : Continental groupings\nyear : Years from 1952 to 2007 (5 year periods)\nlifeExp : Life expectancy in years\npop : Population count\ngdpPercap : GDP per capita (inflation-adjusted US dollars)",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#csv-files",
    "href": "02-data-wrangling.html#csv-files",
    "title": "Quantitative Methods and Statistics",
    "section": "CSV files",
    "text": "CSV files\nCSV (comma-separated values) is probably the most common data exchange format:\n\ndf_from_csv &lt;- read_csv(\"data/gapminder.csv\")\n\nRows: 1704 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (4): year, lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_from_csv\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nCSV is so common because it is a (deceivingly) simple plain-text format. However, its simplicity means it is somewhat underspecified which can lead to complications. Because of this, reading csv data properly can tricky and messy.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#excel-files",
    "href": "02-data-wrangling.html#excel-files",
    "title": "Quantitative Methods and Statistics",
    "section": "Excel files",
    "text": "Excel files\nExcel files require a separate package, and for pain-free reading the spreadsheet containing the data should follow a plain, single-header table format. If that is the case, the read command looks similarly simple to the csv case:\n\nlibrary(readxl)\ndf_from_excel &lt;- read_excel(\"data/gapminder.xlsx\")\ndf_from_excel\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nThe package also has options for reading data from different spreadsheets or from specified ranges of a spreadsheet.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#parquet-files",
    "href": "02-data-wrangling.html#parquet-files",
    "title": "Quantitative Methods and Statistics",
    "section": "Parquet files",
    "text": "Parquet files\nParquet is a more modern, efficient format that’s becoming increasingly popular:\n\nlibrary(arrow)\ndf_from_parquet &lt;- read_parquet(\"data/gapminder.parquet\")\ndf_from_parquet\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nParquet can be used to read huge datasets very quickly and is less ambiguous about data types than csv or excel files, which makes it a much more suitable data exchange format.\nLet’s verify these all have the same number of rows and columns with dim():\n\ndim(df_from_csv) == dim(df_from_excel) \n\n[1] TRUE TRUE\n\ndim(df_from_csv) == dim(df_from_parquet)\n\n[1] TRUE TRUE\n\n\nFrom now on, we’ll work with the CSV version and call it simply gapminder:\n\ngapminder &lt;- df_from_csv",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#subsetting-and-sorting-data-frames",
    "href": "02-data-wrangling.html#subsetting-and-sorting-data-frames",
    "title": "Quantitative Methods and Statistics",
    "section": "Subsetting and sorting data frames",
    "text": "Subsetting and sorting data frames\n\nSelecting columns with select\nMany real-world datasets come with a huge number of columns, of which you often only need a subset. You can use the select() function to return the input data frame with only the specified columns:\n\n# first argument is data frame, rest are column names to keep\nselect(gapminder, country, year, lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\n\nYou can also select by column position, specify ranges of columns, indicate which columns to drop instead of which to keep, or use helper functions like starts_with:\n\n# Select columns by position\nselect(gapminder, 1:3)\n\n# Drop columns by putting a minus in front\nselect(gapminder, -continent, -pop)\n\n# Select columns that start with a string\nselect(gapminder, starts_with(\"c\"))\n\n\n\nRenaming columns with rename\nColumn names aren’t always what you want. If you want to rename some functions and keep all the others as is, use rename() with the pattern new_name = old_name:\n\nrename(gapminder, life_expectancy = lifeExp, gdp_per_capita = gdpPercap)\n\n# A tibble: 1,704 × 6\n   country     continent  year life_expectancy      pop gdp_per_capita\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1 Afghanistan Asia       1952            28.8  8425333           779.\n 2 Afghanistan Asia       1957            30.3  9240934           821.\n 3 Afghanistan Asia       1962            32.0 10267083           853.\n 4 Afghanistan Asia       1967            34.0 11537966           836.\n 5 Afghanistan Asia       1972            36.1 13079460           740.\n 6 Afghanistan Asia       1977            38.4 14880372           786.\n 7 Afghanistan Asia       1982            39.9 12881816           978.\n 8 Afghanistan Asia       1987            40.8 13867957           852.\n 9 Afghanistan Asia       1992            41.7 16317921           649.\n10 Afghanistan Asia       1997            41.8 22227415           635.\n# ℹ 1,694 more rows\n\n\nIf you instead want to only keep the columns that you want to rename, you can use select with the same renaming pattern:\n\nselect(gapminder, life_expectancy = lifeExp, gdp_per_capita = gdpPercap)\n\n# A tibble: 1,704 × 2\n   life_expectancy gdp_per_capita\n             &lt;dbl&gt;          &lt;dbl&gt;\n 1            28.8           779.\n 2            30.3           821.\n 3            32.0           853.\n 4            34.0           836.\n 5            36.1           740.\n 6            38.4           786.\n 7            39.9           978.\n 8            40.8           852.\n 9            41.7           649.\n10            41.8           635.\n# ℹ 1,694 more rows\n\n\nThe difference is that rename will keep all the other columns while select will only keep the specified ones.\n\n\nFiltering rows with filter\nWhile select is used to pick out certain columns, filter() keeps rows based on certain specified conditions:\n\nfilter(gapminder, year == 2007)\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp       pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2007    43.8  31889923      975.\n 2 Albania     Europe     2007    76.4   3600523     5937.\n 3 Algeria     Africa     2007    72.3  33333216     6223.\n 4 Angola      Africa     2007    42.7  12420476     4797.\n 5 Argentina   Americas   2007    75.3  40301927    12779.\n 6 Australia   Oceania    2007    81.2  20434176    34435.\n 7 Austria     Europe     2007    79.8   8199783    36126.\n 8 Bahrain     Asia       2007    75.6    708573    29796.\n 9 Bangladesh  Asia       2007    64.1 150448339     1391.\n10 Belgium     Europe     2007    79.4  10392226    33693.\n# ℹ 132 more rows\n\n\nNote how the output data frame has 142 rows, i.e., one for each country.\nWe can use logical operators like & (logical and) or | (logical or) to specify more complex conditions:\n\n# Keep rows for Europe and year 2007\nfilter(gapminder, year == 2007 & continent == \"Europe\")\n\n# Keep rows for Europe or Asia\nfilter(gapminder, continent == \"Europe\" | continent == \"Asia\")\n\nIf we only want to keep values appearing in a specified list, we can use the %in% operator:\n\n# Using %in% for multiple values\nfilter(gapminder, country %in% c(\"Germany\", \"France\", \"Italy\"))\n\nThis is equivalent to specifying a chain of OR statements (via |), but shorter and more convenient because we can use a pre-specified vector of allowed values.\n\n\nDropping missing values with drop_na\nThe gapminder dataset is clean, but real data often has missing values. While missing data requires care and additional assumptions to deal with potentially biased results, a first exploratory step is often to just drop rows with missing data, which we can achieve with drop_na():\n\n# Create some missing data for demonstration\ngapminder_with_na &lt;- gapminder\ngapminder_with_na$lifeExp[1:5] &lt;- NA\n\n# Drop rows with any missing values\ndrop_na(gapminder_with_na)\n\n# A tibble: 1,699 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1977    38.4 14880372      786.\n 2 Afghanistan Asia       1982    39.9 12881816      978.\n 3 Afghanistan Asia       1987    40.8 13867957      852.\n 4 Afghanistan Asia       1992    41.7 16317921      649.\n 5 Afghanistan Asia       1997    41.8 22227415      635.\n 6 Afghanistan Asia       2002    42.1 25268405      727.\n 7 Afghanistan Asia       2007    43.8 31889923      975.\n 8 Albania     Europe     1952    55.2  1282697     1601.\n 9 Albania     Europe     1957    59.3  1476505     1942.\n10 Albania     Europe     1962    64.8  1728137     2313.\n# ℹ 1,689 more rows\n\n\nWe can also specify to only drop rows with missing values in certain columns:\n\ndrop_na(gapminder_with_na, lifeExp)\n\nYou should however keep in mind that simply dropping rows with missing values can be dangerous because it might bias the results (why are these values missing? For which observations are they missing?). Accordingly, you should always make sure that you at least understand the extent and the pattern of missing values in your dataset before dropping incomplete rows.\n\n\nRemoving duplicate rows with distinct\ndistinct() removes duplicate rows with respect to the specified columns. E.g., if we only wanted a list of continents and countries, we could do the following:\n\n# Get unique combinations of continent and year\ndistinct(gapminder, continent, country)\n\n# A tibble: 142 × 2\n   continent country    \n   &lt;chr&gt;     &lt;chr&gt;      \n 1 Asia      Afghanistan\n 2 Europe    Albania    \n 3 Africa    Algeria    \n 4 Africa    Angola     \n 5 Americas  Argentina  \n 6 Oceania   Australia  \n 7 Europe    Austria    \n 8 Asia      Bahrain    \n 9 Asia      Bangladesh \n10 Europe    Belgium    \n# ℹ 132 more rows\n\n\nNote that this drops all the other columns. If you want to keep the first value for the other columns, specify .keep_all = TRUE (note the dot in the argument name):\n\ndistinct(gapminder, country, .keep_all = TRUE)\n\n\n\nSorting rows with arrange\narrange() sorts rows by one or more columns:\n\n# Sort by life expectancy\narrange(gapminder, lifeExp)\n\n# A tibble: 1,704 × 6\n   country      continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Rwanda       Africa     1992    23.6 7290203      737.\n 2 Afghanistan  Asia       1952    28.8 8425333      779.\n 3 Gambia       Africa     1952    30    284320      485.\n 4 Angola       Africa     1952    30.0 4232095     3521.\n 5 Sierra Leone Africa     1952    30.3 2143249      880.\n 6 Afghanistan  Asia       1957    30.3 9240934      821.\n 7 Cambodia     Asia       1977    31.2 6978607      525.\n 8 Mozambique   Africa     1952    31.3 6446316      469.\n 9 Sierra Leone Africa     1957    31.6 2295678     1004.\n10 Burkina Faso Africa     1952    32.0 4469979      543.\n# ℹ 1,694 more rows\n\n\nWe can also sort by descending order using the desc() helper function and of course sort by multiple columns, in the specified order:\n\n# Sort by life expectancy in descending order\narrange(gapminder, desc(lifeExp))\n\n# Sort by multiple columns\narrange(gapminder, year, desc(lifeExp))\n\n\n\nExercises\n\nLoad the gapminder dataset and explore its structure using glimpse().\n\n\nSelect only the country, year, and population columns.\n\n\nFilter for data from Asian or American countries in the year 2007.\n\n\nWhich are the top 5 most populous countries in 2002?",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#grouping-mutating-and-summarizing-data-frames",
    "href": "02-data-wrangling.html#grouping-mutating-and-summarizing-data-frames",
    "title": "Quantitative Methods and Statistics",
    "section": "Grouping, mutating and summarizing data frames",
    "text": "Grouping, mutating and summarizing data frames\n\nMaking new columns with mutate\nmutate() creates new columns or modifies existing ones. E.g., to create a column with the total GDP (instead of per-capita GDP), we can create a new column containing the product of gdpPercap and pop:\n\nmutate(gapminder, total_gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap    total_gdp\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n# ℹ 1,694 more rows\n\n\nYou can create multiple columns at once and in the process reference newly created columns:\n\nmutate(gapminder,\n       total_gdp = gdpPercap * pop,\n       gdp_billions = total_gdp / 1e9,\n       gdp_trillions = gdp_billions / 1000)\n\n# A tibble: 1,704 × 9\n   country     continent  year lifeExp      pop gdpPercap total_gdp gdp_billions\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.   6.57e 9         6.57\n 2 Afghanistan Asia       1957    30.3  9240934      821.   7.59e 9         7.59\n 3 Afghanistan Asia       1962    32.0 10267083      853.   8.76e 9         8.76\n 4 Afghanistan Asia       1967    34.0 11537966      836.   9.65e 9         9.65\n 5 Afghanistan Asia       1972    36.1 13079460      740.   9.68e 9         9.68\n 6 Afghanistan Asia       1977    38.4 14880372      786.   1.17e10        11.7 \n 7 Afghanistan Asia       1982    39.9 12881816      978.   1.26e10        12.6 \n 8 Afghanistan Asia       1987    40.8 13867957      852.   1.18e10        11.8 \n 9 Afghanistan Asia       1992    41.7 16317921      649.   1.06e10        10.6 \n10 Afghanistan Asia       1997    41.8 22227415      635.   1.41e10        14.1 \n# ℹ 1,694 more rows\n# ℹ 1 more variable: gdp_trillions &lt;dbl&gt;\n\n\nA particularly useful helper function for creating new columns is case_when(), which let’s you specify conditional logic by specifying condition ~ value pairs. For example, we could assign continents to different groups:\n\nmutate(gapminder,\n   continent_group = case_when(\n     continent == \"Europe\" | continent == \"America\" ~ \"West\",\n     continent == \"Asia\" ~ \"East\",\n     continent == \"Africa\" ~ \"South\",\n     TRUE ~ \"Other\"\n   )\n)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap continent_group\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          \n 1 Afghanistan Asia       1952    28.8  8425333      779. East           \n 2 Afghanistan Asia       1957    30.3  9240934      821. East           \n 3 Afghanistan Asia       1962    32.0 10267083      853. East           \n 4 Afghanistan Asia       1967    34.0 11537966      836. East           \n 5 Afghanistan Asia       1972    36.1 13079460      740. East           \n 6 Afghanistan Asia       1977    38.4 14880372      786. East           \n 7 Afghanistan Asia       1982    39.9 12881816      978. East           \n 8 Afghanistan Asia       1987    40.8 13867957      852. East           \n 9 Afghanistan Asia       1992    41.7 16317921      649. East           \n10 Afghanistan Asia       1997    41.8 22227415      635. East           \n# ℹ 1,694 more rows\n\n\n\n\nGrouped summaries with group_by and summarize\nOne of the most powerful patterns is grouping data and calculating summaries for each group. The first step is grouping a data frame:\n\n# Average life expectancy by continent\ngapminder_grouped &lt;- group_by(gapminder, continent)\ngapminder_grouped\n\n# A tibble: 1,704 × 6\n# Groups:   continent [5]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nThe result doesn’t look much different than the original data frame. But if you look closely, you will see that it is marked as being grouped by continent, with a total of [5] groups. The next step is to compute one or more summaries for each of the groups:\n\nsummarize(gapminder_grouped, \n          count = n(), \n          mean_lifeexp = mean(lifeExp))\n\n# A tibble: 5 × 3\n  continent count mean_lifeexp\n  &lt;chr&gt;     &lt;int&gt;        &lt;dbl&gt;\n1 Africa      624         48.9\n2 Americas    300         64.7\n3 Asia        396         60.1\n4 Europe      360         71.9\n5 Oceania      24         74.3\n\n\nWe will come back to this soon in the context of more complex analysis pipelines.\nBecause counting rows by group is such a common practice, there is a function count() which is a shortcut for group_by() + summarize(n = n()):\n\n# Count observations by continent\ncount(gapminder, continent)\n\n# Count with sorting\ncount(gapminder, continent, sort = TRUE)\n\n# Count by multiple variables\ncount(gapminder, continent, year)\n\nMany of the tidyverse functions respect grouping structure, such as filter(). If we group by continent, for example, we can see that the filter below uses the maximum within each group and returns the row with the highest GDP for each continent:\n\ngroup_by(gapminder, continent) |&gt;\nfilter(gdpPercap == max(gdpPercap))\n\n# A tibble: 5 × 6\n# Groups:   continent [5]\n  country       continent  year lifeExp       pop gdpPercap\n  &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Australia     Oceania    2007    81.2  20434176    34435.\n2 Kuwait        Asia       1957    58.0    212846   113523.\n3 Libya         Africa     1977    57.4   2721783    21951.\n4 Norway        Europe     2007    80.2   4627926    49357.\n5 United States Americas   2007    78.2 301139947    42952.\n\n\nBecause this kind of operation is common, there is even a specialized function called slice_max() which lets you pick out the top n values according to some variable. Here’s an example of how to get the top 3 most populous countries for each continent in the year 2007:\n\ngapminder_2007 &lt;- filter(gapminder, year == 2007)\ndf_grouped &lt;- group_by(gapminder_2007, continent)\nslice_max(df_grouped, pop, n = 3)\n\n# A tibble: 14 × 6\n# Groups:   continent [5]\n   country       continent  year lifeExp        pop gdpPercap\n   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 Nigeria       Africa     2007    46.9  135031164     2014.\n 2 Egypt         Africa     2007    71.3   80264543     5581.\n 3 Ethiopia      Africa     2007    52.9   76511887      691.\n 4 United States Americas   2007    78.2  301139947    42952.\n 5 Brazil        Americas   2007    72.4  190010647     9066.\n 6 Mexico        Americas   2007    76.2  108700891    11978.\n 7 China         Asia       2007    73.0 1318683096     4959.\n 8 India         Asia       2007    64.7 1110396331     2452.\n 9 Indonesia     Asia       2007    70.6  223547000     3541.\n10 Germany       Europe     2007    79.4   82400996    32170.\n11 Turkey        Europe     2007    71.8   71158647     8458.\n12 France        Europe     2007    80.7   61083916    30470.\n13 Australia     Oceania    2007    81.2   20434176    34435.\n14 New Zealand   Oceania    2007    80.2    4115771    25185.\n\n\nNotice also how the output is still grouped. If you want to drop groups so that operations which come afterwards are ungrouped, many functions have arguments for that (look at their help page).\n\n\nExercises\n\nCreate a new column called gdp_category that categorizes countries as “High” (gdpPercap &gt; 10000), “Medium” (between 3000 and 10000), or “Low” (&lt; 3000) GDP.\n\n\nCalculate the average life expectancy for each continent in 2007.\n\n\nCount how many countries are in each continent.\n\n\nFind the richest country in each continent for the year 2007.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#reshaping-and-joining-data-frames",
    "href": "02-data-wrangling.html#reshaping-and-joining-data-frames",
    "title": "Quantitative Methods and Statistics",
    "section": "Reshaping and joining data frames",
    "text": "Reshaping and joining data frames\n\nLong and wide format data with pivot_wider and pivot_longer\nData can be organized in “long” format (one observation per row) or “wide” format (multiple observations per row spread across different columns). If that seems a little abstract, here’s a picture showing the difference:\n\n\n\nLong vs. wide format data (source: graph gallery)\n\n\nHere is how we can switch between the two representations using the pivot_wider() function:\n\ngapminder_wide &lt;- pivot_wider(gapminder,\n                              id_cols = country,\n                              names_from = year, \n                              values_from = lifeExp)\n\ngapminder_wide\n\n# A tibble: 142 × 13\n   country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997`\n   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghan…   28.8   30.3   32.0   34.0   36.1   38.4   39.9   40.8   41.7   41.8\n 2 Albania   55.2   59.3   64.8   66.2   67.7   68.9   70.4   72     71.6   73.0\n 3 Algeria   43.1   45.7   48.3   51.4   54.5   58.0   61.4   65.8   67.7   69.2\n 4 Angola    30.0   32.0   34     36.0   37.9   39.5   39.9   39.9   40.6   41.0\n 5 Argent…   62.5   64.4   65.1   65.6   67.1   68.5   69.9   70.8   71.9   73.3\n 6 Austra…   69.1   70.3   70.9   71.1   71.9   73.5   74.7   76.3   77.6   78.8\n 7 Austria   66.8   67.5   69.5   70.1   70.6   72.2   73.2   74.9   76.0   77.5\n 8 Bahrain   50.9   53.8   56.9   59.9   63.3   65.6   69.1   70.8   72.6   73.9\n 9 Bangla…   37.5   39.3   41.2   43.5   45.3   46.9   50.0   52.8   56.0   59.4\n10 Belgium   68     69.2   70.2   70.9   71.4   72.8   73.9   75.4   76.5   77.5\n# ℹ 132 more rows\n# ℹ 2 more variables: `2002` &lt;dbl&gt;, `2007` &lt;dbl&gt;\n\n\nHere, id_cols refers to the column(s) that identify the new rows, names_from specifies from which column the new column names in the wide table should be taken, and values_from specifies from which column the values to populate the new columns should be taken.\nTo convert back to long format, we can use the pivot_longer() function:\n\npivot_longer(gapminder_wide, -country)\n\n# A tibble: 1,704 × 3\n   country     name  value\n   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan 1952   28.8\n 2 Afghanistan 1957   30.3\n 3 Afghanistan 1962   32.0\n 4 Afghanistan 1967   34.0\n 5 Afghanistan 1972   36.1\n 6 Afghanistan 1977   38.4\n 7 Afghanistan 1982   39.9\n 8 Afghanistan 1987   40.8\n 9 Afghanistan 1992   41.7\n10 Afghanistan 1997   41.8\n# ℹ 1,694 more rows\n\n\nHere, it is simpler to just specify the columns which not to stack on top of each other, which in our case is just the country column used as identifier in the last step. You can see that the grouping column and the value column just have the generic names name and value, which you can change with the names_to and values_to arguments.\nThe tidyverse generally prefers long format for analysis (e.g. visualization), but sometimes wide format is more efficient for storage or more useful for presentation.\n\n\nJoining tables on a common index\nOften data is spread across multiple tables. Let’s create a second dataset with hypothetical continent identifiers:\n\ncontinent_codes &lt;- data.frame(\n  continent = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\", \"Oceania\"),\n  continent_code = c(\"AF\", \"AM\", \"AS\", \"EU\", \"OC\")\n)\n\ncontinent_codes\n\n  continent continent_code\n1    Africa             AF\n2  Americas             AM\n3      Asia             AS\n4    Europe             EU\n5   Oceania             OC\n\n\nWe can add this identifier column to our main data frame by matching the continent column in both datasets. This operation is generally called a join - here we perform a left join, which means we want to keep all entries in the first (left) data frame, even if there is no matching identifier in the second (right) data frame. In that case, missing values will be inserted for the rows without a match.\nTo perform the left join, use the left_join() function with the two datasets and by specifying the common column with the by keyword:\n\nleft_join(gapminder, continent_codes, by = \"continent\")\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap continent_code\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         \n 1 Afghanistan Asia       1952    28.8  8425333      779. AS            \n 2 Afghanistan Asia       1957    30.3  9240934      821. AS            \n 3 Afghanistan Asia       1962    32.0 10267083      853. AS            \n 4 Afghanistan Asia       1967    34.0 11537966      836. AS            \n 5 Afghanistan Asia       1972    36.1 13079460      740. AS            \n 6 Afghanistan Asia       1977    38.4 14880372      786. AS            \n 7 Afghanistan Asia       1982    39.9 12881816      978. AS            \n 8 Afghanistan Asia       1987    40.8 13867957      852. AS            \n 9 Afghanistan Asia       1992    41.7 16317921      649. AS            \n10 Afghanistan Asia       1997    41.8 22227415      635. AS            \n# ℹ 1,694 more rows\n\n\nIf the common identifier column is called differently in the two datasets, you can pass something like this to by: c(\"left_name\" = \"right_name\") .",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#exercises-2",
    "href": "02-data-wrangling.html#exercises-2",
    "title": "Quantitative Methods and Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nCreate a pipeline that: calculates total GDP, groups by continent, and calculates the total GDP per continent and year.\n\n\nCreate a wide format dataset showing life expectancy for each country (rows) across different years (columns), but only for countries that start with “A”.",
    "crumbs": [
      "Basics of `R` programming",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html",
    "href": "04-descriptive-univariate.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "While the gapminder dataset has served us well so far, it is time to mix it up a bit. For this session, we’ll work with CO₂ emissions data from Our World in Data, which contains annual per-capita CO₂ emissions for countries worldwide. In addition to the emissions data, we will also get population data and GDP from the same source.\nQuite neatly, we can just directly download the file from their web page with the read_csv() function. However, to be good digital citizens, we’ll download the file and then save it to our computer so that we don’t need to download it again every time we run the script. We can achieve that with an if block, like so:\n\nlibrary(tidyverse)\n\nif (!file.exists(\"data/emissions-raw.parquet\")) {\n    df &lt;- read_csv(\"https://ourworldindata.org/grapher/co-emissions-per-capita.csv\")\n    arrow::write_parquet(df, \"data/emissions-raw.parquet\")\n}\n\nif (!file.exists(\"data/population-raw.parquet\")) { \n    df &lt;- read_csv(\"https://ourworldindata.org/grapher/population-unwpp.csv\")\n    arrow::write_parquet(df, \"data/population-raw.parquet\")\n}\n\nemissions_raw &lt;- arrow::read_parquet(\"data/emissions-raw.parquet\")\npopulation_raw &lt;- arrow::read_parquet(\"data/population-raw.parquet\")\n\nLet’s rename some variables and filter out non-country entities (such as the European Union or other aggregates) to prepare our data for further use:\n\nemissions &lt;- select(\n        emissions_raw,\n        entity = Entity,\n        code = Code,\n        year = Year,\n        emissions = `Annual CO₂ emissions (per capita)`\n    ) |&gt;\n    # filter out non-country entities\n    filter(!is.na(code))\n\n# Look at the dataset structure\nglimpse(emissions)\n\nRows: 23,206\nColumns: 4\n$ entity    &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ code      &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG…\n$ year      &lt;dbl&gt; 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, …\n$ emissions &lt;dbl&gt; 0.001992146, 0.010837197, 0.011625335, 0.011467511, 0.013123…\n\n\nWe do the same for population:\n\npopulation &lt;- select(\n        population_raw,\n        entity = Entity,\n        code = Code,\n        year = Year,\n        population = `Population - Sex: all - Age: all - Variant: estimates`\n    ) |&gt;\n    filter(!is.na(code))\n\nFinally, these two datasets look like they could be combined into a single data frame with a country per row and columns for population and emissions. We achieve this with a join:\n\ndf &lt;- left_join(emissions, population) # joins on all common name columns\n\nJoining with `by = join_by(entity, code, year)`\n\ndf\n\n# A tibble: 23,206 × 5\n   entity      code   year emissions population\n   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Afghanistan AFG    1949   0.00199         NA\n 2 Afghanistan AFG    1950   0.0108     7776180\n 3 Afghanistan AFG    1951   0.0116     7879343\n 4 Afghanistan AFG    1952   0.0115     7987784\n 5 Afghanistan AFG    1953   0.0131     8096703\n 6 Afghanistan AFG    1954   0.0129     8207954\n 7 Afghanistan AFG    1955   0.0185     8326981\n 8 Afghanistan AFG    1956   0.0217     8454303\n 9 Afghanistan AFG    1957   0.0341     8588340\n10 Afghanistan AFG    1958   0.0378     8723412\n# ℹ 23,196 more rows\n\n\n\n\n\nLoad the GDP per capita data from Our World in Data into a data frame.\n\n\nurl &lt;- \"https://ourworldindata.org/grapher/gdp-per-capita-worldbank.csv\"\n\n# write your code here...\n\n\nWrite the data to the file \"data/gdp-raw.parquet\" using the arrow package.\n\n\nPerform the same changes to variable names that we applied to the emissions data.\n\n\nJoin all three cleaned datasets into a single data frame. Save the result as a new file called emi-pop-gdp.parquet.\n\n\nCheck the availability of the variables across time and countries.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#exercises",
    "href": "04-descriptive-univariate.html#exercises",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "Load the GDP per capita data from Our World in Data into a data frame.\n\n\nurl &lt;- \"https://ourworldindata.org/grapher/gdp-per-capita-worldbank.csv\"\n\n# write your code here...\n\n\nWrite the data to the file \"data/gdp-raw.parquet\" using the arrow package.\n\n\nPerform the same changes to variable names that we applied to the emissions data.\n\n\nJoin all three cleaned datasets into a single data frame. Save the result as a new file called emi-pop-gdp.parquet.\n\n\nCheck the availability of the variables across time and countries.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#histograms",
    "href": "04-descriptive-univariate.html#histograms",
    "title": "Quantitative Methods and Statistics",
    "section": "Histograms",
    "text": "Histograms\nHistograms show the distribution of a continuous variable by dividing it into bins (usually of equal size) and counting observations in each bin. We can plot one using the now familiar ggplot2 machinery:\n\n# load combined dataset\ndf &lt;- arrow::read_parquet(\"data/emi-pop-gdp.parquet\")\n\n# filter dataset to single year\ndf2020 &lt;- filter(df, year == 2020)\n\np &lt;- ggplot(data = df2020, aes(x = emissions)) +\n    geom_histogram(color = \"white\", fill = \"tomato\", bins = 15) +\n    labs(\n        x = \"per-capita emissions (tons of CO₂)\", \n        y = \"Number of countries\"\n    )\n\np\n\n\n\n\n\n\n\n\nThe histogram tells us that the distribution is heavily right-skewed, with most countries having relatively low per-capita emissions and a few countries exhibiting very high per-capita emissions. Next to being an interesting fact in and of itself (guess which country has the highest per-capita emissions), it is also relevant for the choice of summary statistics.\n\nExercise\n\nCreate histograms of GDP per capita, population, and emissions, and combine them into a single plot. How would you describe the distributions?",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#measures-of-central-tendency",
    "href": "04-descriptive-univariate.html#measures-of-central-tendency",
    "title": "Quantitative Methods and Statistics",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nThere are different things we might want to say about the distribution shown by the histogram above, for example what its ‘center’ is. There is a variety of approaches to achieve this, each with different properties and trade-offs, making them suitable for different data and scenarios.\n\nThe arithmetic mean\nThe arithmetic mean (or simply “mean”) is the sum of all values divided by the number of observations. Mathematically, we can express it using sum notation (using the greek capital letter Sigma, \\(\\Sigma\\)):\n\\[ AM(x) = \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\]\nIt is probably the most well known and most widely used measure of central tendency, even if not always appropriate. We can call the corresponding R function on our emissions variable by accessing the data frame column with $ :\n\nmean(df2020$emissions)\n\n[1] 4.503439\n\n\nWe can see that the mean per-capita emissions across countries in 2020 is around 4.48 tons of CO2.\nThere is however a problem with the mean we just computed: It weighs all countries equally. In this case, it would however be much more appropriate to compute a weighted mean which weighs each country proportionally to its population size. Formally, this means multiplying each observation by its corresponding weight and then dividing by the sum of weights:\n\\[ AM^w(x) = \\frac{1}{\\sum_{i=1}^n w_i} \\sum_{i=1}^n w_i x_i \\]\nThe unweighted arithmetic ean is just a special case of this where all of the weights are equal to 1. We can compute the weighted mean in R with the weighted.mean() function, using the population column in our data frame as weights:\n\nwm &lt;- weighted.mean(df2020$emissions, df2020$population, na.rm = TRUE)\nwm\n\n[1] 4.406433\n\n\nWe can see that the weighted mean is a bit smaller, indicating that some of the high-emissions countries received lower weights (due to smaller populations) than some of the low-emissions countries. Here, we also had to specify na.rm = TRUE to remove observations with missing values (NAs) in our population column before computing the mean.\nWe can also add a line to our histogram plot to indicate our mean (or any measure of central tendency):\n\np + geom_vline(xintercept = wm, color = \"blue\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nThe geometric mean\nWhen our data represents ratios or rates of change, the arithmetic mean is no longer appropriate. We should instead use the geometric mean which correctly handles the multiplicative logic inherent to the data.\nWe might think about, e.g., the average growth rate of per-capita emissions in Germany over the last 20 years. We can obtain this growth series as follows, using the lag() helper function:\n\ngrowth_de &lt;- emissions |&gt;\n    filter(entity == \"Germany\" & \n           year &gt;= 2000 & \n           year &lt;= 2020) |&gt;\n    arrange(year) |&gt;\n    mutate(growth_rate = emissions / lag(emissions))\n\nIn analogy to the additive nature of the arithmetic mean (we’re adding up \\(n\\) things for which we account by dividing by \\(n\\)), the geometric mean is defined as the \\(n\\)’th root of the product of values:\n\\[GM(x) = \\sqrt[n]{x_1 \\times x_2 \\times ... \\times x_n} = \\left(\\prod_{i=1}^n x_i \\right)^\\frac{1}{n}\\]\nThere is no built-in function in R to compute the geometric mean, but it turns out that the geometric mean is just the exponential of the mean of the logs of the series, i.e. \\[GM(x) =  \\exp \\left( \\frac{1}{n} \\sum_{i=1}^n \\log(x_i) \\right)\\]\nwhich we can compute easily ourselves with a single line of R:\n\nexp(mean(log(growth_de$growth_rate), na.rm = TRUE))\n\n[1] 0.9826139\n\n\nThe result is around 0.98, which tells us that, on average, the per-capita emissions in Germany have been decreasing by about 2% a year over the last 20 years.\n\n\nThe median\nThe median is the middle value when observations are ordered from smallest to largest. If there is an even number of observations, there is no middle and the median is instead the mean of the two adjacent middle observations.\nBecause it does not care about the actual values in the tails (the ends) of a distribution, the median is less sensitive to outliers than the mean. We can compute it with the median() function:\n\nmedian(df2020$emissions, na.rm = TRUE)\n\n[1] 2.960939\n\n\nAt 2.97, the median is quite a bit lower than the mean. This is to be expected given the shape of our distribution: In right-skewed distributions like this one, the mean is always larger than the median because it’s pulled towards the extreme values.\nWe can add lines for both the median and the arithmetic mean to our plot to see how they compare:\n\np + geom_vline(xintercept = median(df2020$emissions), linetype = \"dashed\") + \n    geom_vline(xintercept = mean(df2020$emissions))\n\n\n\n\n\n\n\n\nAs for the mean, there is a weighted version of the median, which we however need to pull from the ggstats package (install it before you load it):\n\nlibrary(ggstats)\nwmed &lt;- weighted.median(df2020$emissions, df2020$population, na.rm = TRUE)\nwmed\n\n[1] 4.457317\n\n\n\n\nQuantiles\nQuantiles are values which divide the sorted data into equally-sized groups. The median is just one specific quantile, namely the 0.5 quantile (or equivalently the 50th percentile), which divides the data into two groups, with half the data below the median value and half above it.\nBut we can just as well report other quantiles, such as the 0.25 quantile (a quarter of the data below it , three quarters above) or the 0.99 quantile (just one percent of the data above it). Here is how to compute quantiles in R, with the probs argument specifying the quantiles to return:\n\nqs &lt;- quantile(df2020$emissions, probs = c(0.33, 0.66, 0.9, 0.99), na.rm = TRUE)\n\nWe could again add them to our plot:\n\np + geom_vline(xintercept = qs)\n\n\n\n\n\n\n\n\nLooking at the plot, we see that 90% of countries have per-capita emissions of around 10t or less.\nIf we want a quick summary of a numerical variable and mean, we can use the summary() function, which gives us the variables minimum and maximum, the variables mean and the 25%, 50% (i.e. the median), and 75% quantiles (called the `quartiles). It also reports the number of missing values.\n\n\nExercises\n\nFilter the dataset to the 2000 to 2020 period and keep only rows for country entities (which have entity codes of length 3).\n\n\nCalculate the (population-weighted) mean and median GDP per capita. Which is larger and why?\n\n\nInstall and load the countrycode package. Use it to create a new region column in your dataframe. You can use it like so:\n\n\ncountrycode::countrycode(\n    c(\"DEU\", \"USA\"),\n    origin = \"iso3c\",\n    destination  = \"region\")\n\n[1] \"Europe & Central Asia\" \"North America\"        \n\n\n\nCompute weighted means and medians in each region, for the years 2000 and 2020. Plot the result.\n\n\nAdd new columns to your data frame with the growth rate of total regional GDP and total regional emissions. Plot the regional development of GDP and emissions for East Asia, Europe, and North America.\n\n\nCompute the average growth rates for emissions and gdp since the year 2000 for each region.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#measures-of-dispersion",
    "href": "04-descriptive-univariate.html#measures-of-dispersion",
    "title": "Quantitative Methods and Statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nWhile measures of central tendency tell us something about the ‘center’ of the distribution, measures of dispersion try to capture how ‘spread out’ (or concentrated) the data are. Just as for measures of central tendencies, there are many ways to measure dispersion, each again with their own characteristics and trade-offs.\n\nThe range\nThe simplest measure of dispersion is the range, which is just the difference between the maximum and minimum values. In R, the range() function gives us the minimum and maximum values of a variable, the difference of which we can get with diff():\n\ndf2020$emissions |&gt; range() |&gt; diff()\n\n[1] 36.1006\n\n\nThe range is useful to understand the empirical limits of the studied variable but of course very sensitive to outliers and probably not representative of the typical data so it is not all that useful as a dispersion measure.\n\n\nThe interquartile range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles, representing the spread of the middle 50% of the data. As such it is much more robust to outliers than the range. You can compute it with the IQR() function:\n\nIQR(df2020$emissions)\n\n[1] 4.566232\n\n\n\n\nVariance and standard deviation\nThe variance measures the average squared deviation from the data mean: \\[ s^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\]\nSquaring differences from the mean make it so that positive and negative deviations don’t cancel each other out, but have the consequence that the result is somewhat awkward to interpret. The standard deviation addresses this by taking the square root of the variance so that the result is in the same units as the original data again:\n\\[ s = \\sqrt{s^2} \\]\nYou can easily compute both in R:\n\nvar(df2020$emissions)\n\n[1] 28.46136\n\nsd(df2020$emissions)\n\n[1] 5.334919\n\n\nVariance and standard deviation have nice theoretical properties and analogies in probability theory, making them the most widely used dispersion measures for numerical data. Just as for the mean, however, they are sensitive to outliers and can thus give misleading or surprising results.\n\n\nCoefficient of variation\nThe coefficient of variation (CV) is the ratio of standard deviation to the mean. It is useful for comparing variability across different datasets or across groups. We can compute it using the functions we already know:\n\nsd(df2020$emissions) / mean(df2020$emissions)\n\n[1] 1.184632\n\n\nBy itself, this is not particularly interesting. But we could use it to compare the spread of emissions across world regions, which have very different means.\nBased on this, we can use our data wrangling techniques to split the data and compute our summary for the two groups:\n\nlibrary(countrycode)\n\ndf2020 |&gt; mutate(\n    region = countrycode(code, origin = \"iso3c\", destination = \"region\")\n    ) |&gt;\n    summarize(\n        mean = mean(emissions),\n        sd = sd(emissions),\n        vc = sd / mean,\n        .by = region\n    )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `region = countrycode(code, origin = \"iso3c\", destination =\n  \"region\")`.\nCaused by warning:\n! Some values were not matched unambiguously: OWID_KOS, OWID_WRL, SHN, WLF\n\n\n# A tibble: 8 × 4\n  region                       mean     sd    vc\n  &lt;chr&gt;                       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 South Asia                  1.23   0.988 0.801\n2 Europe & Central Asia       5.82   2.99  0.515\n3 Middle East & North Africa  8.97  10.3   1.15 \n4 Sub-Saharan Africa          0.846  1.37  1.62 \n5 Latin America & Caribbean   4.17   4.40  1.05 \n6 East Asia & Pacific         5.33   5.68  1.07 \n7 North America              11.2    3.33  0.296\n8 &lt;NA&gt;                        3.45   1.39  0.404\n\n\nBased on this, we can see that while the standard deviation is much larger in Europe in absolute terms, relative to the mean Europe is much more homogeneous in its per-capita emissions than Africa.\n\n\nMedian absolute deviation (MAD)\nSimilarly to the median being an alternative to the mean that is more robust against outliers, the median absolute deviation (MAD) is a robust dispersion measure, computed as the median of the absolute differences to the median. We could compute it ourselves but R has a built-in function:\n\nmad(df2020$emissions)\n\n[1] 3.255291\n\n\nAs before, we see that MAD is lower compared to the standard deviation because it is less sensitive to large outliers.\n\n\nExercises\n\nIdentify countries that are outliers in terms of per-capita emissions (you can use the IQR method: values beyond Q1 - 1.5×IQR or Q3 + 1.5×IQR). What are your theories about why they are outliers?\n\n\nInvestigate the hypothesis that economic development in Europe has been more convergent (or at least less divergent) due to the EU’s cohesion policies than other world regions (pick one comparison region).",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#bar-plots-and-frequency-tables",
    "href": "04-descriptive-univariate.html#bar-plots-and-frequency-tables",
    "title": "Quantitative Methods and Statistics",
    "section": "Bar plots and frequency tables",
    "text": "Bar plots and frequency tables\nThe most basic description of categorical data is a frequency table, showing how many observations fall into each category:\n\ndf2020_cat |&gt; count(emission_level)\n\n# A tibble: 3 × 2\n  emission_level     n\n  &lt;ord&gt;          &lt;int&gt;\n1 Low               88\n2 Medium            95\n3 High              32\n\n\nWe can also get proportions instead of raw counts by dividing our counts by the sum of counts across all categories:\n\nprops &lt;- df2020_cat |&gt; \n    count(emission_level) |&gt;\n    mutate(proportion = n / sum(n))\n\nprops\n\n# A tibble: 3 × 3\n  emission_level     n proportion\n  &lt;ord&gt;          &lt;int&gt;      &lt;dbl&gt;\n1 Low               88      0.409\n2 Medium            95      0.442\n3 High              32      0.149\n\n\nAbout 40% of countries fall into the low emissions category, while only about 14% are in the high category.\nWe can easily visualize a frequency table as a bar chart as an equivalent to the histogram for quantitative variables:\n\nggplot(df2020_cat, aes(x = emission_level)) +\n    geom_bar(fill = \"tomato\") +\n    labs(x = \"Emission Level\", y = \"Number of Countries\")",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#categorical-measures-of-central-tendency",
    "href": "04-descriptive-univariate.html#categorical-measures-of-central-tendency",
    "title": "Quantitative Methods and Statistics",
    "section": "Categorical measures of central tendency",
    "text": "Categorical measures of central tendency\n\nMedian for ordinal data\nWhen we have ordinal categorical data (categories with a meaningful order), we can use the median as a measure of central tendency because it only relies on the order of values but not the actual values themselves (in R, we still need to convert to a numerical representation, though):\n\nmedian(as.numeric(df2020_cat$emission_level))\n\n[1] 2\n\n\nMaybe unsurprisingly giving the above bar chart, the median category is ‘Medium’.\n\n\nMode for nominal data\nFor nominal (unorderded categorical) data, we cannot use the median any more. The most useful representation of the ‘center’ is in this case just the most frequently occurring category, called the mode. While R doesn’t have a built-in mode function, it is just the top value of our sorted counts:\n\ndf2020_cat |&gt; count(emission_level, sort = TRUE) |&gt; head(1)\n\n# A tibble: 1 × 2\n  emission_level     n\n  &lt;ord&gt;          &lt;int&gt;\n1 Medium            95\n\n\nThe mode of our emission level variable is again “Medium”, although in this case it would be more useful to report the median category anyways, as discussed above.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#categorical-measures-of-dispersion",
    "href": "04-descriptive-univariate.html#categorical-measures-of-dispersion",
    "title": "Quantitative Methods and Statistics",
    "section": "Categorical measures of dispersion",
    "text": "Categorical measures of dispersion\n\nEntropy\nFor measuring dispersion in categorical data, we have to measure how concentrated the distribution is, with maximum concentration achieved when all observations fall into a single group. One measure which achieves this is entropy (having origins in physics and information theory), which reaches its maximum when all categories are equally likely, and its minimum when all observations fall into a single category. The entropy is mathematically defined as minus the sum of the probability-weighted log of each probability/proportion, which we can normalize to the (0, 1) range by dividing by the log of the number of categories:\n\\[\nH = -\\frac{1}{\\log(k)} \\sum_{i=1}^k p_i \\log(p_i)\n\\]\nwhere \\(p_i\\) is the proportion of category \\(i\\). There is no built in function in base R but we can easily compute it ourselves:\n\n-sum(props$proportion * log(props$proportion)) / log(nrow(props))\n\n[1] 0.9193826\n\n\nThe resulting value is 0.92, telling us that the distribution is relatively but not quite perfectly even. A more interesting finding would probably come from comparing against a reference value (e.g., compare the spread of today’s distribution with 20 years ago).",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "04-descriptive-univariate.html#exercises-3",
    "href": "04-descriptive-univariate.html#exercises-3",
    "title": "Quantitative Methods and Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nDefine three categories: “Low income” (&lt; $5,000), “Middle income” ($5,000 - $20,000), and “High income” (≥ $20,000). Create a corresponding categorical variable in your dataset.\n\n\nCreate a frequency table showing the number and proportion of countries in each income category.\n\n\nCreate a bar chart of the income categories.\n\n\nCompute the median and entropy for income categories for each of the world regions.",
    "crumbs": [
      "Describing data",
      "Univariate descriptions"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "03-data-visualization.html",
    "href": "03-data-visualization.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "Last session, we learned basic data wrangling using the gapminder dataset. As we should now have a basic idea about its contents and structure, we will use it again to learn how to make informative and nice looking visualizations in R.\n\nlibrary(gapminder); data(gapminder) # makes the dataset available\n\nWe will also load the tidyverse again, which contains the ggplot2 package for visualization:\n\nlibrary(tidyverse)\n\nFor the sake of making a simpler plot, we will filter the dataset to the year 2007, using the filter() function we learned about last time:\n\ngapminder2007 &lt;- filter(gapminder, year == 2007)",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#our-first-plot",
    "href": "03-data-visualization.html#our-first-plot",
    "title": "Quantitative Methods and Statistics",
    "section": "Our first plot",
    "text": "Our first plot\nFor plotting, we use the ggplot2 package, which is contained in the tidyverse. The fundamental building block for any plot is a call to the ggplot() function, to which we pass the data that we want to plot. On its own, this just creates a blank plot, as we haven’t specified which variables we want to plot and how we want to plot them:\n\nlibrary(ggplot2)\n\np &lt;- ggplot(data = gapminder2007)\np\n\n\n\n\n\n\n\n\nTo specify what pieces of our data we want to plot, we have to specify an aesthetic mapping from our data variables to the visual elements of our plot (such as positions or colors). This allows us to specify that we want to display variation in GDP along the x-axis and life expectancy along the y-axis:\n\np &lt;- ggplot(data = gapminder2007,              \n            mapping = aes(x = gdpPercap, y = lifeExp))\np\n\n\n\n\n\n\n\n\nWe can see that our plot now contains axis labels and ticks informed by the range of the data, but there is still no visual representation of the data, because we haven’t specified what kind of plot we want. To specify the how of our visualization, we need to add further instructions to our plotting code.\nggplot2 operates in terms of layers which we can add to our basic plot specification with + to include specific geometric representations of our data (such as points in a scatter plot):\n\np &lt;- p + geom_point()\np\n\n\n\n\n\n\n\n\nThis plot now contains one point for each country in the dataset, using GDP per capita and life expectancy for the x and y coordinates, respectively. It already tells us a lot about the relationship between per capita GDP and life expectancy:\n\nLife expectancy varies a lot and is generally lower for countries with very low GDP (&lt; 5000 USD), and is consistently high (mostly above 75 years) for richer countries (&gt; 20 000 USD).\nAt some point (beyond around 25 000 USD or so), more GDP is not really associated with higher life expectancy.\n\nWhile the plot is already informative, there are many things we can improve, such as including more information from our data by making use of additional visual channels, changing variable scaling, adding better labels, or giving the plot a title.\n\nExercise\nMake a plot that investigates the hypothesis that larger countries are wealthier.",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#refining-the-plot",
    "href": "03-data-visualization.html#refining-the-plot",
    "title": "Quantitative Methods and Statistics",
    "section": "Refining the plot",
    "text": "Refining the plot\nOne first improvement would be to add some more information to our plot, such as the continental grouping of the countries. The obvious choice for the visual channel would be colors: There is only a handful of continents and countries in the same continent should be clustered to a certain degree, making colors easily distinguishable. To color points by continent, we need to add another entry to our mapping:\n\np &lt;- ggplot(\n    data = gapminder2007,\n    mapping = aes(\n        x = gdpPercap, \n        y = lifeExp, \n        color = continent\n    )\n) + geom_point()\n\np\n\n\n\n\n\n\n\n\nThat’s much better - we can now see a familiar picture of geographically differentiated economic development and its impact on life expectancy. If we want different colors, we can change them by adding another layer to our plot. We could manually add colors with scale_color_manual() but often it is more convenient to pick a predefined scale, like the ones available in the RColorBrewer package:\n\np &lt;- p + scale_color_brewer(palette = \"Set1\")\np\n\n\n\n\n\n\n\n\nIf you want to inspect the available scales, call RColorBrewer::display_brewer_all().\nRight now, the variation in the left part of the plot is hard to differentiate because the points are close together and there is strong variation in a very narrow GDP range. We can solve this by using a log transformation for GDP, i.e. by plotting the logarithm of GDP (typically base 10 in this kind of scenario) instead of actual GDP. Moving along the x axis by a fixed length will then no longer indicate an additive increase in GDP, but a multiplicative one. Because of this nonlinear nature of the logarithm transform, the points in the left part of the figure will be spread out more while the points to the right will be closer to each other.\nWe can implement this by simply adding another layer with the scale_x_log10() function to our plot:\n\np &lt;- p + scale_x_log10()\np\n\n\n\n\n\n\n\n\nWe can now see the variation in the lower GDP range much clearer and we now can see a log-linear trend, i.e., a linear increase in life expectancy with respect to the log of GDP. Some getting used to is required and care needs to be taken when interpreting and creating log scales - they will often yield more readable plots but also have a tendency to confuse people.\nFor some finishing touches, we should add labels and descriptions to our plot, which we do with the labs() function:\n\n p &lt;- p + labs(\n    x = \"Per-capita GDP\",\n    y = \"Life expectancy\",\n    color = \"\",\n    title = \"Economic development and life expectancy\",\n    subtitle = \"Inflation corrected GDP and life expectancy across 140 countries, recorded in 2007\",\n    caption = \"Source: Gapminder\"\n  )\n\np\n\n\n\n\n\n\n\n\nWe can also change the theme of our plot by picking one of the predefined themes and change some stylistic details like the legend position using the theme() function:\n\np &lt;- p + \n    theme_minimal() + \n    theme(legend.position = \"bottom\")\n\np\n\n\n\n\n\n\n\n\nIf we want to use the same theme for all the plots in our session, we can activate it as the default theme (ideally at the start of the script):\n\ntheme_set(theme_minimal())\n\nWhen we are happy with our plot, it is time to export it to a file so that we can easily include it in a paper, report, or presentation, which we do with the ggsave() function:\n\nggsave(\"myplot.png\", p, width = 8, height = 8, dpi = 320)\n\nHere, width and height are specified in inches, but there is an option to switch to cm. The dpi argument sets the output resolution, which should be reasonably high for inclusion in a document.",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#the-final-plot",
    "href": "03-data-visualization.html#the-final-plot",
    "title": "Quantitative Methods and Statistics",
    "section": "The final plot",
    "text": "The final plot\nHere is everything put together and commented, without using an incrementally improved plot variable as before. It is a lot of code, but at this point we should understand what each line does and be able to make further additions or changes.\n\np &lt;- ggplot(\n    # Dataset from which to plot variables\n    data = gapminder2007,\n    \n    # Mapping variables to visual aspects of the plot\n    mapping = aes(\n      x = gdpPercap, \n      y = lifeExp,\n      color = continent, \n      label = country\n    )\n  ) +\n  \n  # Visualizing point geometries for observations\n  geom_point() +\n\n  # Log scale for x variable\n  scale_x_log10() +\n\n  # Categorical color palette for continents\n  scale_color_brewer(palette = \"Set1\") +\n\n  # Labels for axes, titles etc.\n  labs(\n    x = \"Per-capita GDP\",\n    y = \"Life expectancy\",\n    color = \"\",\n    title = \"Economic development and life expectancy\",\n    subtitle = \"Inflation corrected GDP and life expectancy across 140 countries, recorded in 2007\",\n    caption = \"Source: Gapminder\"\n  ) +\n\n  # General design of the plot\n  theme_minimal() +\n\n  # Special design aspects, like legend position\n  theme(legend.position = \"bottom\")\n\n# Save the plot to a file (will save the last plot that was generated)\nggsave(\"myplot.png\", p, dpi = 320)\n\nSaving 7 x 5 in image\n\n\n\nExercise\nUse the techniques discussed above to refine the plot investigating the relationship between size and GDP you created in the last exercise.",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#small-multiples-with-faceting",
    "href": "03-data-visualization.html#small-multiples-with-faceting",
    "title": "Quantitative Methods and Statistics",
    "section": "Small multiples with faceting",
    "text": "Small multiples with faceting\nOne powerful visualization technique we have not yet studied is faceting, which is used to create small-multiples plots, i.e. plots which repeat the same visualization structure across multiple panels. This is very useful to, e.g., highlight different trends across subgroups of a dataset.\nThe easiest way to facet a plot is with the facet_wrap() function, which we can again just add as a layer to a plot specification. Here is an example visualizing per-country GDP trends split by continent:\n\np2 &lt;- ggplot(\n    data = gapminder, \n    mapping = aes(\n        x = year, \n        y = gdpPercap,\n        group = country\n        )\n    ) +\n    geom_line(linewidth = 0.5, alpha = 0.3) +\n    facet_wrap(~continent, scales = \"free_y\", nrow = 1)\n\np2\n\n\n\n\n\n\n\n\nThe ~ is part of Rs formula notation, which here just indicates that we want to facet by the continent variable. Note also the use of the scales = \"free_y\" argument, which indicates that each panel should get its own y-axis instead of a shared one. This is necessary because of the big differences in GDP across continents, which would make the variation in some continents hard to see with a shared axis. Finally, note also the group = country in the mapping, which is necessary so that every country gets its own line.",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "03-data-visualization.html#combining-multiple-plots-with-patchwork",
    "href": "03-data-visualization.html#combining-multiple-plots-with-patchwork",
    "title": "Quantitative Methods and Statistics",
    "section": "Combining multiple plots with patchwork",
    "text": "Combining multiple plots with patchwork\nIf we want to combined multiple plots, we can use the patchwork package, which lets us stack plot objects on top of each other with / or side by side with +. We can also add instructions for the relative widhts / heights of each plot with plot_layout() and add annotations, such as letter tags to each plot. Here is an example combining the two plots we just created:\n\nlibrary(patchwork)\n\np / p2 + \n    plot_layout(heights = c(0.6, 0.4)) + \n    plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nLet’s refine this just a bit by giving the second plot the same colors as the first and cleaning up the spacing for the x ticks:\n\np2_colors &lt;- ggplot(gapminder, aes(year, gdpPercap, group = country, color = continent)) +\n    geom_line(linewidth = 0.5, alpha = 0.3, show.legend = FALSE) +\n    facet_wrap(~continent, scales = \"free_y\", nrow = 1) +\n    scale_color_brewer(palette = \"Set1\") +\n    scale_x_continuous(breaks = seq(1950, 2010, by = 20)) +\n    labs(x = \"\", y = \"Per-capita GDP\")\n\np / p2_colors + \n    plot_layout(heights = c(0.6, 0.4)) + \n    plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nNote how we also suppressed the color legend for the lines by setting show.legend = FALSE because we already have a legend for colors from the first plot. The customization possibilities are endless!",
    "crumbs": [
      "Describing data",
      "Data visualization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "This website accompanies the introduction to quantitative methods and statistics taught in the Masters Degree in Human Geography at the Department of Geography, LMU Munich. The objective of this class is for students to develop a working knowledge of common applied statistical procedures which enables them to independently identify appropriate methods and conduct analysis using the R programming language in the context of quantitative research designs. Although most students in the class will have previously visited a first course in statistics, the class has no prerequisites in either statistics or programming. It will however move beyond descriptive statistics relatively quickly to focus on statistical modelling and inference.\n\n\nThe course will roughly follow the below structure, but we are also flexible to adapt the programme as we go along:\n\n\n\n\n\n\n\nSession\nTopics\n\n\n\n\n1 - 3\nR basics: data wrangling, visualization, descriptive statistics\n\n\n4\nStochastic models: probability & simulation\n\n\n5\nStatistical inference: sampling, estimation & uncertainty\n\n\n6 - 7\nLinear regression: basics, prediction, goodness of fit, transformations\n\n\n8\nLinear regression: multiple regression, interactions, smooth effects\n\n\n9 - 10\nCausal inference: experiments and observational studies\n\n\n11 - 13\nAdvanced topics: GLMs, random effects, dependent data\n\n\n\n\n\n\nThe class will finish with a 60 minute exam in the last week of the semester which will test proficiency with both theoretical as well as practical skills learned throughout the class.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#course-programme",
    "href": "index.html#course-programme",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "The course will roughly follow the below structure, but we are also flexible to adapt the programme as we go along:\n\n\n\n\n\n\n\nSession\nTopics\n\n\n\n\n1 - 3\nR basics: data wrangling, visualization, descriptive statistics\n\n\n4\nStochastic models: probability & simulation\n\n\n5\nStatistical inference: sampling, estimation & uncertainty\n\n\n6 - 7\nLinear regression: basics, prediction, goodness of fit, transformations\n\n\n8\nLinear regression: multiple regression, interactions, smooth effects\n\n\n9 - 10\nCausal inference: experiments and observational studies\n\n\n11 - 13\nAdvanced topics: GLMs, random effects, dependent data",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#certification",
    "href": "index.html#certification",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "The class will finish with a 60 minute exam in the last week of the semester which will test proficiency with both theoretical as well as practical skills learned throughout the class.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "Programming workflows enable far more complex analyses than classical tools, by allowing to iterate faster and scaling better with data size and complexity. Scripting also changes how we think about and conduct data analysis: Unlike point-and-click interfaces, programming forces us to be explicit about every step of our analysis, making our work reproducible and our assumptions transparent.\n\n\nThere are many different ways to approach any data analysis project, but most follow the same rough outline:\n\nState research question or objective : what do we want to understand?\nGather and clean data : What data do we have to support our objective?\nVisualize and explore data : What is in our data?\nState assumptions and formulate model : How can our data provide evidence for our research objective?\nCheck the model : Are our assumptions violated? How well does our model capture the characteristics of the data?\nIterate : Go back to the drawing board to refine the model, if necessary\nDraw inferences : Condense evidence regarding the primary estimands of interest\n\nThis process is much easier to execute and much more transparent when the steps of your analysis are written down in a file compared to just existing in you head, and R is tailored for this kind of workflow.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#basic-workflow-of-quantitative-research",
    "href": "01-introduction.html#basic-workflow-of-quantitative-research",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "There are many different ways to approach any data analysis project, but most follow the same rough outline:\n\nState research question or objective : what do we want to understand?\nGather and clean data : What data do we have to support our objective?\nVisualize and explore data : What is in our data?\nState assumptions and formulate model : How can our data provide evidence for our research objective?\nCheck the model : Are our assumptions violated? How well does our model capture the characteristics of the data?\nIterate : Go back to the drawing board to refine the model, if necessary\nDraw inferences : Condense evidence regarding the primary estimands of interest\n\nThis process is much easier to execute and much more transparent when the steps of your analysis are written down in a file compared to just existing in you head, and R is tailored for this kind of workflow.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#variables",
    "href": "01-introduction.html#variables",
    "title": "Quantitative Methods and Statistics",
    "section": "Variables",
    "text": "Variables\nIn R, we assign values to names using the assignment operator &lt;- (the equal sign = also works for assignment but is generally frowned upon in R circles - I don’t really care, use what you want). Variable names are allowed to contain underscores _ and dots . but not any other symbols or spaces. They are also case sensitive:\n\nmy_number &lt;- 100\nmy.even.bigger.number &lt;- 1000\n\nTHIS_is_also.a_legal_name &lt;- -3.3334\n\n# this is}an-illegal*name &lt;- 10000\n\nBest practice: Use only lowercase and underscores (you will still see dots, but they are olschool). Use descriptive names that make your code self-documenting without being overly explicit (good variable naming is an art).\nBy creating variables, we keep track of values which we want to reuse in later steps in the analysis. If you want to show the value of a variable, just state it in its own line and execute the cell:\n\nx &lt;- 5\nx\n\n[1] 5\n\n\nVariables can be overwritten, which can be useful to keep the name space (the set off all names defined in the session) clean. The following code overwrites the variable x from the last cell, which you can validate by looking at the global environment:\n\nx &lt;- x + 10\nx\n\n[1] 15\n\n\nBut be careful, overwriting variables can lead to unexpected behavior and headaches down the road.\n\nExercises\n\nCreate a variable called my_age with your age. Confirm that the variable exists and has the specified value in your global environment in RStudio.\n\n\nCreate another variable called my_age_in_months that multiplies your age by 12 (you can multiply two numbers with *).",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#functions",
    "href": "01-introduction.html#functions",
    "title": "Quantitative Methods and Statistics",
    "section": "Functions",
    "text": "Functions\nFunctions are objects which take inputs and return outputs (some functions don’t take inputs, or return any output, but produce some other side effect). Functions are called with parentheses directly behind their name and can take arguments. Arguments can be provided by position (in the order expected by the function) or by name (as keyword arguments):\n\n# Positional arguments: order matters\nround(3.14159, 2)\n\n[1] 3.14\n\n# Named arguments: order doesn't matter\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n# Mixed: positional first, then named\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nBest practice: Use positional arguments for the first 1-2 arguments if they are obvious, then switch to named arguments for clarity. For the example above, option three is preferred.\nBasically everything interesting you will do in R involves calling functions, so get comfortable with them.\n\nExercises\n\nThe seq(from = ..., to = ..., by = ...) function creates sequences of evenly spaced numbers. Use it to create a sequence of numbers from 1 to 100 in increments of 5. Write your code once with keyword arguments and one with positional arguments.\n\n\nUse the mean function to calculate the average of the sequence.\n\n\nUse the length function to count how many numbers you have.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#data-types",
    "href": "01-introduction.html#data-types",
    "title": "Quantitative Methods and Statistics",
    "section": "Data types",
    "text": "Data types\nComputers need to distinguish different types of data, such as numbers or text, to be able to do anything useful. While the internal structure of, e.g., floating point numbers or strings of characters representing text is quite complex, we here only need to know how R differentiates them.\nThe class function can be used to tell the data type of a variable. Here is a review of some of the most common data types and some of the basic things you can do with them:\n\nNumbers\nR has a distinction between real numbers (class double) and integers (class integer), which in practice you don’t often have to care about:\n\n# Numeric (double)\nx &lt;- 42.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer (less common)\ny &lt;- 42L\nclass(y)\n\n[1] \"integer\"\n\n\nNumbers support all the usual arithmetic operations:\n\n5 + 3 # addition\n10 - 4 # subtraction\n6 * 7 # multiplication\n15 / 3 # division\n2^3  # exponentiation\n\n# Order of operations matters\n2 + 3 * 4\n(2 + 3) * 4\n\n\n\nStrings\nStrings are how computers represent text and are of class character in R. If you want to create a variable holding a string, you need to put the text into quotes \":\n\nname &lt;- \"Jakob\"\nclass(name)\n\n[1] \"character\"\n\n\nStrings will be frequently used to specify how a function should go about its business and a common beginner mistake is forgetting the quotes.\nYou can join together different strings with paste or extract bits of a string with substr (among many other useful string processing operations):\n\n# Combine strings\nfirst_name &lt;- \"John\"\nlast_name &lt;- \"Doe\"\npaste0(first_name, \"_\", last_name)\n\n[1] \"John_Doe\"\n\n\n\n# Extract substrings (position-based)\ntext &lt;- \"Statistics\"\nsubstr(text, 1, 4)  # characters 1 through 4\n\n[1] \"Stat\"\n\n\n\n\nBooleans\nThe boolean (logical) values true and false have their own datatype, called logical:\n\nis_R_great &lt;- TRUE\nis_programming_boring &lt;- FALSE\nclass(is_R_great)\n\n[1] \"logical\"\n\n\nLogical values typically result from comparing values:\n\n5 &gt; 3 # greater than\n5 &lt;= 3 # smaller or equal than\n\"phillip\" == \"phillip\"  # equal (note the double ==)\n\"philipp\" != \"phillipp\" # not equal\n\nWe can perform boolean logic on boolean values, the result of which is again a boolean value:\n\na &lt;- TRUE\nb &lt;- FALSE\n\na & b  # AND\na | b  # OR\n!a     # NOT\n\nThis is very useful when, e.g., filtering a set of values by multiple conditions. When doing arithmetic, FALSE and TRUE are treated as 0 and 1, respectively. This can be useful for counting the cases that meet a certain condition, which can be achieved using a sum of booleans.\n\n\nVectors\nVectors are the most fundamental container data type and represent collections of elements of the same basic type (numbers, strings, booleans…):\n\n# Numeric vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\nnumbers\n\n[1] 1 2 3 4 5\n\n# Character vector\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\")\nnames\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# Logical vector\nbools &lt;- c(TRUE, FALSE, TRUE)\nbools\n\n[1]  TRUE FALSE  TRUE\n\n\nIf you try to combine different data types, they will be converted to the least common denominator:\n\nc(1, TRUE, \"abc\") # converted to string\n\n[1] \"1\"    \"TRUE\" \"abc\" \n\n\nYou can access individual elements or subsets of vectors with square brackets:\n\nnumbers &lt;- c(10, 20, 30, 40, 50)\n\nnumbers[1]  # first element\nnumbers[3]  # third element\n\nnumbers[c(1, 3, 5)]  # first, third, and fifth elements\nnumbers[2:4]         # elements 2 through 4\n\nInstead of specifying the positions of the elements to extract, you can also use a logical vector of the same length to indicate for each element whether to select it or not:\n\nmy_idx &lt;- c(TRUE, FALSE, FALSE, TRUE, TRUE)\nnumbers[my_idx]\n\n[1] 1 4 5\n\n\n\n\nMissing values\nReal data often contains missing values, which R represents as NA:\n\n# Vector with missing values\nages &lt;- c(25, NA, 30, 22, NA)\nages\n\n[1] 25 NA 30 22 NA\n\n# Check for missing values\nis.na(ages)\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nMissing values are contagious, i.e. a summary of a vector with missings will often also return missing (how can you know the mean when some values are not known?). Most functions with this behavior have a flag to ignore missing values, like na.rm:\n\nmean(ages) # Returns NA\n\n[1] NA\n\nmean(ages, na.rm = TRUE)  # Remove NAs first\n\n[1] 25.66667\n\n\n\n\nFactors\nFactors are a special kind of vector and are Rs way of representing categorical (qualitative) data, i.e. statistical variables which can only take on a set of distinct values: They are quite essential for statistical analysis and will come up frequently when we start doing actual statistics. You can create a factor variable by wrapping a vector in the factor function:\n\ncolors &lt;- factor(c(\"red\", \"blue\", \"red\", \"green\", \"blue\"))\ncolors\n\n[1] red   blue  red   green blue \nLevels: blue green red\n\n\nBy default, factors represent unordered (nominal) categorical variables. You can also create ordered factors with the ordered flag, in which case you also probably want to give the order of levels explicitly with the levels argument. In addition, you can change the labels used for the different levels of the factor with the labels argument. Here’s an example combining all of this:\n\nsizes &lt;- factor(\n  c(\"S\", \"B\", \"S\", \"M\", \"M\", \"S\"),\n  levels = c(\"S\", \"M\", \"B\"),\n  labels = c(\"small\", \"medium\", \"big\"),\n  ordered = TRUE\n)\nsizes\n\n[1] small  big    small  medium medium small \nLevels: small &lt; medium &lt; big\n\n\nNote how we put the arguments to the factor function each on their own line to avoid producing one very long and hard to read line. You can (and should) generally format your code to be more readable like that. R is not sensitive to whitespace, so you don’t need to follow any specific indentation rules but there are some best practices.\n\n\nExercises\n\nTo ride the Oktoberfest roller coaster, you need to be at least 16 years old but also above 150 cm. Using logical expressions, write a piece of code which checks whether Lisa can ride the roller coaster:\n\n\nlisa_age &lt;- 21\nlisa_height &lt;- 148\n\n# Write your code here...\n\n\nTim, Tom and Tina are 185cm, 173cm, and 175cm tall. Create a vector with their sizes. Then, write a piece of code to express their sizes in meters.\n\n\nThe variable below represents a bike store’s inventory. Turn it into a factor variable using the shorter labels “gravel”, “city”, and “racing” instead. Use the summary() function on your new variable. What does it tell you?\n\n\ninventory &lt;- rep(\n  c(\"Gravel bike\", \"City bike\", \"Racing bike\"),\n  times = c(10, 15, 5)\n)\n\n# write your code here...\n\n\n\nData frames\nData frames are the bread-and-butter data structure for data analysis in R. They are like spreadsheets, with rows representing observations and columns representing variables of potentially different data type. Here’s an example:\n\n# Create a data frame\nstudents &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(20, 21, 19),\n  passed = c(TRUE, FALSE, TRUE)\n)\nstudents\n\n     name age passed\n1   Alice  20   TRUE\n2     Bob  21  FALSE\n3 Charlie  19   TRUE\n\n\nThe values of a data frame can be accessed in several ways. First, if we want to extract a full column as a vector we use the df$column syntax:\n\nstudents$name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n\nWe can chain this with vector indexing via [] to extract a specific value of a column:\n\nstudents$name[2]\n\n[1] \"Bob\"\n\n\nSimilarly to how we index vectors (or higher-dimensional arrays, such as matrices), we can also use square brackets directly to index data frames:\n\nstudents[1, 2]       # first row, second columns\nstudents[1:3, \"age\"] # rows 1 to 3, 'age' column\nstudents[, 2]        # all rows, second column\n\nIn the future, we will often rely on higher-level functions to process data frames, but the basic options for accessing their data are still good to know.\n\n\nLists\nUnder the hood, data frames are a special kind of what R calls lists, which are a more general form of container that relaxes the primary restriction of vectors: While vector can only store elements of the same basic data type (numbers, strings, booleans, …), lists can contain combinations of things (and even other lists):\n\n# Mixed list\nmy_list &lt;- list(\n  numbers = c(1, 2, 3),\n  names = c(\"A\", \"B\", \"C\"),\n  flag = TRUE\n)\nmy_list\n\n$numbers\n[1] 1 2 3\n\n$names\n[1] \"A\" \"B\" \"C\"\n\n$flag\n[1] TRUE\n\n\nList elements can be accessed either by name using the $ operator or by using double square brackets:\n\nmy_list &lt;- list(\n  numbers = c(1, 2, 3),\n  names = c(\"A\", \"B\", \"C\"),\n  bool = TRUE\n)\n\n# Access by name with $\nmy_list$numbers\nmy_list$flag\n\n# Access by position or name with [[]]\nmy_list[[1]]        # first element\nmy_list[[\"names\"]]  # by name\n\n\n\nExercises\n\nCreate a data frame with information about 3 of your favorite movies / series:\n\n\nColumn 1: title (character)\nColumn 2: year (numeric)\nColumn 3: rating (ordered factor, 1-5 stars)\n\n\nUse $ to access just the ratings column.\n\n\nUse [] to access the first row.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#packages",
    "href": "01-introduction.html#packages",
    "title": "Quantitative Methods and Statistics",
    "section": "Packages",
    "text": "Packages\nWhile R has a big standard library containing many useful functions, one of its biggest strengths is its huge package ecosystem which has packages for all kinds of basic or advanced functionality, from visualization to advanced modeling or GIS.\nIf you want to use a package (also called a library), there are two steps you need to perform:\n\nInstall the package (only needs to be done once, or if you want to update)\nLoad the package (needs to be done every time you start a new session)\n\nHere is how to do these:\n\n# Install once (downloads to your computer)\ninstall.packages(\"ggplot2\")\n\n# Load in each session (makes functions available)\nlibrary(ggplot2)\n\nAfter having succesfully installed a package, you should remove or comment out the install statement or else you will get prompted to reinstall the package whenever you rerun your notebook. Alternatively, you can use the RStudio interface for installing packages.\nOne package (or rather collection of packages) that comprises many useful functions for data science (data wrangling, visualization) is the tidyverse, which includes packages like dplyr for data manipulation, ggplot2 for visualization, readr for reading data, tidyr for data reshaping. You could of course also install and load these individually.\n\nExercises\n\nInstall the tidyverse using RStudio’s package installation interface.\nLoad the package using the library() function.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "01-introduction.html#getting-help",
    "href": "01-introduction.html#getting-help",
    "title": "Quantitative Methods and Statistics",
    "section": "Getting help",
    "text": "Getting help\nR has great built-in help, which you can access by writing a ? followed by the name of the function you need help with:\n\n?mean\nhelp(mean)\n\nThe help pages you get with that follow a standard structure:\n\nDescription: What the function does\nUsage: How to call it\nArguments: What inputs it takes\nValue: What it returns\nExamples: Working code examples\n\nWhile they can be a little bit overwhelming at first, they are usually also the most authoritative resource on any piece of code.\nBeyond the technical help page for specific functions, you can also get an overview of a package’s functionality and look for vignettes, which are longer tutorials for certain packages:\n\n# See all functions in a package\nhelp(package = \"dplyr\")\n\n# Browse vignettes (longer tutorials)\nbrowseVignettes(\"dplyr\")\n\n\nLLMs\nLarge Language Models like ChatGPT, Claude, or GitHub Copilot can be very useful for explaining error messages, suggesting code solutions, or learning new functions. However, they can also be dangerous for at least two reasons:\n\nThey can produce wrong (or even malicous) code\nThey can keep you from learning what you need to learn\n\nIn general, LLMs become more useful when you know the basics - at this stage, you will know how to prompt effectively and the chatbot can help you with quickly writing boilerplate code or suggesting different approaches. Try to be a conscious and responsible user and don’t just copy paste blindly.",
    "crumbs": [
      "Basics of `R` programming",
      "Introduction to `R`"
    ]
  },
  {
    "objectID": "casestudy-eurostat.html",
    "href": "casestudy-eurostat.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "In the wake of the 2030 Agenda for Sustainable Development, the United Nations has defined 17 Sustainable Development Goals (SDGs). The SDGs cover a range of social, economic, and environmental areas, such as poverty, education, or climate change. This notebook aims to provide an answer to the question of how European regions vary regarding their accomplishment of SDG 1 (no poverty) and how Germany fares compared to other European countries.\nThe plot that we obtain at the end of the notebook looks like this:",
    "crumbs": [
      "Case studies",
      "Making a map from Eurostat data"
    ]
  },
  {
    "objectID": "casestudy-eurostat.html#step-0-asking-a-question",
    "href": "casestudy-eurostat.html#step-0-asking-a-question",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "In the wake of the 2030 Agenda for Sustainable Development, the United Nations has defined 17 Sustainable Development Goals (SDGs). The SDGs cover a range of social, economic, and environmental areas, such as poverty, education, or climate change. This notebook aims to provide an answer to the question of how European regions vary regarding their accomplishment of SDG 1 (no poverty) and how Germany fares compared to other European countries.\nThe plot that we obtain at the end of the notebook looks like this:",
    "crumbs": [
      "Case studies",
      "Making a map from Eurostat data"
    ]
  },
  {
    "objectID": "casestudy-eurostat.html#step-1-getting-data",
    "href": "casestudy-eurostat.html#step-1-getting-data",
    "title": "Quantitative Methods and Statistics",
    "section": "Step 1: Getting data",
    "text": "Step 1: Getting data\nWe’re asking about differences in European regions, so a good place to start is Eurostat, the statistical office of the European Union. Indeed, Eurostat has compiled a range of statistical indicators tracking all 17 SDGs.\nTypically, we would now proceed to manually find and download a suitable dataset using the web interface, e.g. as an Excel spreadsheet. However, there is a more convenient way: We can use the eurostat R package to directly interface with Eurostat.\n\nlibrary(eurostat) # R package to interface with Eurostat\n\nLooking at the above website, a suitable table is ilc_peps11, which contains information on persons at risk of poverty or social exclusion, aggregated by NUTS regions. We can use this table code to download the table to an R data frame (the equivalent of an excel spreadhseet):\n\ndat &lt;- get_eurostat(id = \"ilc_peps11\", time_format = \"num\")\n\nTable ilc_peps11 cached at /tmp/RtmpJpzsdB/eurostat/faeacdd4504e47e73c0f199e2f434f34.rds\n\n\n\ndat\n\n# A tibble: 3,210 × 5\n   freq  unit  geo   TIME_PERIOD values\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 A     PC    AL           2017   51.8\n 2 A     PC    AL           2018   49  \n 3 A     PC    AL           2019   46.2\n 4 A     PC    AL           2020   43.4\n 5 A     PC    AL01         2017   56.7\n 6 A     PC    AL01         2018   48.2\n 7 A     PC    AL01         2019   47.3\n 8 A     PC    AL01         2020   44.8\n 9 A     PC    AL02         2017   49.4\n10 A     PC    AL02         2018   51  \n# ℹ 3,200 more rows\n\n\nNext to the data about poverty rates, we need data about the spatial geometries of our units to create a map. Luckily, this is also available from Eurostat through the GISCO service, which we can also query through the R package:\n\nlibrary(sf) # R package for handling spatial data\n\ndat_geo &lt;- get_eurostat_geospatial(nuts_level = 2)\n\nHere, we specify the NUTS regional level at which to obtain the data (level 2 means Regierungsbezirke in Germany) to match the highest administrative resolution of our poverty indicator.",
    "crumbs": [
      "Case studies",
      "Making a map from Eurostat data"
    ]
  },
  {
    "objectID": "casestudy-eurostat.html#step-2-wrangling-data",
    "href": "casestudy-eurostat.html#step-2-wrangling-data",
    "title": "Quantitative Methods and Statistics",
    "section": "Step 2: Wrangling data",
    "text": "Step 2: Wrangling data\nAs we can see in the table above, the data we got contains information for different years (2003 to 2022), and at different levels of spatial aggregation (NUTS 0 - 2).\nWe here filter down the data set to contain data for 2019 at NUTS 2 (which is represented by 4-digit codes):\n\nlibrary(tidyverse) # R package(s) to handle and plot data\n\ndat_clean &lt;- rename(dat, time = TIME_PERIOD)\ndat_2019 &lt;- filter(dat_clean, time == 2019)\n\n\ndat_clean\n\n# A tibble: 3,210 × 5\n   freq  unit  geo    time values\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 A     PC    AL     2017   51.8\n 2 A     PC    AL     2018   49  \n 3 A     PC    AL     2019   46.2\n 4 A     PC    AL     2020   43.4\n 5 A     PC    AL01   2017   56.7\n 6 A     PC    AL01   2018   48.2\n 7 A     PC    AL01   2019   47.3\n 8 A     PC    AL01   2020   44.8\n 9 A     PC    AL02   2017   49.4\n10 A     PC    AL02   2018   51  \n# ℹ 3,200 more rows\n\n\nBefore we proceed to visualizing our data, we need to combine our poverty indicator with the spatial data. We do this with a left_join operation:\n\ndat_combined &lt;- left_join(dat_geo, dat_2019, by=\"geo\")",
    "crumbs": [
      "Case studies",
      "Making a map from Eurostat data"
    ]
  },
  {
    "objectID": "casestudy-eurostat.html#step-3-visualizing-data",
    "href": "casestudy-eurostat.html#step-3-visualizing-data",
    "title": "Quantitative Methods and Statistics",
    "section": "Step 3: Visualizing data",
    "text": "Step 3: Visualizing data\n\nPanel A: Map with poverty risk for NUTS-2 regions, 2019\nWith the combined data on poverty and spatial geometries at hand, we are now ready to plot a map. We do this using the ggplot2 R package, which is contained in the tidyverse already loaded above.\nYou create a plot with ggplot by adding together different layers (geom_*) and further specifications, and by specifying a mapping between variables in the input data set and visual properties of the plot (using aes()):\n\ntheme_set(theme_minimal())\n\np_map &lt;- ggplot(dat_combined) +\n  geom_sf(aes(fill=values)) +\n  coord_sf(xlim=c(-10, 35), ylim=c(35, 70)) +\n  scale_fill_viridis_c(direction=-1, option = \"C\")\n\np_map\n\n\n\n\n\n\n\n\n\n\nPanel B: Time series of poverty risk at the national level\nWe now also want to plot the change in poverty risk, for which we resort to a time series plot. We start by filtering down the original data frame to two-digit country codes:\n\ndat_ts &lt;- filter(dat_clean, nchar(geo) == 2)\n\nTo highlight both a mean trend as well as the trend for Germany, we furthermore create two helper data sets. First, We group by year and compute the mean over all countries:\n\ndat_ts_mean &lt;- group_by(dat_ts, time) |&gt; summarize(values=mean(values))\n\nSecond, we filter our national trend data to only include rows with the ‘DE’ country code:\n\ndat_ts_de &lt;- filter(dat_ts, geo == \"DE\")\n\nWe are now ready to put together a plot with all country trends in grey and highlighted trends for the mean and Germany. We do this by stacking geom_line() specifications and adding annotations with annotate() :\n\nhighlight_col &lt;- \"darkorange\"\n\np_ts &lt;- ggplot() + \n  geom_line(data=dat_ts, aes(x=time, y=values, group=geo), color=\"grey\") +\n  geom_line(data=dat_ts_mean, aes(x=time, y=values), color=\"black\", lwd=1) +\n  geom_line(data=dat_ts_de, aes(x=time, y=values), color=highlight_col, lwd=1) +\n  annotate(\"text\", x=2017, y=29, label=\"Mean\", color=\"black\") +\n  annotate(\"text\", x=2017, y=15, label=\"Germany\", color=highlight_col) +\n  lims(y=c(0, NA), x=c(2004, 2020)) +\n  labs(x=NULL, y=\"At risk of poverty (%)\")\n\np_ts\n\n\n\n\n\n\n\n\n\n\nPanel C: Histogram with individual regions highlighted\nFinally, we want to create a histogram of the regional poverty risk (at NUTS 2 level) and highlight the highest risk and lowest risk regions in Germany. We can obtain the latter with a combination of filtering and sorting (via arrange()) and then pulling out the first and last row (with slice()):\n\nde_hi_lo &lt;- dat_combined |&gt;\n  drop_na() |&gt;\n  filter(time == 2019,\n         nchar(geo) == 4,\n         str_starts(geo, \"DE\")) |&gt;\n  arrange(values) |&gt;\n  slice(c(1, n()))\n\nWith this in place, we create the histogram with geom_histogram() and add highlights with geom_vline() and geom_text() :\n\np_hist &lt;- ggplot(de_hi_lo) +\n  geom_histogram(\n    data=drop_na(dat_combined), \n    aes(x=values), \n    binwidth=2\n  ) +\n  geom_vline(\n    aes(xintercept=values), \n    color=highlight_col, \n    lwd=1\n  ) +\n  geom_text(\n    aes(values, label=NUTS_NAME), \n    y=16, \n    nudge_x=c(-3, 3), \n    color=highlight_col\n  ) +\n  labs(\n    x=\"At risk of poverty (%)\", \n    y=\"Frequency\"\n  )\n\np_hist\n\n\n\n\n\n\n\n\n\n\nCombining the plots\nFinally, we can put together and annotate the three plots with the patchwork R package, which allows us to stack plots on top of each other (with /) or put them side by side (with |):\n\nlibrary(patchwork)\n\np_map / p_ts | p_hist\n\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\ntitle &lt;- \"Attainment of SDG 1 (no poverty) across Europe\"\n\nsubtitle &lt;- \"Regionalized risk of poverty in European regions. Panel A shows data for 2019 at NUTS level 2, panel B shows national trends.\\nPanel C shows the distribution of regional risk rates and highlights the lowest and highest rate German regions for reference.\"\n\ncaption = \"Eurostat table: ilc_peps11\"\n\np_combined &lt;- p_map | (p_ts / p_hist)\n\np_combined &lt;- p_combined + \n  plot_annotation(\n    title = title,\n    subtitle = subtitle, \n    caption = caption, \n    tag_levels = \"A\",\n    theme = theme(\n      plot.title = element_text(size=20),\n      plot.subtitle = element_text(colour=\"grey50\")\n    )\n  )\n\nggsave(\"sdg_1_europe.png\", width=16.5, height=11.7, dpi=300)\n\np_combined",
    "crumbs": [
      "Case studies",
      "Making a map from Eurostat data"
    ]
  },
  {
    "objectID": "10-generalized-linear-models-and-additive-models.html#nonlinear-relationships",
    "href": "10-generalized-linear-models-and-additive-models.html#nonlinear-relationships",
    "title": "Quantitative Methods and Statistics",
    "section": "Nonlinear relationships",
    "text": "Nonlinear relationships"
  },
  {
    "objectID": "10-generalized-linear-models-and-additive-models.html#smoothing-splines",
    "href": "10-generalized-linear-models-and-additive-models.html#smoothing-splines",
    "title": "Quantitative Methods and Statistics",
    "section": "Smoothing splines",
    "text": "Smoothing splines"
  },
  {
    "objectID": "10-generalized-linear-models-and-additive-models.html#informational-efficiency-and-partial-pooling",
    "href": "10-generalized-linear-models-and-additive-models.html#informational-efficiency-and-partial-pooling",
    "title": "Quantitative Methods and Statistics",
    "section": "Informational efficiency and partial pooling",
    "text": "Informational efficiency and partial pooling"
  },
  {
    "objectID": "10-generalized-linear-models-and-additive-models.html#varying-intercepts",
    "href": "10-generalized-linear-models-and-additive-models.html#varying-intercepts",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts",
    "text": "Varying intercepts"
  },
  {
    "objectID": "10-generalized-linear-models-and-additive-models.html#varying-intercepts-and-coefficients",
    "href": "10-generalized-linear-models-and-additive-models.html#varying-intercepts-and-coefficients",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts and coefficients",
    "text": "Varying intercepts and coefficients"
  },
  {
    "objectID": "10-generalized-linear-models-and-additive-models.html#higher-level-predictors",
    "href": "10-generalized-linear-models-and-additive-models.html#higher-level-predictors",
    "title": "Quantitative Methods and Statistics",
    "section": "Higher-level predictors",
    "text": "Higher-level predictors"
  },
  {
    "objectID": "10-observational-studies.html#random-assignment-of-treatments",
    "href": "10-observational-studies.html#random-assignment-of-treatments",
    "title": "Quantitative Methods and Statistics",
    "section": "Random assignment of treatments",
    "text": "Random assignment of treatments"
  },
  {
    "objectID": "10-observational-studies.html#confounding",
    "href": "10-observational-studies.html#confounding",
    "title": "Quantitative Methods and Statistics",
    "section": "Confounding",
    "text": "Confounding"
  },
  {
    "objectID": "10-observational-studies.html#graphical-causal-models",
    "href": "10-observational-studies.html#graphical-causal-models",
    "title": "Quantitative Methods and Statistics",
    "section": "Graphical causal models",
    "text": "Graphical causal models"
  },
  {
    "objectID": "12-generalized-additive-models.html#nonlinear-relationships",
    "href": "12-generalized-additive-models.html#nonlinear-relationships",
    "title": "Quantitative Methods and Statistics",
    "section": "Nonlinear relationships",
    "text": "Nonlinear relationships"
  },
  {
    "objectID": "12-generalized-additive-models.html#smoothing-splines",
    "href": "12-generalized-additive-models.html#smoothing-splines",
    "title": "Quantitative Methods and Statistics",
    "section": "Smoothing splines",
    "text": "Smoothing splines"
  },
  {
    "objectID": "12-generalized-additive-models.html#informational-efficiency-and-partial-pooling",
    "href": "12-generalized-additive-models.html#informational-efficiency-and-partial-pooling",
    "title": "Quantitative Methods and Statistics",
    "section": "Informational efficiency and partial pooling",
    "text": "Informational efficiency and partial pooling"
  },
  {
    "objectID": "12-generalized-additive-models.html#varying-intercepts",
    "href": "12-generalized-additive-models.html#varying-intercepts",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts",
    "text": "Varying intercepts"
  },
  {
    "objectID": "12-generalized-additive-models.html#varying-intercepts-and-coefficients",
    "href": "12-generalized-additive-models.html#varying-intercepts-and-coefficients",
    "title": "Quantitative Methods and Statistics",
    "section": "Varying intercepts and coefficients",
    "text": "Varying intercepts and coefficients"
  },
  {
    "objectID": "12-generalized-additive-models.html#higher-level-predictors",
    "href": "12-generalized-additive-models.html#higher-level-predictors",
    "title": "Quantitative Methods and Statistics",
    "section": "Higher-level predictors",
    "text": "Higher-level predictors"
  },
  {
    "objectID": "05-descriptive-multivariate.html",
    "href": "05-descriptive-multivariate.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "As in the last session, we will continue working with our CO₂ emissions, population, and GDP data. However, instead of analyzing variables in isolation, we will now move to multivariate descriptions and explore how variables relate to each other. Ultimately, many multivariate research designs will be asymmetric in the roles that variables occupy, i.e., typically we want to study variation in one primary variable as a function of other variables. However, we will here start with simple symmetric, bivariate measures used to describe the strength and sign of a statistical association between two variables.\nWe begin by loading packages and reading the datasets from parquet files:\n\nlibrary(tidyverse)\n\ndf &lt;- arrow::read_parquet(\"data/emi-pop-gdp.parquet\")\n\nA quick glance at the summaries of our three primary variables shows some of the summaries we learned about last session but also indicates some missing values in GDP and population, which are not available for all years and all countries:\n\nsummary(df$emissions)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.174   1.016   3.945   4.337 782.743 \n\nsummary(df$population)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n1.472e+03 6.820e+05 4.571e+06 5.004e+07 1.484e+07 8.092e+09      8128 \n\nsummary(df$gdp)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   510.8   4305.9  12391.6  21291.5  30701.2 174339.0    16499 \n\n\nTo avoid the missing data problem and to make things easier, we will focus on a single year again, to begin with:\n\ndf2020 &lt;- filter(df, year == 2020) \n\nNow we have a combined dataset with emissions, population, and GDP data for countries. The kinds of multivariate analyses which are sensible depend on the combination of our variables’ data types, which we will walk through in the following.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#scatterplots",
    "href": "05-descriptive-multivariate.html#scatterplots",
    "title": "Quantitative Methods and Statistics",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots are the fundamental tool for visualizing the relationship between two numerical variables. Each point represents one observation, with its position determined by the values of both variables.\n\nggplot(df2020, aes(x = gdp, y = emissions)) +\n    geom_point(alpha = 0.4) +\n    labs(\n        x = \"GDP per capita (USD)\",\n        y = \"CO2 emissions per capita (tons)\"\n    )\n\nWarning: Removed 19 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe scatterplot reveals a clear positive relationship: wealthier countries tend to have higher per-capita emissions. However, there is considerable variation, especially among wealthier countries, and a lot of the observations are clumped together at the lower end.\nTwo sessions ago, we have learned that in such a case, it might make sense to apply a log transformation to our variables:\n\nggplot(df2020, aes(x = gdp, y = emissions)) +\n    geom_point(alpha = 0.6) +\n    scale_x_log10() +\n    scale_y_log10() +\n    labs(\n        x = \"log GDP per capita (USD)\",\n        y = \"log CO2 emissions per capita (tons)\"\n    )\n\nWarning: Removed 19 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis reveals a much less noisy picture of the relationship and also makes sense, conceptually: An increase in GDP per capita by a factor of X is typically associated with an increase in emissions by a factor of Y. Going forward, take note of the fact that the relationship is quite linear, but not perfectly so.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#pearsons-correlation-coefficient",
    "href": "05-descriptive-multivariate.html#pearsons-correlation-coefficient",
    "title": "Quantitative Methods and Statistics",
    "section": "Pearson’s correlation coefficient",
    "text": "Pearson’s correlation coefficient\nWhile scatterplots show us the relationship visually, correlation coefficients quantify the strength and direction of relationships between two variables in a symmetric fashion, meaning that the correlation of x and y is the same as the correlation of y and x. The best known correlation coefficient is Pearson’s correlation coefficient, which measures the strength of a linear relationship. It is defined as the covariance divided by the product of the standard deviations of the two input variables:\n\\[ r = \\frac{s_{xy}}{s_x s_y} \\]\nwhere the covariance is the mean of the product of the deviations from x and y:\n\\[ s_{xy} = \\frac{1}{n}\\sum_{i = 1}^n (x_i - \\bar{x}) (y_i - \\bar{y}) \\]\nThe covariance depends on the scaling of x and y and is thus hard to interpret. The correlation coefficient takes values in the -1 to 1 range, with -1 indicating a perfect negative linear relationship (i.e. the points lie on a downwards-sloping line) and +1 indicating a perfect positive linear relationship.\nWe can easily compute the covariance and the correlation in R using the cov() and cor() functions:\n\ncov(df2020$gdp, df2020$emissions, use = \"complete.obs\")\n\n[1] 86516.98\n\ncor(df2020$gdp, df2020$emissions, use = \"complete.obs\")\n\n[1] 0.6665462\n\n\nThe correlation of 0.67 for GDP per capita and per capita emissions confirms our visual impression of a strong positive relationship. The correlation becomes even stronger (0.9) when we correlate the log transformed values, again confirming our visual intuitions:\n\ncor(log(df2020$gdp), log(df2020$emissions), use = \"complete.obs\")\n\n[1] 0.8957234\n\n\nLet’s also check the correlation between GDP and population:\n\ncor(df2020$gdp, df2020$population, use = \"complete.obs\")\n\n[1] -0.02851595\n\n\nThe near-zero correlation (-0.03) tells us there’s essentially no linear relationship between a country’s wealth and its population size - rich countries can be small or large, and vice versa.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#spearmans-correlation-coefficient",
    "href": "05-descriptive-multivariate.html#spearmans-correlation-coefficient",
    "title": "Quantitative Methods and Statistics",
    "section": "Spearman’s correlation coefficient",
    "text": "Spearman’s correlation coefficient\nPearson correlation measures linear relationships, but we have seen before that the relationship between GDP and emissions is not perfectly linear. Spearman’s correlation coefficient measures monotonic relationships - whether one variable tends to increase (or decrease) as the other increases, regardless of whether this relationship is linear. It does so by first transforming the input variables to their rank representations and then computing the Pearson’s correlation coefficient. Because we are looking at ranks, it doesn’t matter if the magnitude of change in y differs with respect to the level of x, as long as the direction of change is still the same (i.e. the relationship is monotonic). Spearman correlation is also more robust to outliers since it works with ranks rather than actual values.\nTo see what the rank transformation is about, try it out yourself:\n\nrank(c(3.5, 12.7, 11.2, 2.8))\n\n[1] 2 4 3 1\n\n\nWith that, we could compute Spearman’s correlation ourselves by putting the rank transformed variables into the cor() function. However, we can also just pass the method = \"spearman\" argument to the cor() function, which is more convenient and efficient:\n\ncor(df2020$gdp, df2020$emissions, method = \"spearman\", use = \"complete.obs\")\n\n[1] 0.8793504\n\n\nThe Spearman correlation also assumes values in the -1 to +1 interval and so the value of 0.88 again signals a strong monotonic relationship, stronger than the basic version of Pearson’s correlation and similar to the log-log version. Note that log transforming variables before using Spearman’s correlation coefficient doesn’t do anything since the log transformation is a monotonic function and so the ranks are the same as for the untransformed variable.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#exercises",
    "href": "05-descriptive-multivariate.html#exercises",
    "title": "Quantitative Methods and Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nCreate a scatterplot showing the relationship between population and total GDP (GDP per capita × population). What pattern do you observe?\n\n\nCalculate the Pearson and Spearman correlations between population and total GDP. Which one is appropriate?\n\n\nLoad the gapminder dataset and plot life expectancy against GDP per capita. Describe the relationship. Choose a correlation measure and compute it.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#cross-tabulations",
    "href": "05-descriptive-multivariate.html#cross-tabulations",
    "title": "Quantitative Methods and Statistics",
    "section": "Cross-tabulations",
    "text": "Cross-tabulations\nCross-tabulations (or contingency tables) show the frequency of combinations between two categorical variables. Our primary variables are numerical, but we can create categorical ones (ordinal, to be precise) from them:\n\ndf2020 &lt;- df2020 |&gt; mutate(\n    emissions_level = case_when(\n        emissions &lt; 2 ~ \"Low emissions\",\n        emissions &lt; 8 ~ \"Medium emissions\",\n        emissions &gt;= 8 ~ \"High emissions\",\n        TRUE ~ NA\n    ),\n    emissions_level = factor(\n        emissions_level,\n        levels = c(\"Low emissions\", \"Medium emissions\", \"High emissions\"),\n        ordered = TRUE\n    ),\n    gdp_level = case_when(\n        gdp &lt; 5000 ~ \"Low GDP\",\n        gdp &lt; 20000 ~ \"Medium GDP\",\n        gdp &gt;= 20000 ~ \"High GDP\",\n        TRUE ~ NA\n    ),\n    gdp_level = factor(\n        gdp_level,\n        levels = c(\"Low GDP\", \"Medium GDP\", \"High GDP\"),\n        ordered = TRUE\n    )\n)\n\nNow we can create a cross-tabulation of development level and emissions level. The easiest way is to use the table() function with two vectors a input:\n\ntable(df2020$gdp_level, df2020$emissions_level) \n\n            \n             Low emissions Medium emissions High emissions\n  Low GDP               39                0              0\n  Medium GDP            41               37              5\n  High GDP               4               47             23\n\n\nAlternatively, we can also use the familiar tidyverse machinery to create it manually (more flexible but also more code):\n\ncount(df2020, gdp_level, emissions_level) |&gt;\n    pivot_wider(\n        names_from = emissions_level, \n        values_from = n, \n        values_fill = 0)\n\n# A tibble: 4 × 4\n  gdp_level  `Low emissions` `Medium emissions` `High emissions`\n  &lt;ord&gt;                &lt;int&gt;              &lt;int&gt;            &lt;int&gt;\n1 Low GDP                 39                  0                0\n2 Medium GDP              41                 37                5\n3 High GDP                 4                 47               23\n4 &lt;NA&gt;                     4                 11                4\n\n\nThis table shows clear patterns: all low-income countries have low emissions, while high-income countries are distributed across all emission levels, with most having medium or high per capita emissions.\nIf we want proportions instead of raw counts, we can wrap our original table into prop.table(), to which we can pass an additional margin argument (where one indicates to divide by row totals and two would produce column :\n\nprop.table(table(df2020$gdp_level, df2020$emissions_level), margin = 1)\n\n            \n             Low emissions Medium emissions High emissions\n  Low GDP       1.00000000       0.00000000     0.00000000\n  Medium GDP    0.49397590       0.44578313     0.06024096\n  High GDP      0.05405405       0.63513514     0.31081081",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#stacked-barplots",
    "href": "05-descriptive-multivariate.html#stacked-barplots",
    "title": "Quantitative Methods and Statistics",
    "section": "Stacked barplots",
    "text": "Stacked barplots\nCross-tabulations give us the numbers, but visualizations make the patterns clearer. We can use stacked bar charts to compare the distribution of one categorical variable across levels of another.\n\ndf2020 |&gt; drop_na(gdp_level) |&gt;\n    ggplot(aes(x = gdp_level, fill = emissions_level)) + geom_bar(position = \"stack\")\n\n\n\n\n\n\n\n\nOften, we care more about the relative numbers than the overall group sizes. In that case, we can use proportional stacked bars, which make it even clearer how the emission profiles differ across development levels. We can create a proportional barplot by changing the position = \"stack\" argument to position = \"fill\":\n\ndf2020 |&gt; drop_na(gdp_level) |&gt;\n    ggplot(aes(x = gdp_level, fill = emissions_level)) + geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nIf we instead mostly care about the absolute numbers, creating a grouped barplot by passing position = \"dodge\" might be preferred.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#chi-squared-statistic-and-cramérs-v",
    "href": "05-descriptive-multivariate.html#chi-squared-statistic-and-cramérs-v",
    "title": "Quantitative Methods and Statistics",
    "section": "Chi-squared statistic and Cramér’s V",
    "text": "Chi-squared statistic and Cramér’s V\nWhile we cannot use the various correlation coefficients for categorical data, we can quantify the strength of association between categorical variables using the chi-squared statistic and Cramér’s V.\nThe chi-squared statistic measures how much the observed frequencies deviate from what we’d expect if there were no association, accounting for differences in group sizes. To compute it, we first need to produce the contingency table:\n\ncont_table &lt;- table(df2020$gdp_level, df2020$emissions_level)\n\nBased on that, we can use the chisq.test() function, which also produces a statistical test, which we will ignore for now. Instead, we just look at the summary statistics, which we can access with $statistics:\n\nchi_result &lt;- chisq.test(cont_table)\nchi_result$statistic\n\nX-squared \n 103.0496 \n\n\nThe resulting value of 103 is difficult to interpret because it depends on sample size. Cramér’s V standardizes this to a 0 to 1 scale, making it more useful as a measure of association. While there is again no built in function in base R, it is easy to compute so we don’t need to pull in a package:\n\ncramers_v &lt;- sqrt(chi_result$statistic / (sum(cont_table) * min(dim(cont_table) - 1)))\ncramers_v\n\nX-squared \n0.5127198 \n\n\nA Cramér’s V of 0.51 indicates a reasonably strong statistical association, but whether any specific measurement of a statistical relationship is judged to be high of course always depends on context.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#exercises-1",
    "href": "05-descriptive-multivariate.html#exercises-1",
    "title": "Quantitative Methods and Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nCreate a new categorical variable that groups countries as “Small” (population &lt; 10 million), “Medium” (10-50 million), or “Large” (&gt; 50 million).\n\n\nCreate a cross-tabulation between your population size categories and development levels. What patterns do you see?\n\n\nVisualize the cross-tabulation.\n\n\nCalculate Cramér’s V for the association between population size and development level. How does this compare to the association strength between development and emissions level?",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#boxplots",
    "href": "05-descriptive-multivariate.html#boxplots",
    "title": "Quantitative Methods and Statistics",
    "section": "Boxplots",
    "text": "Boxplots\nA classical visualization which is designed to compare the distributional differences of a numerical variable across groups is the boxplot or ‘box and whiskers’ plot. They display the median, quartiles (i.e. the range containing 50% of the data), and outliers, allowing for quick visual comparison of numerical summaries. Let’s look at a boxplot of emissions by GDP level:\n\nggplot(df2020, aes(x = gdp_level, y = emissions)) + geom_boxplot()\n\n\n\n\n\n\n\n\nIn a boxplot, the ‘box’ shows the interquartile range (IQR), the bar inside the box shows the median, the ‘whiskers’ show 1.5 * the IQR, and everything beyond the whiskers is considered an outlier. The plot is again a little bit hard to read due to the presence of some extreme emissions outliers. We have learned that we can help ourselves with a log transformation. To make it easier to read off values, we also add log ticks with the guide = \"axis_logticks\" argument:\n\nggplot(df2020, aes(x = gdp_level, y = emissions)) + \n    geom_boxplot() + \n    scale_y_log10(guide = \"axis_logticks\")\n\n\n\n\n\n\n\n\nThe usual caveats for log plots apply: While it has become easier to see the distribution at lower levels, the same visual length now means different things at different levels (in absolute terms), so more care needs to be taken to not misinterpret the plot.\nA simpler alternative is to again just show the full data instead of summaries. To avoid points being drawn on top of each other (which would make the plot hard to read), we can ‘jitter’ them, i.e., randomly shift them horizontally a little bit:\n\np &lt;- ggplot(df2020, aes(x = gdp_level, y = emissions, group = gdp_level)) + \n    geom_jitter(width = 0.1) +\n    scale_y_log10(guide = \"axis_logticks\")\n\np\n\n\n\n\n\n\n\n\nIf we still want an indication for the central tendency (e.g. a mean or median), we can add a horizontal line with stat_summary() and a crossbar geometry:\n\np + stat_summary(fun = median, geom = \"crossbar\", color = \"tomato\")",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#groupwise-summaries",
    "href": "05-descriptive-multivariate.html#groupwise-summaries",
    "title": "Quantitative Methods and Statistics",
    "section": "Groupwise summaries",
    "text": "Groupwise summaries\nTo produce numerical summaries across groups, we can just use the now familiar group_by() and summarize() machinery and a set of measures of location and dispersion suited for our numerical variable:\n\ngroup_summaries &lt;- df2020 |&gt;\n    group_by(gdp_level) |&gt;\n    summarize(\n        count = n(),\n        mean = mean(emissions),\n        weighted_mean = weighted.mean(emissions, population),\n        median = median(emissions),\n        sd = sd(emissions),\n        vc = sd / mean,\n        mad = mad(emissions),\n    )\n\ngroup_summaries\n\n# A tibble: 4 × 8\n  gdp_level  count  mean weighted_mean median    sd    vc   mad\n  &lt;ord&gt;      &lt;int&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Low GDP       39 0.388         0.280  0.290 0.338 0.872 0.241\n2 Medium GDP    83 2.77          4.10   2.15  2.47  0.891 1.72 \n3 High GDP      74 8.54          9.28   6.71  6.52  0.763 3.46 \n4 &lt;NA&gt;          19 4.80          3.28   3.65  4.54  0.945 1.97 \n\n\nWe could just report this as a summary table or use it to compute specific comparisons, such as the ratio between two groups:\n\ngroup_summaries$mean[3] / group_summaries$mean[1]\n\n[1] 22.00109\n\n\nWe see that the average emissions per capita in rich countries is more than 22 times the average emissions of poor countries.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "05-descriptive-multivariate.html#exercises-2",
    "href": "05-descriptive-multivariate.html#exercises-2",
    "title": "Quantitative Methods and Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize the distribution of emissions by world region, as per the countrycode package introduced last session.\nVisualize the distribution of emissions by world region and by year from 2000 to 2020.",
    "crumbs": [
      "Describing data",
      "Multivariate descriptions"
    ]
  },
  {
    "objectID": "07-globe-tossing.html",
    "href": "07-globe-tossing.html",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "There are different competing paradigms for inferring unobserved model quantities, such as parameters, from data. The approach we will discuss here is Bayesian inference, which directly uses the language and formalism of probability to describe uncertainty in unknown quantities of interest. (the alternative is frequentist inference, which has been dominant for almost 100 years). Both approaches have certain advantages and disadvantages, but in my experience, Bayesian inference is often more intuitive in its handling of uncertainty and allows to focus on model construction, which should be the focus of statistical modelling efforts.\n\n\n\nIn his excellent book Statistical Rethinking, Richard McElreath (2020) describes the following experiment:\n\nWe are interested in inferring the share of the earth’s surface that is covered by Water. All we have is a waterball representing a globe which we throw repeatedly, each time recording whether our left thumb (or any other finger of choice) is on land or on water when we catch the ball. Assuming the throws are random, the sequence of observations generated from repeatedly throwing the ball should carry information about the proportion of the ball’s surface that is covered by water.\n\nWith this description, we can start building a simple model of the data generating process. Assuming the throws are independent and the probability of landing water or land only depends on the proportion of the surface covered by each, we can specify the result of throw \\(i\\) as coming from a Bernoulli distribution:\n\\[ Y_i \\sim \\textrm{Bernoulli}(\\theta) \\]\nHere, \\(Y_i\\) is a random variable equal to 1 when the throw produces ‘water’, and 0 if it produces ‘land’. The parameter \\(\\theta\\) is the underlying probability of observing ‘water’, which we want to learn about by observing how often our throws produce each of the two possible outcomes. Given our assumptions above, knowing \\(\\theta\\) would tell us the share of the Earth’s surface covered by water.\nIf we knew \\(\\theta\\), we could simulate the process with the rbinom function (there’s no built-in bernoulli distribution, because it is just a special case of the binomial distribution with size = 1):\n\ntheta &lt;- 0.7\nthrows &lt;- rbinom(n = 100, size = 1, prob = theta)\ntable(throws)\n\nthrows\n 0  1 \n30 70 \n\n\nHowever, we usually don’t know the model parameters beforehand and instead have to estimate them from data. The next section will discuss how to do this.\n\n\n\nIn Bayesian inference, our goal is to obtain the posterior distribution of the parameters, given the data. Our two main components in getting it are the prior distribution of parameters (before we have seen the data) and the likelihood, which captures the influence of the data.\n\n\nThe likelihood evaluates how likely the observed data would have been under certain parameter configurations: E.g., imagine observing 10 water out of 100 throws for \\(\\theta = 0.7\\). For this model, the likelihood of a single throw the probability mass function of the Bernoulli distribution. The likelihood of multiple throws is the product of the individual likelihoods:\n\\[ \\textrm{likelihood}(\\theta) = \\prod_{i=1}^n \\textrm{Bernoulli}(X=x_i | \\theta) \\]\nwhere \\(\\textrm{Bernoulli}(X=x | \\theta) = \\theta^x (1 - \\theta)^{1-x}\\).\nAnalogously to how we can simulate data from the distribution with rbinom(), we can use dbinom() to evaluate the distribution function on a set of observations with a specific value for \\(\\theta\\):\n\ntheta &lt;- 0.7\nobs &lt;- c(0, 1, 1, 0, 1)\ndbinom(obs, size = 1, prob = theta)\n\n[1] 0.3 0.7 0.7 0.3 0.7\n\n\nThese are just the probabilities of observing the respective outcome in obs based on a Bernoulli distribution with \\(\\theta = 0.7\\). Their combined likelihood (assuming that they are independent) is their product:\n\nprod(dbinom(obs, size = 1, prob = theta))\n\n[1] 0.03087\n\n\nBased on this, we can see that the above data are more likely to be produced by, e.g., \\(\\theta = 0.6\\) than \\(\\theta = 0.2\\):\n\nprod(dbinom(obs, size = 1, prob = 0.6))\n\n[1] 0.03456\n\nprod(dbinom(obs, size = 1, prob = 0.2))\n\n[1] 0.00512\n\n\nWith the likelihood understood as a function of the parameter, we could now, e.g., find the value of \\(\\theta\\) which maximizes it (aptly known as maximum likelihood estimation). Instead, we’re going to use it to get the posterior distribution of the parameter to additionally quantify our uncertainty about it. For this, we need an additional ingredient, the prior.\n\n\n\nPlot the likelihood for a range of parameter values (remember the seq() function). What does it tell you?\n\n\nPlay around with different specifications for both the data and the parameters to get a feeling for how the likelihood behaves.\n\n\n\n\n\nThe prior distribution is the distribution of the parameter before seeing the data. If we are ignorant about the problem we’re studying, we can start from a point where we treat each possible value for \\(\\theta\\) (i.e., each value in the (0, 1) range) as a priori equally likely. One distribution to encode this for the case discussed here is the beta distribution, which models a random variable taking values in the (0, 1) range. It has two parameters, and we get a flat distribution if we set both of them to 1:\n\nlibrary(tidyverse)\n\nggplot() +\n  stat_function(\n    fun = dbeta,\n    args = list(shape1 = 1, shape2 = 1)\n  ) + labs(\n    x = \"Theta\", y = \"Beta(theta | 1, 1)\"\n  )\n\n\n\n\n\n\n\n\nWe can confirm that all possible values are equally likely by directly using the dbeta() function:\n\ndbeta(c(0.2, 0.5, 0.8), shape1 = 1, shape2 = 1)\n\n[1] 1 1 1\n\n\nWe can also confirm that values outside the possible range get a value of 0:\n\ndbeta(2, shape1 = 1, shape2 = 1)\n\n[1] 0\n\n\nSpecifying a flat prior, like we did above, means that we didn’t know anything about our problem before conducting the analysis. That is often not the case; Sometimes, we know something substantial about the problem at hand, and sometimes we just have a vague knowledge about what would be an implausible outcome. We can use this knowledge to specify better priors, and indeed specifying reasonable priors has a range of benefits. Luckily, we often can rely on software to help us with this, although it is always good to check one’s priors through something called prior predictive checks (more about this later).\n\n\nThink about what you know about the water cover of the earth and specify a prior which is more appropriate than the flat one used above. Play around with the parameters of the beta distribution until you find a specification that you agree with by visualizing the result.\n\n\n\n\nHaving observed a sequence of throws of the globe, we can use the likelihood and the prior to compute the posterior distribution of the parmeters, i.e., the distribution of the parameter given the data. Based on Bayes rule, the posterior distribution of the data given the parameters is proportional to the prior times the likelihood:\n\\[\n\\textrm{posterior}(\\theta) \\propto \\textrm{likelihood}(\\theta) \\times \\textrm{prior}(\\theta)\n\\]\nIn our specific case, we can write out the posterior distribution (up to a constant) based on the specifications above as:\n\\[\np(\\theta | y) \\propto \\prod_{i = 1}^n \\textrm{Bernoulli}(y_i | \\theta) \\times \\textrm{Beta}(\\theta | 1, 1)\n\\]\nWe can again compute this for specified values of \\(\\theta\\) using R:\n\ntheta &lt;- 0.7\nlik &lt;- prod(dbinom(obs, size = 1, prob = theta))\npri &lt;- dbeta(theta, shape1 = 1, shape2 = 1)\nlik * pri\n\n[1] 0.03087\n\n\nWe can also compute this over a range of values and then visualize the posterior distribution (up to a constant):\n\npost &lt;- tibble(\n  theta = seq(0, 1, length.out = 100),\n  post = sapply(theta, FUN = function(x) {\n    prod(dbinom(obs, size = 1, prob = x)) * dbeta(x, 1, 1)\n  })\n)\n\n# NOTE: sapply() computes a function over a range of values;\n# Here, the function just computes the product of likelihood and prior.\n\nggplot(post, aes(theta, post)) + geom_line()\n\n\n\n\n\n\n\n\nWhile we have departed from our flat prior distribution, we also see that our handful of data points are not that informative - given these data, many values of the underlying parameter \\(\\theta\\) are plausible. On the other hand, even based on very limited data the posterior already tells us that extreme values of \\(\\theta\\) are unlikely to have generated the data.\nNote also that what we see above is not actually the full posterior distribution. To turn the above into an actual probability distribution, we would have to compute a normalizing constant, which can actually be very tricky (remember that the posterior is only proportional to the product of likelihood and prior).\n\n\nFor the simple case we have here, it turns out the posterior is given by a \\(\\textrm{Beta}(1 + \\#\\textrm{Land}, 1 + \\#\\textrm{Water})\\) distribution (replace the 1s with whatever you picked for the prior if you want something else than a flat prior):\n\nggplot() +\n  stat_function(\n    fun = dbeta,\n    args = list(\n      shape1 = 1 + sum(obs == 1),\n      shape2 = 1 + sum(obs == 0)\n    )\n  ) + stat_function(\n    fun = dbeta,\n    args = list(\n      shape1 = 1 + sum(obs == 1),\n      shape2 = 1 + sum(obs == 0)\n    ),\n    xlim = c(0.5, 1), geom = \"area\", alpha = 0.4\n  ) + labs(\n    x = \"Theta\", y = \"Beta(theta | 1, 1)\"\n  )\n\n\n\n\n\n\n\n\nObserve how the shape is exactly the same as above but the y scale is different. This is because now the distribution is properly normalized, i.e. the constant which we dismissed before is now accounted for.\nThe shaded area represents the probability that \\(\\theta\\) is 0.5 or greater. We can compute this based on the cumulative distribution function (CDF), i.e. the probability that \\(\\theta\\) is smaller or equal than a given value. We can compute this with the pbeta() function:\n\npbeta(0.5, 1 + sum(obs == 1), 1 + sum(obs == 0))\n\n[1] 0.34375\n\n\nThis would give us the area under the curve until 0.5. But what we want is everything from 0.5 to 1, so we just take \\(1 - P(\\theta &gt; 0.5)\\):\n\n1 - pbeta(0.5, 1 + sum(obs == 1), 1 + sum(obs == 0))\n\n[1] 0.65625\n\n\nUsing the CDF this way, we can compute probabilities for arbitrary ranges.\nIf we instead want to know the values of \\(\\theta\\) so that a certain amount of probability is below it, we can use the quantile function, which is implemented by qbeta(). E.g., to get a 95%-interval (i.e., the value range, such that only 2.5% of probability is below it and only 2.5% is above it), we can compute:\n\nqbeta(c(0.025, 0.975), 1 + sum(obs == 1), 1 + sum(obs == 0))\n\n[1] 0.2227781 0.8818828\n\n\nThis says that based on the observed data there’s a 95% chance that the true proportion of water is between 0.22 and 0.88. If we collected more data, the width of this range would decrease as we gain more evidence.\nThe kind of analytical solution for the posterior distribution (which can be derived algebraically) we have just seen is sadly not always available. However, there are many different ways to approximate the posterior distribution numerically, which is often even more convenient than having to manipulate distributions to obtain posterior quantities of interest, like we just did.\n\n\n\n\nPick different values for the observed data and check how the posterior distribution of \\(\\theta\\) changes.\n\n\nPick a value for theta and simulate 1000 throws of the globe. Compute the posterior distribution and compare it to the real value (make a plot).\n\n\nCompute a 90% interval for the posterior.",
    "crumbs": [
      "Statistical modeling",
      "Bayesian inference"
    ]
  },
  {
    "objectID": "07-globe-tossing.html#how-to-learn-from-data",
    "href": "07-globe-tossing.html#how-to-learn-from-data",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "There are different competing paradigms for inferring unobserved model quantities, such as parameters, from data. The approach we will discuss here is Bayesian inference, which directly uses the language and formalism of probability to describe uncertainty in unknown quantities of interest. (the alternative is frequentist inference, which has been dominant for almost 100 years). Both approaches have certain advantages and disadvantages, but in my experience, Bayesian inference is often more intuitive in its handling of uncertainty and allows to focus on model construction, which should be the focus of statistical modelling efforts.",
    "crumbs": [
      "Statistical modeling",
      "Bayesian inference"
    ]
  },
  {
    "objectID": "07-globe-tossing.html#example-estimating-the-earths-water-cover-by-throwing-a-globe",
    "href": "07-globe-tossing.html#example-estimating-the-earths-water-cover-by-throwing-a-globe",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "In his excellent book Statistical Rethinking, Richard McElreath (2020) describes the following experiment:\n\nWe are interested in inferring the share of the earth’s surface that is covered by Water. All we have is a waterball representing a globe which we throw repeatedly, each time recording whether our left thumb (or any other finger of choice) is on land or on water when we catch the ball. Assuming the throws are random, the sequence of observations generated from repeatedly throwing the ball should carry information about the proportion of the ball’s surface that is covered by water.\n\nWith this description, we can start building a simple model of the data generating process. Assuming the throws are independent and the probability of landing water or land only depends on the proportion of the surface covered by each, we can specify the result of throw \\(i\\) as coming from a Bernoulli distribution:\n\\[ Y_i \\sim \\textrm{Bernoulli}(\\theta) \\]\nHere, \\(Y_i\\) is a random variable equal to 1 when the throw produces ‘water’, and 0 if it produces ‘land’. The parameter \\(\\theta\\) is the underlying probability of observing ‘water’, which we want to learn about by observing how often our throws produce each of the two possible outcomes. Given our assumptions above, knowing \\(\\theta\\) would tell us the share of the Earth’s surface covered by water.\nIf we knew \\(\\theta\\), we could simulate the process with the rbinom function (there’s no built-in bernoulli distribution, because it is just a special case of the binomial distribution with size = 1):\n\ntheta &lt;- 0.7\nthrows &lt;- rbinom(n = 100, size = 1, prob = theta)\ntable(throws)\n\nthrows\n 0  1 \n30 70 \n\n\nHowever, we usually don’t know the model parameters beforehand and instead have to estimate them from data. The next section will discuss how to do this.",
    "crumbs": [
      "Statistical modeling",
      "Bayesian inference"
    ]
  },
  {
    "objectID": "07-globe-tossing.html#bayesian-inference-in-theory",
    "href": "07-globe-tossing.html#bayesian-inference-in-theory",
    "title": "Quantitative Methods and Statistics",
    "section": "",
    "text": "In Bayesian inference, our goal is to obtain the posterior distribution of the parameters, given the data. Our two main components in getting it are the prior distribution of parameters (before we have seen the data) and the likelihood, which captures the influence of the data.\n\n\nThe likelihood evaluates how likely the observed data would have been under certain parameter configurations: E.g., imagine observing 10 water out of 100 throws for \\(\\theta = 0.7\\). For this model, the likelihood of a single throw the probability mass function of the Bernoulli distribution. The likelihood of multiple throws is the product of the individual likelihoods:\n\\[ \\textrm{likelihood}(\\theta) = \\prod_{i=1}^n \\textrm{Bernoulli}(X=x_i | \\theta) \\]\nwhere \\(\\textrm{Bernoulli}(X=x | \\theta) = \\theta^x (1 - \\theta)^{1-x}\\).\nAnalogously to how we can simulate data from the distribution with rbinom(), we can use dbinom() to evaluate the distribution function on a set of observations with a specific value for \\(\\theta\\):\n\ntheta &lt;- 0.7\nobs &lt;- c(0, 1, 1, 0, 1)\ndbinom(obs, size = 1, prob = theta)\n\n[1] 0.3 0.7 0.7 0.3 0.7\n\n\nThese are just the probabilities of observing the respective outcome in obs based on a Bernoulli distribution with \\(\\theta = 0.7\\). Their combined likelihood (assuming that they are independent) is their product:\n\nprod(dbinom(obs, size = 1, prob = theta))\n\n[1] 0.03087\n\n\nBased on this, we can see that the above data are more likely to be produced by, e.g., \\(\\theta = 0.6\\) than \\(\\theta = 0.2\\):\n\nprod(dbinom(obs, size = 1, prob = 0.6))\n\n[1] 0.03456\n\nprod(dbinom(obs, size = 1, prob = 0.2))\n\n[1] 0.00512\n\n\nWith the likelihood understood as a function of the parameter, we could now, e.g., find the value of \\(\\theta\\) which maximizes it (aptly known as maximum likelihood estimation). Instead, we’re going to use it to get the posterior distribution of the parameter to additionally quantify our uncertainty about it. For this, we need an additional ingredient, the prior.\n\n\n\nPlot the likelihood for a range of parameter values (remember the seq() function). What does it tell you?\n\n\nPlay around with different specifications for both the data and the parameters to get a feeling for how the likelihood behaves.\n\n\n\n\n\nThe prior distribution is the distribution of the parameter before seeing the data. If we are ignorant about the problem we’re studying, we can start from a point where we treat each possible value for \\(\\theta\\) (i.e., each value in the (0, 1) range) as a priori equally likely. One distribution to encode this for the case discussed here is the beta distribution, which models a random variable taking values in the (0, 1) range. It has two parameters, and we get a flat distribution if we set both of them to 1:\n\nlibrary(tidyverse)\n\nggplot() +\n  stat_function(\n    fun = dbeta,\n    args = list(shape1 = 1, shape2 = 1)\n  ) + labs(\n    x = \"Theta\", y = \"Beta(theta | 1, 1)\"\n  )\n\n\n\n\n\n\n\n\nWe can confirm that all possible values are equally likely by directly using the dbeta() function:\n\ndbeta(c(0.2, 0.5, 0.8), shape1 = 1, shape2 = 1)\n\n[1] 1 1 1\n\n\nWe can also confirm that values outside the possible range get a value of 0:\n\ndbeta(2, shape1 = 1, shape2 = 1)\n\n[1] 0\n\n\nSpecifying a flat prior, like we did above, means that we didn’t know anything about our problem before conducting the analysis. That is often not the case; Sometimes, we know something substantial about the problem at hand, and sometimes we just have a vague knowledge about what would be an implausible outcome. We can use this knowledge to specify better priors, and indeed specifying reasonable priors has a range of benefits. Luckily, we often can rely on software to help us with this, although it is always good to check one’s priors through something called prior predictive checks (more about this later).\n\n\nThink about what you know about the water cover of the earth and specify a prior which is more appropriate than the flat one used above. Play around with the parameters of the beta distribution until you find a specification that you agree with by visualizing the result.\n\n\n\n\nHaving observed a sequence of throws of the globe, we can use the likelihood and the prior to compute the posterior distribution of the parmeters, i.e., the distribution of the parameter given the data. Based on Bayes rule, the posterior distribution of the data given the parameters is proportional to the prior times the likelihood:\n\\[\n\\textrm{posterior}(\\theta) \\propto \\textrm{likelihood}(\\theta) \\times \\textrm{prior}(\\theta)\n\\]\nIn our specific case, we can write out the posterior distribution (up to a constant) based on the specifications above as:\n\\[\np(\\theta | y) \\propto \\prod_{i = 1}^n \\textrm{Bernoulli}(y_i | \\theta) \\times \\textrm{Beta}(\\theta | 1, 1)\n\\]\nWe can again compute this for specified values of \\(\\theta\\) using R:\n\ntheta &lt;- 0.7\nlik &lt;- prod(dbinom(obs, size = 1, prob = theta))\npri &lt;- dbeta(theta, shape1 = 1, shape2 = 1)\nlik * pri\n\n[1] 0.03087\n\n\nWe can also compute this over a range of values and then visualize the posterior distribution (up to a constant):\n\npost &lt;- tibble(\n  theta = seq(0, 1, length.out = 100),\n  post = sapply(theta, FUN = function(x) {\n    prod(dbinom(obs, size = 1, prob = x)) * dbeta(x, 1, 1)\n  })\n)\n\n# NOTE: sapply() computes a function over a range of values;\n# Here, the function just computes the product of likelihood and prior.\n\nggplot(post, aes(theta, post)) + geom_line()\n\n\n\n\n\n\n\n\nWhile we have departed from our flat prior distribution, we also see that our handful of data points are not that informative - given these data, many values of the underlying parameter \\(\\theta\\) are plausible. On the other hand, even based on very limited data the posterior already tells us that extreme values of \\(\\theta\\) are unlikely to have generated the data.\nNote also that what we see above is not actually the full posterior distribution. To turn the above into an actual probability distribution, we would have to compute a normalizing constant, which can actually be very tricky (remember that the posterior is only proportional to the product of likelihood and prior).\n\n\nFor the simple case we have here, it turns out the posterior is given by a \\(\\textrm{Beta}(1 + \\#\\textrm{Land}, 1 + \\#\\textrm{Water})\\) distribution (replace the 1s with whatever you picked for the prior if you want something else than a flat prior):\n\nggplot() +\n  stat_function(\n    fun = dbeta,\n    args = list(\n      shape1 = 1 + sum(obs == 1),\n      shape2 = 1 + sum(obs == 0)\n    )\n  ) + stat_function(\n    fun = dbeta,\n    args = list(\n      shape1 = 1 + sum(obs == 1),\n      shape2 = 1 + sum(obs == 0)\n    ),\n    xlim = c(0.5, 1), geom = \"area\", alpha = 0.4\n  ) + labs(\n    x = \"Theta\", y = \"Beta(theta | 1, 1)\"\n  )\n\n\n\n\n\n\n\n\nObserve how the shape is exactly the same as above but the y scale is different. This is because now the distribution is properly normalized, i.e. the constant which we dismissed before is now accounted for.\nThe shaded area represents the probability that \\(\\theta\\) is 0.5 or greater. We can compute this based on the cumulative distribution function (CDF), i.e. the probability that \\(\\theta\\) is smaller or equal than a given value. We can compute this with the pbeta() function:\n\npbeta(0.5, 1 + sum(obs == 1), 1 + sum(obs == 0))\n\n[1] 0.34375\n\n\nThis would give us the area under the curve until 0.5. But what we want is everything from 0.5 to 1, so we just take \\(1 - P(\\theta &gt; 0.5)\\):\n\n1 - pbeta(0.5, 1 + sum(obs == 1), 1 + sum(obs == 0))\n\n[1] 0.65625\n\n\nUsing the CDF this way, we can compute probabilities for arbitrary ranges.\nIf we instead want to know the values of \\(\\theta\\) so that a certain amount of probability is below it, we can use the quantile function, which is implemented by qbeta(). E.g., to get a 95%-interval (i.e., the value range, such that only 2.5% of probability is below it and only 2.5% is above it), we can compute:\n\nqbeta(c(0.025, 0.975), 1 + sum(obs == 1), 1 + sum(obs == 0))\n\n[1] 0.2227781 0.8818828\n\n\nThis says that based on the observed data there’s a 95% chance that the true proportion of water is between 0.22 and 0.88. If we collected more data, the width of this range would decrease as we gain more evidence.\nThe kind of analytical solution for the posterior distribution (which can be derived algebraically) we have just seen is sadly not always available. However, there are many different ways to approximate the posterior distribution numerically, which is often even more convenient than having to manipulate distributions to obtain posterior quantities of interest, like we just did.\n\n\n\n\nPick different values for the observed data and check how the posterior distribution of \\(\\theta\\) changes.\n\n\nPick a value for theta and simulate 1000 throws of the globe. Compute the posterior distribution and compare it to the real value (make a plot).\n\n\nCompute a 90% interval for the posterior.",
    "crumbs": [
      "Statistical modeling",
      "Bayesian inference"
    ]
  }
]